{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment, to be able to run the notebook, the 2 files on D2L (Assignment 2) need to be dragged to the notebook folder"
      ],
      "metadata": {
        "id": "B02WxN3qHf4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1"
      ],
      "metadata": {
        "id": "RQ0QH9I5WmAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1 (10 marks)\n",
        "For this exercise, you will be scraping information from Quotes to Scrape. Write a function to\n",
        "extract the quotes and author names from each page of the site. Write the results into a table\n",
        "that collects the information as separate columns OR store the results as a dictionary if you\n",
        "prefer. Design your method so that it would work regardless of the number of pages the site\n",
        "actually has. (10 marks)"
      ],
      "metadata": {
        "id": "Bee52zL9wYtS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIPPIAagmt7e",
        "outputId": "244975bf-c8d5-4941-c6ac-a79209fa8183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Initialize dictionary (I store the quote using dictionary)\n",
        "quotes = {}\n",
        "def extract_quotes(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Initial page number\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "      # Page format: https://quotes.toscrape.com/page/1/\n",
        "      page_url = f\"{html}/page/{page_number}/\"\n",
        "      print(page_url)\n",
        "      response = requests.get(page_url)\n",
        "\n",
        "      soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "      if \"No quotes found!\" in response.text:\n",
        "          break\n",
        "\n",
        "      quote_blocks = soup.find_all('div', class_='quote')\n",
        "      for quote_block in quote_blocks:\n",
        "        text = quote_block.find('span', class_='text').text.strip()\n",
        "        author = quote_block.find('small', class_='author').text.strip()\n",
        "        # Save in dictionary\n",
        "        quotes[text] = author\n",
        "\n",
        "      page_number += 1\n",
        "    return quotes\n",
        "\n",
        "# Example usage:\n",
        "html_link = \"https://quotes.toscrape.com\"\n",
        "quotes_result = extract_quotes(html_link)\n",
        "\n",
        "for quote, author in quotes.items():\n",
        "    print(f\"Quote: {quote}\")\n",
        "    print(f\"Author: {author}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKq2pgd6wvq6",
        "outputId": "96397a2c-02ad-4e2c-bb0f-fc8cf8debdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-9619d87775c3>:7: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(html, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://quotes.toscrape.com/page/1/\n",
            "https://quotes.toscrape.com/page/2/\n",
            "https://quotes.toscrape.com/page/3/\n",
            "https://quotes.toscrape.com/page/4/\n",
            "https://quotes.toscrape.com/page/5/\n",
            "https://quotes.toscrape.com/page/6/\n",
            "https://quotes.toscrape.com/page/7/\n",
            "https://quotes.toscrape.com/page/8/\n",
            "https://quotes.toscrape.com/page/9/\n",
            "https://quotes.toscrape.com/page/10/\n",
            "https://quotes.toscrape.com/page/11/\n",
            "Quote: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
            "Author: Jane Austen\n",
            "\n",
            "Quote: “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
            "Author: Marilyn Monroe\n",
            "\n",
            "Quote: “Try not to become a man of success. Rather become a man of value.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “It is better to be hated for what you are than to be loved for what you are not.”\n",
            "Author: André Gide\n",
            "\n",
            "Quote: “I have not failed. I've just found 10,000 ways that won't work.”\n",
            "Author: Thomas A. Edison\n",
            "\n",
            "Quote: “A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
            "Author: Eleanor Roosevelt\n",
            "\n",
            "Quote: “A day without sunshine is like, you know, night.”\n",
            "Author: Steve Martin\n",
            "\n",
            "Quote: “This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\n",
            "Author: Marilyn Monroe\n",
            "\n",
            "Quote: “It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “If you can't explain it to a six year old, you don't understand it yourself.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\n",
            "Author: Bob Marley\n",
            "\n",
            "Quote: “I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”\n",
            "Author: Dr. Seuss\n",
            "\n",
            "Quote: “I may not have gone where I intended to go, but I think I have ended up where I needed to be.”\n",
            "Author: Douglas Adams\n",
            "\n",
            "Quote: “The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\n",
            "Author: Elie Wiesel\n",
            "\n",
            "Quote: “It is not a lack of love, but a lack of friendship that makes unhappy marriages.”\n",
            "Author: Friedrich Nietzsche\n",
            "\n",
            "Quote: “Good friends, good books, and a sleepy conscience: this is the ideal life.”\n",
            "Author: Mark Twain\n",
            "\n",
            "Quote: “Life is what happens to us while we are making other plans.”\n",
            "Author: Allen Saunders\n",
            "\n",
            "Quote: “I love you without knowing how, or when, or from where. I love you simply, without problems or pride: I love you in this way because I do not know any other way of loving but this, in which there is no I or you, so intimate that your hand upon my chest is my hand, so intimate that when I fall asleep your eyes close.”\n",
            "Author: Pablo Neruda\n",
            "\n",
            "Quote: “For every minute you are angry you lose sixty seconds of happiness.”\n",
            "Author: Ralph Waldo Emerson\n",
            "\n",
            "Quote: “If you judge people, you have no time to love them.”\n",
            "Author: Mother Teresa\n",
            "\n",
            "Quote: “Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
            "Author: Garrison Keillor\n",
            "\n",
            "Quote: “Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
            "Author: Jim Henson\n",
            "\n",
            "Quote: “Today you are You, that is truer than true. There is no one alive who is Youer than You.”\n",
            "Author: Dr. Seuss\n",
            "\n",
            "Quote: “If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “It is impossible to live without failing at something, unless you live so cautiously that you might as well not have lived at all - in which case, you fail by default.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “Logic will get you from A to Z; imagination will get you everywhere.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “One good thing about music, when it hits you, you feel no pain.”\n",
            "Author: Bob Marley\n",
            "\n",
            "Quote: “The more that you read, the more things you will know. The more that you learn, the more places you'll go.”\n",
            "Author: Dr. Seuss\n",
            "\n",
            "Quote: “Of course it is happening inside your head, Harry, but why on earth should that mean that it is not real?”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “The truth is, everyone is going to hurt you. You just got to find the ones worth suffering for.”\n",
            "Author: Bob Marley\n",
            "\n",
            "Quote: “Not all of us can do great things. But we can do small things with great love.”\n",
            "Author: Mother Teresa\n",
            "\n",
            "Quote: “To the well-organized mind, death is but the next great adventure.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “All you need is love. But a little chocolate now and then doesn't hurt.”\n",
            "Author: Charles M. Schulz\n",
            "\n",
            "Quote: “We read to know we're not alone.”\n",
            "Author: William Nicholson\n",
            "\n",
            "Quote: “Any fool can know. The point is to understand.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “I have always imagined that Paradise will be a kind of library.”\n",
            "Author: Jorge Luis Borges\n",
            "\n",
            "Quote: “It is never too late to be what you might have been.”\n",
            "Author: George Eliot\n",
            "\n",
            "Quote: “A reader lives a thousand lives before he dies, said Jojen. The man who never reads lives only one.”\n",
            "Author: George R.R. Martin\n",
            "\n",
            "Quote: “You can never get a cup of tea large enough or a book long enough to suit me.”\n",
            "Author: C.S. Lewis\n",
            "\n",
            "Quote: “You believe lies so you eventually learn to trust no one but yourself.”\n",
            "Author: Marilyn Monroe\n",
            "\n",
            "Quote: “If you can make a woman laugh, you can make her do anything.”\n",
            "Author: Marilyn Monroe\n",
            "\n",
            "Quote: “Life is like riding a bicycle. To keep your balance, you must keep moving.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “The real lover is the man who can thrill you by kissing your forehead or smiling into your eyes or just staring into space.”\n",
            "Author: Marilyn Monroe\n",
            "\n",
            "Quote: “A wise girl kisses but doesn't love, listens but doesn't believe, and leaves before she is left.”\n",
            "Author: Marilyn Monroe\n",
            "\n",
            "Quote: “Only in the darkness can you see the stars.”\n",
            "Author: Martin Luther King Jr.\n",
            "\n",
            "Quote: “It matters not what someone is born, but what they grow to be.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “Love does not begin and end the way we seem to think it does. Love is a battle, love is a war; love is a growing up.”\n",
            "Author: James Baldwin\n",
            "\n",
            "Quote: “There is nothing I would not do for those who are really my friends. I have no notion of loving people by halves, it is not my nature.”\n",
            "Author: Jane Austen\n",
            "\n",
            "Quote: “Do one thing every day that scares you.”\n",
            "Author: Eleanor Roosevelt\n",
            "\n",
            "Quote: “I am good, but not an angel. I do sin, but I am not the devil. I am just a small girl in a big world trying to find someone to love.”\n",
            "Author: Marilyn Monroe\n",
            "\n",
            "Quote: “If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “If you only read the books that everyone else is reading, you can only think what everyone else is thinking.”\n",
            "Author: Haruki Murakami\n",
            "\n",
            "Quote: “The difference between genius and stupidity is: genius has its limits.”\n",
            "Author: Alexandre Dumas fils\n",
            "\n",
            "Quote: “He's like a drug for you, Bella.”\n",
            "Author: Stephenie Meyer\n",
            "\n",
            "Quote: “There is no friend as loyal as a book.”\n",
            "Author: Ernest Hemingway\n",
            "\n",
            "Quote: “When one door of happiness closes, another opens; but often we look so long at the closed door that we do not see the one which has been opened for us.”\n",
            "Author: Helen Keller\n",
            "\n",
            "Quote: “Life isn't about finding yourself. Life is about creating yourself.”\n",
            "Author: George Bernard Shaw\n",
            "\n",
            "Quote: “That's the problem with drinking, I thought, as I poured myself a drink. If something bad happens you drink in an attempt to forget; if something good happens you drink in order to celebrate; and if nothing happens you drink to make something happen.”\n",
            "Author: Charles Bukowski\n",
            "\n",
            "Quote: “You don’t forget the face of the person who was your last hope.”\n",
            "Author: Suzanne Collins\n",
            "\n",
            "Quote: “Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\n",
            "Author: Suzanne Collins\n",
            "\n",
            "Quote: “To love at all is to be vulnerable. Love anything and your heart will be wrung and possibly broken. If you want to make sure of keeping it intact you must give it to no one, not even an animal. Wrap it carefully round with hobbies and little luxuries; avoid all entanglements. Lock it up safe in the casket or coffin of your selfishness. But in that casket, safe, dark, motionless, airless, it will change. It will not be broken; it will become unbreakable, impenetrable, irredeemable. To love is to be vulnerable.”\n",
            "Author: C.S. Lewis\n",
            "\n",
            "Quote: “Not all those who wander are lost.”\n",
            "Author: J.R.R. Tolkien\n",
            "\n",
            "Quote: “Do not pity the dead, Harry. Pity the living, and, above all those who live without love.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “There is nothing to writing. All you do is sit down at a typewriter and bleed.”\n",
            "Author: Ernest Hemingway\n",
            "\n",
            "Quote: “Finish each day and be done with it. You have done what you could. Some blunders and absurdities no doubt crept in; forget them as soon as you can. Tomorrow is a new day. You shall begin it serenely and with too high a spirit to be encumbered with your old nonsense.”\n",
            "Author: Ralph Waldo Emerson\n",
            "\n",
            "Quote: “I have never let my schooling interfere with my education.”\n",
            "Author: Mark Twain\n",
            "\n",
            "Quote: “I have heard there are troubles of more than one kind. Some come from ahead and some come from behind. But I've bought a big bat. I'm all ready you see. Now my troubles are going to have troubles with me!”\n",
            "Author: Dr. Seuss\n",
            "\n",
            "Quote: “If I had a flower for every time I thought of you...I could walk through my garden forever.”\n",
            "Author: Alfred Tennyson\n",
            "\n",
            "Quote: “Some people never go crazy. What truly horrible lives they must lead.”\n",
            "Author: Charles Bukowski\n",
            "\n",
            "Quote: “The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
            "Author: Terry Pratchett\n",
            "\n",
            "Quote: “Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”\n",
            "Author: Dr. Seuss\n",
            "\n",
            "Quote: “What really knocks me out is a book that, when you're all done reading it, you wish the author that wrote it was a terrific friend of yours and you could call him up on the phone whenever you felt like it. That doesn't happen much, though.”\n",
            "Author: J.D. Salinger\n",
            "\n",
            "Quote: “The reason I talk to myself is because I’m the only one whose answers I accept.”\n",
            "Author: George Carlin\n",
            "\n",
            "Quote: “You may say I'm a dreamer, but I'm not the only one. I hope someday you'll join us. And the world will live as one.”\n",
            "Author: John Lennon\n",
            "\n",
            "Quote: “I am free of all prejudice. I hate everyone equally. ”\n",
            "Author: W.C. Fields\n",
            "\n",
            "Quote: “The question isn't who is going to let me; it's who is going to stop me.”\n",
            "Author: Ayn Rand\n",
            "\n",
            "Quote: “′Classic′ - a book which people praise and don't read.”\n",
            "Author: Mark Twain\n",
            "\n",
            "Quote: “Anyone who has never made a mistake has never tried anything new.”\n",
            "Author: Albert Einstein\n",
            "\n",
            "Quote: “A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\n",
            "Author: Jane Austen\n",
            "\n",
            "Quote: “Remember, if the time should come when you have to make a choice between what is right and what is easy, remember what happened to a boy who was good, and kind, and brave, because he strayed across the path of Lord Voldemort. Remember Cedric Diggory.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “I declare after all there is no enjoyment like reading! How much sooner one tires of any thing than of a book! -- When I have a house of my own, I shall be miserable if I have not an excellent library.”\n",
            "Author: Jane Austen\n",
            "\n",
            "Quote: “There are few people whom I really love, and still fewer of whom I think well. The more I see of the world, the more am I dissatisfied with it; and every day confirms my belief of the inconsistency of all human characters, and of the little dependence that can be placed on the appearance of merit or sense.”\n",
            "Author: Jane Austen\n",
            "\n",
            "Quote: “Some day you will be old enough to start reading fairy tales again.”\n",
            "Author: C.S. Lewis\n",
            "\n",
            "Quote: “We are not necessarily doubting that God will do the best for us; we are wondering how painful the best will turn out to be.”\n",
            "Author: C.S. Lewis\n",
            "\n",
            "Quote: “The fear of death follows from the fear of life. A man who lives fully is prepared to die at any time.”\n",
            "Author: Mark Twain\n",
            "\n",
            "Quote: “A lie can travel half way around the world while the truth is putting on its shoes.”\n",
            "Author: Mark Twain\n",
            "\n",
            "Quote: “I believe in Christianity as I believe that the sun has risen: not only because I see it, but because by it I see everything else.”\n",
            "Author: C.S. Lewis\n",
            "\n",
            "Quote: “The truth.\" Dumbledore sighed. \"It is a beautiful and terrible thing, and should therefore be treated with great caution.”\n",
            "Author: J.K. Rowling\n",
            "\n",
            "Quote: “I'm the one that's got to die when it's time for me to die, so let me live my life the way I want to.”\n",
            "Author: Jimi Hendrix\n",
            "\n",
            "Quote: “To die will be an awfully big adventure.”\n",
            "Author: J.M. Barrie\n",
            "\n",
            "Quote: “It takes courage to grow up and become who you really are.”\n",
            "Author: E.E. Cummings\n",
            "\n",
            "Quote: “But better to get hurt by the truth than comforted with a lie.”\n",
            "Author: Khaled Hosseini\n",
            "\n",
            "Quote: “You never really understand a person until you consider things from his point of view... Until you climb inside of his skin and walk around in it.”\n",
            "Author: Harper Lee\n",
            "\n",
            "Quote: “You have to write the book that wants to be written. And if the book will be too difficult for grown-ups, then you write it for children.”\n",
            "Author: Madeleine L'Engle\n",
            "\n",
            "Quote: “Never tell the truth to people who are not worthy of it.”\n",
            "Author: Mark Twain\n",
            "\n",
            "Quote: “A person's a person, no matter how small.”\n",
            "Author: Dr. Seuss\n",
            "\n",
            "Quote: “... a mind needs books as a sword needs a whetstone, if it is to keep its edge.”\n",
            "Author: George R.R. Martin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2"
      ],
      "metadata": {
        "id": "ULBg2QYnWsez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2 (20 marks)\n",
        "\n",
        "A) Use the following corpus of D1 and D2 to manually calculate TF-IDF vectors for all the\n",
        "terms in Document 2 the same way scikit-learn does (by default, but assume no application of\n",
        "L2/Euclidean normalization) as discussed in class. (10 marks)\n",
        "\n",
        "D1: \"the cat leaps onto the fence, surprising both the dog and the nearby fox\"\n",
        "\n",
        "D2: \"the dog barks loudly and the fox dashes away\"\n",
        "\n",
        "B) Read in the reviews of IMDB_Dataset.csv.\n",
        "1. Convert the table of reviews into a bag-of-words (BOW) matrix using scikit-learn. (2\n",
        "marks)\n",
        "2. Do the same thing for each bigram of the text. (2 marks)\n",
        "3. Calculate the TF-IDF of these documents using scikit-learn. (2 marks)\n",
        "4. Describe 2 reasons why one hot encoding is rarely used for text representation any-\n",
        "more. Are there other vector space model representations that also share the issues you\n",
        "described? Name them. (4 marks)"
      ],
      "metadata": {
        "id": "x4ZIYypi82Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2A\n",
        "documents = ['the cat leaps onto the fence, surprising both the dog and the nearby fox',\n",
        "             'the dog barks loudly and the fox dashes away']\n",
        "\n",
        "'''\n",
        "As the question doesn't specify to preprocess or not, I still perfrom preprocess\n",
        "but it will not affect the answer as the original documents have already been lowercased\n",
        "and remove the '.' sign.\n",
        "'''\n",
        "processed_documents = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "\n",
        "# The documents list\n",
        "print(\"Our corpus: \", processed_documents)"
      ],
      "metadata": {
        "id": "ANvFjGSry2aI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c6bfe8-c883-4af3-9c4d-bc3ae018da49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our corpus:  ['the cat leaps onto the fence, surprising both the dog and the nearby fox', 'the dog barks loudly and the fox dashes away']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of words from documents\n",
        "words_from_docs = []\n",
        "for processed_document in processed_documents:\n",
        "  for word in processed_document.split():\n",
        "    words_from_docs.append(word)\n",
        "\n",
        "print(words_from_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO26pcbUCFfD",
        "outputId": "3c0e7848-0159-4d62-b606-87c0fc1b3204"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cat', 'leaps', 'onto', 'the', 'fence,', 'surprising', 'both', 'the', 'dog', 'and', 'the', 'nearby', 'fox', 'the', 'dog', 'barks', 'loudly', 'and', 'the', 'fox', 'dashes', 'away']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurance of each word in d2\n",
        "word_counts_d2 = {}\n",
        "\n",
        "for word in processed_documents[1].split():\n",
        "    if word in word_counts_d2:\n",
        "        word_counts_d2[word] += 1\n",
        "    else:\n",
        "        word_counts_d2[word] = 1\n",
        "\n",
        "print(word_counts_d2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1Rhgzf-8pcp",
        "outputId": "5ecbf087-2978-48b3-fbcf-614e53050a94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 2, 'dog': 1, 'barks': 1, 'loudly': 1, 'and': 1, 'fox': 1, 'dashes': 1, 'away': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate tf score\n",
        "tf = {}\n",
        "for word in word_counts_d2:\n",
        "  # print(word_counts[word])\n",
        "  tf[word] = word_counts_d2[word]/len(word_counts_d2)\n",
        "\n",
        "tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLXniTX4-Em9",
        "outputId": "158385a6-0e0e-43be-9a02-5161452fcc76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 0.25,\n",
              " 'dog': 0.125,\n",
              " 'barks': 0.125,\n",
              " 'loudly': 0.125,\n",
              " 'and': 0.125,\n",
              " 'fox': 0.125,\n",
              " 'dashes': 0.125,\n",
              " 'away': 0.125}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurance of each word in 2 corpus\n",
        "word_counts = {}\n",
        "\n",
        "for word in words_from_docs:\n",
        "    if word in word_counts:\n",
        "        word_counts[word] += 1\n",
        "    else:\n",
        "        word_counts[word] = 1\n",
        "\n",
        "print(word_counts)\n",
        "\n",
        "# Calculate documents with that contain word in d2\n",
        "word_occurance = {}\n",
        "for word in word_counts:\n",
        "  for doc in processed_documents:\n",
        "    if word in doc.split():\n",
        "      if word in word_occurance:\n",
        "        word_occurance[word] += 1\n",
        "      else:\n",
        "        word_occurance[word] = 1\n",
        "\n",
        "word_occurance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS5t8K1tGnSp",
        "outputId": "1f8ec55b-4ae7-4be3-e695-f311310efeab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 6, 'cat': 1, 'leaps': 1, 'onto': 1, 'fence,': 1, 'surprising': 1, 'both': 1, 'dog': 2, 'and': 2, 'nearby': 1, 'fox': 2, 'barks': 1, 'loudly': 1, 'dashes': 1, 'away': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 2,\n",
              " 'cat': 1,\n",
              " 'leaps': 1,\n",
              " 'onto': 1,\n",
              " 'fence,': 1,\n",
              " 'surprising': 1,\n",
              " 'both': 1,\n",
              " 'dog': 2,\n",
              " 'and': 2,\n",
              " 'nearby': 1,\n",
              " 'fox': 2,\n",
              " 'barks': 1,\n",
              " 'loudly': 1,\n",
              " 'dashes': 1,\n",
              " 'away': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# Calculate idf score like scikit-learn does\n",
        "idf = {}\n",
        "for word in word_counts:\n",
        "  idf[word] = 1 + math.log((len(documents)+1)/(word_occurance[word]+1)) # Here I use log base e (natural log)\n",
        "\n",
        "idf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtMGVdSYD6Wh",
        "outputId": "3472e7b4-8492-4462-f6e6-ed3ca0e451f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1.0,\n",
              " 'cat': 1.4054651081081644,\n",
              " 'leaps': 1.4054651081081644,\n",
              " 'onto': 1.4054651081081644,\n",
              " 'fence,': 1.4054651081081644,\n",
              " 'surprising': 1.4054651081081644,\n",
              " 'both': 1.4054651081081644,\n",
              " 'dog': 1.0,\n",
              " 'and': 1.0,\n",
              " 'nearby': 1.4054651081081644,\n",
              " 'fox': 1.0,\n",
              " 'barks': 1.4054651081081644,\n",
              " 'loudly': 1.4054651081081644,\n",
              " 'dashes': 1.4054651081081644,\n",
              " 'away': 1.4054651081081644}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I feel it's unclear whether we are expected to use the pure scikit-learn way or only the idf calculation in the scikit-learn way and the tf in the lecture slides way\n",
        "\n",
        "So I calculate using both the lecture slides with scikit-learn idf (path 1) and pure scikit-learn way (path 2)"
      ],
      "metadata": {
        "id": "9lH2W2-bGrym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path 1: Calculate tf_idf of word in d2 using the lecture slides and text_representation notebook way\n",
        "d2 = \"the dog barks loudly and the fox dashes away\"\n",
        "d2 = d2.split()\n",
        "tf_idf = {}\n",
        "for word in d2:\n",
        "  tf_idf[word] = tf[word] * idf[word]\n",
        "\n",
        "print(\"The tf-idf using the lecture slide formula for tf and with scikit-learn idf calculation:\")\n",
        "tf_idf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVM58tj_IfSK",
        "outputId": "7142ee07-b67f-4a8f-b1d0-bc319c26997e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tf-idf using the lecture slide formula for tf and with scikit-learn idf calculation:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 0.25,\n",
              " 'dog': 0.125,\n",
              " 'barks': 0.17568313851352055,\n",
              " 'loudly': 0.17568313851352055,\n",
              " 'and': 0.125,\n",
              " 'fox': 0.125,\n",
              " 'dashes': 0.17568313851352055,\n",
              " 'away': 0.17568313851352055}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path 2: Calculate tf_idf of word in d2 using the pure scikit-learn way\n",
        "\n",
        "# Calculate tf score (scikit learn way)\n",
        "tf_scikit = {}\n",
        "for word in word_counts_d2:\n",
        "  # print(word_counts[word])\n",
        "  tf_scikit[word] = word_counts_d2[word]\n",
        "\n",
        "d2 = \"the dog barks loudly and the fox dashes away\"\n",
        "d2 = d2.split()\n",
        "tf_idf_scikit = {}\n",
        "for word in d2:\n",
        "  tf_idf_scikit[word] = tf_scikit[word] * idf[word]\n",
        "\n",
        "print(\"The tf-idf using pure scikit-learn method:\")\n",
        "tf_idf_scikit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRSVveIjGUOh",
        "outputId": "fc774daa-a47d-4c24-ec42-f3423be5a9aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tf-idf using pure scikit-learn method:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 2.0,\n",
              " 'dog': 1.0,\n",
              " 'barks': 1.4054651081081644,\n",
              " 'loudly': 1.4054651081081644,\n",
              " 'and': 1.0,\n",
              " 'fox': 1.0,\n",
              " 'dashes': 1.4054651081081644,\n",
              " 'away': 1.4054651081081644}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2B\n",
        "# Part 1\n",
        "import pandas as pd\n",
        "imdb_dataframe = pd.read_csv(\"/content/IMDB_Dataset.csv\")\n",
        "\n",
        "# List to store all review\n",
        "imdb_review_list = []\n",
        "for i in range(len(imdb_dataframe)):\n",
        "  imdb_review_list.append(imdb_dataframe.review[i])\n",
        "\n",
        "imdb_review_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "TxY2udQNI3Jc",
        "outputId": "6945398c-ef2b-408b-927f-932583955cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the count vectorizer object\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "bow_representation = count_vect.fit_transform(imdb_review_list)\n",
        "\n",
        "for i in range(len(imdb_review_list)):\n",
        "    print(\"BoW representation for review {}: \".format(i), bow_representation[i].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h7CIohvJs96",
        "outputId": "522c6aac-192d-46a9-be7e-0525d5009879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW representation for review 0:  [[ 0  0  0  0  0  0  1  1  0  0  0  0  0  0  1  1  1  0  0  0  1  0  0  0\n",
            "   0  1  6  0  0  0  0  1  2  0  1  0  1  4  0  1  0  0  0  2  0  0  0  0\n",
            "   0  0  2  1  0  1  0  0  0  0  0  0  1  0  0  0  0  6  0  0  0  0  0  1\n",
            "   0  2  0  1  0  1  0  0  0  0  0  1  0  0  1  0  0  0  0  1  2  1  1  0\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0  1  1\n",
            "   0  0  0  1  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  1  2  0  0  0  1  1  0  0\n",
            "   0  0  0  0  2  0  1  0  1  0  0  1  1  0  0  0  1  1  1  0  0  0  1  0\n",
            "   0  0  0  0  0  0  0  0  2  0  0  1  0  5  3  0  0  0  0  1  1  0  0  0\n",
            "   0  0  1  2  0  0  1  0  0  1  1  1  0  0  1  0  1  0  0  0  0  0  1  0\n",
            "   0  0  0  0  0  0  0  0  1  0  0  0  1  0  1  1  0  0  1  0  0  0  0  2\n",
            "   0  1  1  0  0  0  0  0  0  0  1  0  0  0  0  3  0  1  2  0  0  0  1  1\n",
            "   1  9  6  1  2  0  0  0  0  2  0  0  0  0  1  0  0  0  1  0  0  1  0  0\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0\n",
            "   1  1  1  0  0  0  0  1  1  0  0  0  0  1  1  4  0  0  0  1  1  0  1  0\n",
            "   0  0  0  0  2  0  0  0  0  0  0  1  0  0  1  0  1  0  0  0  1  1  1  0\n",
            "   3  0  0  7  0  0  0  0  0  0  3  1  0  0  3  1  0  0  1  2  0  1  0  0\n",
            "   6  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  1  0  0  3  1  0  0  0  1  1  0  0  0\n",
            "   0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  2  0  0  1\n",
            "   0  0  0  0  0  1  2  0  0  1  0  0  1  0  0  0  1  1  0  0  0  0  0  0\n",
            "   0  0  0  1  0  1  0  1  0  0  0  3  1  1  0  0  0  0  0  1  0  0  3  0\n",
            "   1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  1  2  0  0\n",
            "   0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
            "   4  1 16  0  1  0  0  0  0  0  1  1  0  0  3  0  0  0  0  0  1  6  0  0\n",
            "   0  0  1  0  0  0  1  0  1  0  0  1  0  1  0  0  1  0  0  0  0  1  0  4\n",
            "   0  0  0  0  0  3  0  0  0  1  2  0  0  0  0  0  0  1  0  2  0  2  1  0\n",
            "   0  2  0  0  0  5  0  0  0  0  0  2  0  0  0  0  1  1  0  0  0  0  0  3\n",
            "   0  1  0]]\n",
            "BoW representation for review 1:  [[ 0  0  0  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  2  0  0  0\n",
            "   0  0  7  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0\n",
            "   0  2  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0  0  1  1  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  1  0\n",
            "   0  1  0  0  0  0  0  1  0  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0  1  1  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  1\n",
            "   0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  1  0  0  0  1  0  0\n",
            "   1  0  0  0  0  0  2  0  0  0  0  0  0  0  2  0  0  1  0  0  0  0  0  0\n",
            "   1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   2  0  0  5  0  0  0  0  1  0  1  1  2  0  0  0  0  1  0  0  2  0  0  0\n",
            "   0  0  0  0  0  0  2  1  0  0  0  0  0  1  0  0  0  0  2  0  0  0  0  1\n",
            "   0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0\n",
            "   1  0  0  0  0  2  0  1  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  1  0  0  0  0  1\n",
            "   1  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  1  0  1  1  1  0\n",
            "   0  0 16  0  1  0  0  1  0  0  0  0  1  0  0  0  0  0  1  0  0  2  0  0\n",
            "   1  0  0  1  0  1  0  0  0  0  1  0  0  0  0  0  1  0  0  2  0  0  0  0\n",
            "   0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  3  0  0  0  0  1  0\n",
            "   0  0  0  1  0  3  0  0  0  1  0  0  0  0  0  1  0  0  0  0  1  0  0  1\n",
            "   0  0  0]]\n",
            "BoW representation for review 2:  [[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 4 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 0 1 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 4 1 0 0 0 0 0\n",
            "  0 3 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 1 0\n",
            "  0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 4 0 0 0 0 0 1 1 0\n",
            "  0 4 2 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 4 0 0 0 0 0 0 1 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0\n",
            "  1 0 8 1 0 0 0 0 0 0 1 0 0 0 5 2 0 0 1 0 0 4 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 2\n",
            "  0 0 0 0 0 2 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]]\n",
            "BoW representation for review 3:  [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 4 0 0 0 0 0 2 1 0 0\n",
            "  0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 6 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 2 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 1 0 0 0 2 0 0 0 0 1 0 0 0\n",
            "  0 2 0 0 1 4 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0\n",
            "  0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 6 0 0 1 0 1 2 0 0 0 0 1 1 0 2 0 1 0 0 3 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
            "  0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2]]\n",
            "BoW representation for review 4:  [[ 0  0  0  0  0  0  2  0  1  1  0  0  1  0  0  0  0  0  0  1  2  0  0  0\n",
            "   0  0  5  1  1  0  0  0  1  0  0  1  0  1  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  1  0  0  1  0  0  1  0  1  0  0  0  0  0  0  8  0  0  0  0  0  0\n",
            "   1  1  0  0  0  0  0  1  1  1  0  0  0  2  0  0  0  0  0  0  1  0  0  0\n",
            "   1  0  0  0  0  0  0  0  0  0  0  1  1  1  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  1  1  0  0\n",
            "   1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  2  0\n",
            "   0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  2  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0\n",
            "   0  0  0  0  1  0  0  0  0  0  0  0  0  2  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
            "   1  0  0  0  0  1  0  2  0  0  0  0  0  1  0  6  1  0  0  0  0  0  0  0\n",
            "   0  7  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  1  1  0  0  1  1  0  1  0\n",
            "   0  0  0  1  0  0  0  0  0  0  0  0  4  0  0  0  0  0  1  0  0  1  0  0\n",
            "   0  0  0  2  0  1  0  1  0  3  0  0  0  0  0  0  0  1  2  0  0  0  1  0\n",
            "   1  0  0  6  0  1  0  0  0  0  1  6  1  0  1  0  0  0  0  0  0  1  0  1\n",
            "   0  0  0  0  0  0  0  0  0  0  3  0  0  0  1  1  1  0  0  1  1  0  0  0\n",
            "   0  0  1  0  0  1  0  1  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  1  0  0  0  0  0  0  0\n",
            "   0  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  1  2  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  0\n",
            "   0  0  0  0  0  1  1  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  1  0\n",
            "   1  0  1  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  1  0  0  0  0\n",
            "   1  0 20  0  1  0  1  0  0  4  1  1  0  0  2  0  0  0  2  0  0  7  0  0\n",
            "   0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  2  0  1  0  0  0  0  0  0\n",
            "   1  1  0  0  0  0  0  1  0  0  0  0  1  4  0  0  0  0  0  1  0  1  1  0\n",
            "   0  0  0  0  1  1  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  1  0\n",
            "   0  0  0]]\n",
            "BoW representation for review 5:  [[0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 5 0 0 0 0 0 1 0 0 0\n",
            "  0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            "  0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
            "  0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 3 0 0 0 0 0 0 0 0\n",
            "  0 2 3 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 3 0 1 2 0 0 0 0 0 3 0 0 1 0 0 0 0 0 0 1 1 0 0 2 0 0 0 0 1 0 0 1\n",
            "  1 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0\n",
            "  1 0 4 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 4 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  1 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "BoW representation for review 6:  [[1 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 5 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 3 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 3 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 3 0 0 0 0 0 0 0 0\n",
            "  0 1 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 2 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 1 0 0 0 1 3 0 1 0 1 0 1 0 0 0 0 0 0 0 4 0 0 1 0 0 1 1 0\n",
            "  0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 2 0 0 0 1 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            "  0 0 4 0 0 0 0 1 0 0 1 0 0 0 3 0 0 0 0 0 0 5 1 0 0 0 0 0 0 0 0 0 0 3 0 0\n",
            "  1 0 2 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 2 1 0 1 0 0 1 1 0 0 0 0\n",
            "  1 0 0 0 0 2 0 0 0 0 0 0 1 1 0 0 6 0 0 0 0 0 0 3 0 0 0]]\n",
            "BoW representation for review 7:  [[ 0  0  1  0  1  0  0  0  0  0  0  0  0  0  1  0  0  2  1  0  0  0  1  1\n",
            "   1  1  2  0  0  0  1  0  1  0  0  0  0  2  0  0  0  0  0  0  1  0  2  1\n",
            "   0  0  1  0  0  0  0  2  0  0  0  0  0  0  0  0  0  2  0  1  1  0  0  0\n",
            "   0  1  1  0  0  2  0  0  0  2  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  1  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
            "   0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0\n",
            "   0  1  0  0  0  0  0  1  2  1  0  0  1  2  0  0  0  1  0  0  0  0  0  0\n",
            "   1  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
            "   0  0  0  0  1  0  0  1  0  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  1  0  2  1  0  0  1  1  0  0  0  0  1  0  0  0  1  0  0  0  0\n",
            "   0  4  8  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0\n",
            "   2  1  0  4  1  0  0  0  0  0  2  1  0  0  1  0  2  0  0  0  0  1  0  0\n",
            "   0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  1  1  0  0  0  0  1  1  1  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  1  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  2  0  0  0  0  0  0\n",
            "   0  0  1  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   5  0 15  0  0  0  0  1  0  0  0  0  1  0  4  0  0  0  1  0  0  3  1  0\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  2  1  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  1  0  0\n",
            "   0  0  0]]\n",
            "BoW representation for review 8:  [[0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 3 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 2 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0\n",
            "  0 5 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 4 0 0 0 0 0 0 1 2\n",
            "  3 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  1 0 7 0 0 1 0 0 0 0 0 1 0 0 5 0 0 0 0 1 0 3 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "BoW representation for review 9:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 1 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 4 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For part 1, the vocabulary has 675 elements\n",
        "print(len(imdb_review_list))\n",
        "print(bow_representation.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qNCSJJ29IlJ",
        "outputId": "de475c52-770a-44da-f475-06b39fb51def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "(10, 675)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2B\n",
        "# Part 2\n",
        "# Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
        "count_vect = CountVectorizer(ngram_range=(2,2))\n",
        "bow_rep = count_vect.fit_transform(imdb_review_list)\n",
        "\n",
        "# Vocabulary mapping\n",
        "print(\"Vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "# Print the BoW for\n",
        "for i in range(len(imdb_review_list)):\n",
        "  print(\"BoW representation for review {}: \".format(i), bow_rep[i].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4zGdDaIKz45",
        "outputId": "b577dce0-7835-40e6-ca04-9134c5e1acee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:  {'one of': 774, 'of the': 745, 'the other': 1095, 'other reviewers': 799, 'reviewers has': 894, 'has mentioned': 424, 'mentioned that': 657, 'that after': 1035, 'after watching': 23, 'watching just': 1275, 'just oz': 568, 'oz episode': 812, 'episode you': 298, 'you ll': 1376, 'll be': 608, 'be hooked': 126, 'hooked they': 454, 'they are': 1147, 'are right': 89, 'right as': 896, 'as this': 105, 'this is': 1158, 'is exactly': 507, 'exactly what': 306, 'what happened': 1301, 'happened with': 415, 'with me': 1331, 'me br': 647, 'br br': 156, 'br the': 161, 'the first': 1075, 'first thing': 345, 'thing that': 1152, 'that struck': 1045, 'struck me': 1000, 'me about': 645, 'about oz': 9, 'oz was': 813, 'was its': 1259, 'its brutality': 553, 'brutality and': 170, 'and unflinching': 74, 'unflinching scenes': 1229, 'scenes of': 916, 'of violence': 752, 'violence which': 1252, 'which set': 1314, 'set in': 941, 'in right': 482, 'right from': 897, 'from the': 368, 'the word': 1123, 'word go': 1348, 'go trust': 387, 'trust me': 1220, 'me this': 652, 'is not': 516, 'not show': 724, 'show for': 949, 'for the': 358, 'the faint': 1071, 'faint hearted': 316, 'hearted or': 437, 'or timid': 791, 'timid this': 1181, 'this show': 1163, 'show pulls': 953, 'pulls no': 868, 'no punches': 713, 'punches with': 869, 'with regards': 1333, 'regards to': 885, 'to drugs': 1185, 'drugs sex': 283, 'sex or': 943, 'or violence': 792, 'violence its': 1250, 'its is': 555, 'is hardcore': 510, 'hardcore in': 419, 'in the': 483, 'the classic': 1061, 'classic use': 209, 'use of': 1237, 'word br': 1347, 'br it': 158, 'it is': 540, 'is called': 503, 'called oz': 183, 'oz as': 810, 'as that': 104, 'that is': 1039, 'is the': 525, 'the nickname': 1091, 'nickname given': 710, 'given to': 381, 'to the': 1199, 'the oswald': 1094, 'oswald maximum': 798, 'maximum security': 641, 'security state': 925, 'state penitentary': 992, 'penitentary it': 825, 'it focuses': 538, 'focuses mainly': 348, 'mainly on': 622, 'on emerald': 763, 'emerald city': 290, 'city an': 206, 'an experimental': 42, 'experimental section': 310, 'section of': 924, 'the prison': 1104, 'prison where': 861, 'where all': 1309, 'all the': 31, 'the cells': 1059, 'cells have': 195, 'have glass': 427, 'glass fronts': 384, 'fronts and': 369, 'and face': 48, 'face inwards': 314, 'inwards so': 499, 'so privacy': 970, 'privacy is': 862, 'not high': 718, 'high on': 445, 'on the': 767, 'the agenda': 1053, 'agenda em': 24, 'em city': 289, 'city is': 207, 'is home': 512, 'home to': 452, 'to many': 1190, 'many aryans': 630, 'aryans muslims': 96, 'muslims gangstas': 687, 'gangstas latinos': 375, 'latinos christians': 583, 'christians italians': 205, 'italians irish': 552, 'irish and': 500, 'and more': 60, 'more so': 673, 'so scuffles': 971, 'scuffles death': 919, 'death stares': 243, 'stares dodgy': 989, 'dodgy dealings': 271, 'dealings and': 242, 'and shady': 65, 'shady agreements': 945, 'agreements are': 25, 'are never': 88, 'never far': 702, 'far away': 320, 'away br': 113, 'br would': 164, 'would say': 1358, 'say the': 910, 'the main': 1084, 'main appeal': 621, 'appeal of': 81, 'the show': 1113, 'show is': 951, 'is due': 506, 'due to': 284, 'the fact': 1070, 'fact that': 315, 'that it': 1040, 'it goes': 539, 'goes where': 388, 'where other': 1311, 'other shows': 800, 'shows wouldn': 956, 'wouldn dare': 1361, 'dare forget': 236, 'forget pretty': 362, 'pretty pictures': 856, 'pictures painted': 836, 'painted for': 818, 'for mainstream': 355, 'mainstream audiences': 623, 'audiences forget': 109, 'forget charm': 361, 'charm forget': 200, 'forget romance': 363, 'romance oz': 901, 'oz doesn': 811, 'doesn mess': 272, 'mess around': 658, 'around the': 94, 'first episode': 342, 'episode ever': 297, 'ever saw': 301, 'saw struck': 908, 'me as': 646, 'as so': 103, 'so nasty': 969, 'nasty it': 699, 'it was': 548, 'was surreal': 1264, 'surreal couldn': 1015, 'couldn say': 230, 'say was': 911, 'was ready': 1263, 'ready for': 876, 'for it': 353, 'it but': 535, 'but as': 172, 'as watched': 106, 'watched more': 1273, 'more developed': 668, 'developed taste': 253, 'taste for': 1020, 'for oz': 357, 'oz and': 809, 'and got': 51, 'got accustomed': 392, 'accustomed to': 14, 'the high': 1079, 'high levels': 444, 'levels of': 591, 'of graphic': 733, 'graphic violence': 396, 'violence not': 1251, 'not just': 719, 'just violence': 569, 'violence but': 1249, 'but injustice': 174, 'injustice crooked': 491, 'crooked guards': 234, 'guards who': 404, 'who ll': 1320, 'be sold': 131, 'sold out': 974, 'out for': 803, 'for nickel': 356, 'nickel inmates': 709, 'inmates who': 493, 'll kill': 609, 'kill on': 575, 'on order': 765, 'order and': 794, 'and get': 49, 'get away': 376, 'away with': 114, 'with it': 1329, 'it well': 549, 'well mannered': 1296, 'mannered middle': 629, 'middle class': 661, 'class inmates': 208, 'inmates being': 492, 'being turned': 139, 'turned into': 1222, 'into prison': 498, 'prison bitches': 859, 'bitches due': 150, 'to their': 1200, 'their lack': 1130, 'lack of': 580, 'of street': 744, 'street skills': 999, 'skills or': 962, 'or prison': 790, 'prison experience': 860, 'experience watching': 309, 'watching oz': 1277, 'oz you': 814, 'you may': 1377, 'may become': 643, 'become comfortable': 135, 'comfortable with': 219, 'with what': 1338, 'what is': 1303, 'is uncomfortable': 528, 'uncomfortable viewing': 1225, 'viewing thats': 1247, 'thats if': 1048, 'if you': 470, 'you can': 1371, 'can get': 185, 'get in': 377, 'in touch': 486, 'touch with': 1213, 'with your': 1339, 'your darker': 1383, 'darker side': 238, 'wonderful little': 1343, 'little production': 604, 'production br': 866, 'the filming': 1074, 'filming technique': 337, 'technique is': 1023, 'is very': 529, 'very unassuming': 1245, 'unassuming very': 1224, 'very old': 1244, 'old time': 760, 'time bbc': 1172, 'bbc fashion': 123, 'fashion and': 323, 'and gives': 50, 'gives comforting': 382, 'comforting and': 220, 'and sometimes': 66, 'sometimes discomforting': 980, 'discomforting sense': 266, 'sense of': 937, 'of realism': 741, 'realism to': 879, 'the entire': 1068, 'entire piece': 295, 'piece br': 837, 'the actors': 1052, 'actors are': 18, 'are extremely': 85, 'extremely well': 312, 'well chosen': 1294, 'chosen michael': 204, 'michael sheen': 660, 'sheen not': 947, 'not only': 721, 'only has': 778, 'has got': 423, 'got all': 393, 'the polari': 1100, 'polari but': 849, 'but he': 173, 'he has': 435, 'has all': 421, 'the voices': 1119, 'voices down': 1255, 'down pat': 275, 'pat too': 823, 'too you': 1211, 'can truly': 187, 'truly see': 1218, 'see the': 930, 'the seamless': 1110, 'seamless editing': 923, 'editing guided': 286, 'guided by': 406, 'by the': 182, 'the references': 1106, 'references to': 884, 'to williams': 1206, 'williams diary': 1323, 'diary entries': 257, 'entries not': 296, 'only is': 779, 'is it': 513, 'well worth': 1298, 'worth the': 1354, 'the watching': 1120, 'watching but': 1274, 'but it': 175, 'is terrificly': 524, 'terrificly written': 1027, 'written and': 1365, 'and performed': 63, 'performed piece': 832, 'piece masterful': 838, 'masterful production': 635, 'production about': 865, 'about one': 8, 'the great': 1076, 'great master': 399, 'master of': 634, 'of comedy': 730, 'comedy and': 215, 'and his': 54, 'his life': 449, 'life br': 593, 'the realism': 1105, 'realism really': 878, 'really comes': 881, 'comes home': 218, 'home with': 453, 'with the': 1337, 'the little': 1083, 'little things': 605, 'things the': 1154, 'the fantasy': 1072, 'fantasy of': 319, 'the guard': 1077, 'guard which': 403, 'which rather': 1313, 'rather than': 873, 'than use': 1033, 'use the': 1238, 'the traditional': 1118, 'traditional dream': 1214, 'dream techniques': 280, 'techniques remains': 1024, 'remains solid': 888, 'solid then': 975, 'then disappears': 1135, 'disappears it': 263, 'it plays': 543, 'plays on': 843, 'on our': 766, 'our knowledge': 801, 'knowledge and': 579, 'and our': 62, 'our senses': 802, 'senses particularly': 938, 'particularly with': 822, 'the scenes': 1109, 'scenes concerning': 915, 'concerning orton': 223, 'orton and': 797, 'and halliwell': 52, 'halliwell and': 412, 'and the': 70, 'the sets': 1111, 'sets particularly': 942, 'particularly of': 821, 'of their': 746, 'their flat': 1129, 'flat with': 347, 'with halliwell': 1327, 'halliwell murals': 413, 'murals decorating': 686, 'decorating every': 248, 'every surface': 302, 'surface are': 1014, 'are terribly': 91, 'terribly well': 1026, 'well done': 1295, 'thought this': 1168, 'this was': 1165, 'was wonderful': 1267, 'wonderful way': 1344, 'way to': 1284, 'to spend': 1198, 'spend time': 985, 'time on': 1178, 'on too': 768, 'too hot': 1210, 'hot summer': 456, 'summer weekend': 1011, 'weekend sitting': 1292, 'sitting in': 960, 'the air': 1054, 'air conditioned': 26, 'conditioned theater': 224, 'theater and': 1128, 'and watching': 75, 'watching light': 1276, 'light hearted': 594, 'hearted comedy': 436, 'comedy the': 216, 'the plot': 1099, 'plot is': 844, 'is simplistic': 520, 'simplistic but': 958, 'but the': 179, 'the dialogue': 1064, 'dialogue is': 256, 'is witty': 532, 'witty and': 1341, 'the characters': 1060, 'characters are': 197, 'are likable': 87, 'likable even': 595, 'even the': 300, 'the well': 1122, 'well bread': 1293, 'bread suspected': 165, 'suspected serial': 1016, 'serial killer': 939, 'killer while': 577, 'while some': 1316, 'some may': 977, 'may be': 642, 'be disappointed': 125, 'disappointed when': 264, 'when they': 1307, 'they realize': 1150, 'realize this': 880, 'not match': 720, 'match point': 636, 'point risk': 847, 'risk addiction': 899, 'addiction thought': 19, 'thought it': 1167, 'was proof': 1262, 'proof that': 867, 'that woody': 1047, 'woody allen': 1345, 'allen is': 35, 'is still': 523, 'still fully': 995, 'fully in': 371, 'in control': 476, 'control of': 229, 'the style': 1114, 'style many': 1002, 'many of': 632, 'of us': 751, 'us have': 1234, 'have grown': 428, 'grown to': 402, 'to love': 1188, 'love br': 614, 'br this': 162, 'was the': 1265, 'the most': 1087, 'most laughed': 675, 'laughed at': 584, 'at one': 108, 'of woody': 753, 'woody comedies': 1346, 'comedies in': 214, 'in years': 489, 'years dare': 1366, 'dare say': 237, 'say decade': 909, 'decade while': 244, 'while ve': 1318, 've never': 1241, 'never been': 701, 'been impressed': 137, 'impressed with': 474, 'with scarlet': 1334, 'scarlet johanson': 914, 'johanson in': 562, 'in this': 485, 'this she': 1162, 'she managed': 946, 'managed to': 628, 'to tone': 1201, 'tone down': 1209, 'down her': 274, 'her sexy': 440, 'sexy image': 944, 'image and': 472, 'and jumped': 58, 'jumped right': 563, 'right into': 898, 'into average': 497, 'average but': 110, 'but spirited': 177, 'spirited young': 986, 'young woman': 1382, 'woman br': 1342, 'this may': 1159, 'may not': 644, 'not be': 715, 'be the': 133, 'the crown': 1063, 'crown jewel': 235, 'jewel of': 561, 'of his': 736, 'his career': 447, 'career but': 188, 'was wittier': 1266, 'wittier than': 1340, 'than devil': 1029, 'devil wears': 254, 'wears prada': 1290, 'prada and': 853, 'more interesting': 670, 'interesting than': 496, 'than superman': 1032, 'superman great': 1012, 'great comedy': 398, 'comedy to': 217, 'to go': 1186, 'go see': 385, 'see with': 931, 'with friends': 1325, 'basically there': 122, 'there family': 1140, 'family where': 318, 'where little': 1310, 'little boy': 603, 'boy jake': 155, 'jake thinks': 559, 'thinks there': 1155, 'there zombie': 1141, 'zombie in': 1385, 'in his': 478, 'his closet': 448, 'closet his': 210, 'his parents': 451, 'parents are': 819, 'are fighting': 86, 'fighting all': 327, 'the time': 1117, 'time br': 1173, 'this movie': 1160, 'movie is': 681, 'is slower': 521, 'slower than': 964, 'than soap': 1031, 'soap opera': 973, 'opera and': 783, 'and suddenly': 69, 'suddenly jake': 1010, 'jake decides': 557, 'decides to': 246, 'to become': 1183, 'become rambo': 136, 'rambo and': 871, 'and kill': 59, 'kill the': 576, 'the zombie': 1127, 'zombie br': 1384, 'br ok': 159, 'ok first': 757, 'first of': 343, 'of all': 728, 'all when': 34, 'when you': 1308, 'you re': 1379, 're going': 874, 'going to': 389, 'to make': 1189, 'make film': 624, 'film you': 336, 'you must': 1378, 'must decide': 688, 'decide if': 245, 'if its': 466, 'its thriller': 556, 'thriller or': 1169, 'or drama': 786, 'drama as': 277, 'as drama': 98, 'drama the': 278, 'the movie': 1089, 'is watchable': 531, 'watchable parents': 1271, 'are divorcing': 84, 'divorcing arguing': 268, 'arguing like': 93, 'like in': 597, 'in real': 481, 'real life': 877, 'life and': 592, 'and then': 71, 'then we': 1138, 'we have': 1287, 'have jake': 429, 'jake with': 560, 'with his': 1328, 'closet which': 211, 'which totally': 1315, 'totally ruins': 1212, 'ruins all': 904, 'the film': 1073, 'film expected': 330, 'expected to': 308, 'to see': 1197, 'see boogeyman': 926, 'boogeyman similar': 152, 'similar movie': 957, 'movie and': 678, 'and instead': 55, 'instead watched': 495, 'watched drama': 1272, 'drama with': 279, 'with some': 1335, 'some meaningless': 978, 'meaningless thriller': 654, 'thriller spots': 1170, 'spots br': 987, 'br out': 160, 'out of': 804, 'of 10': 727, '10 just': 0, 'just for': 565, 'well playing': 1297, 'playing parents': 842, 'parents descent': 820, 'descent dialogs': 251, 'dialogs as': 255, 'as for': 99, 'the shots': 1112, 'shots with': 948, 'with jake': 1330, 'jake just': 558, 'just ignore': 566, 'ignore them': 471, 'petter mattei': 834, 'mattei love': 639, 'love in': 615, 'time of': 1177, 'of money': 738, 'money is': 666, 'is visually': 530, 'visually stunning': 1253, 'stunning film': 1001, 'film to': 335, 'to watch': 1203, 'watch mr': 1270, 'mr mattei': 685, 'mattei offers': 640, 'offers us': 755, 'us vivid': 1235, 'vivid portrait': 1254, 'portrait about': 850, 'about human': 7, 'human relations': 461, 'relations this': 887, 'is movie': 515, 'movie that': 683, 'that seems': 1044, 'seems to': 932, 'to be': 1182, 'be telling': 132, 'telling us': 1025, 'us what': 1236, 'what money': 1304, 'money power': 667, 'power and': 852, 'and success': 68, 'success do': 1006, 'do to': 270, 'to people': 1193, 'people in': 826, 'the different': 1065, 'different situations': 259, 'situations we': 961, 'we encounter': 1286, 'encounter br': 291, 'this being': 1156, 'being variation': 140, 'variation on': 1239, 'the arthur': 1055, 'arthur schnitzler': 95, 'schnitzler play': 917, 'play about': 840, 'about the': 11, 'the same': 1108, 'same theme': 907, 'theme the': 1134, 'the director': 1066, 'director transfers': 262, 'transfers the': 1215, 'the action': 1051, 'action to': 17, 'the present': 1102, 'present time': 855, 'time new': 1176, 'new york': 705, 'york where': 1369, 'all these': 32, 'these different': 1143, 'different characters': 258, 'characters meet': 199, 'meet and': 656, 'and connect': 46, 'connect each': 225, 'each one': 285, 'one is': 773, 'is connected': 504, 'connected in': 226, 'in one': 480, 'one way': 777, 'way or': 1282, 'or another': 784, 'another to': 77, 'the next': 1090, 'next person': 706, 'person but': 833, 'but no': 176, 'no one': 712, 'one seems': 776, 'to know': 1187, 'know the': 578, 'the previous': 1103, 'previous point': 858, 'point of': 846, 'of contact': 731, 'contact stylishly': 227, 'stylishly the': 1003, 'film has': 331, 'has sophisticated': 425, 'sophisticated luxurious': 982, 'luxurious look': 619, 'look we': 611, 'we are': 1285, 'are taken': 90, 'taken to': 1018, 'see how': 928, 'how these': 459, 'these people': 1144, 'people live': 827, 'live and': 606, 'the world': 1124, 'world they': 1352, 'they live': 1149, 'live in': 607, 'in their': 484, 'their own': 1131, 'own habitat': 807, 'habitat br': 409, 'the only': 1092, 'only thing': 782, 'thing one': 1151, 'one gets': 771, 'gets out': 379, 'these souls': 1145, 'souls in': 983, 'the picture': 1098, 'picture is': 835, 'different stages': 260, 'stages of': 988, 'of loneliness': 737, 'loneliness each': 610, 'one inhabits': 772, 'inhabits big': 490, 'big city': 148, 'not exactly': 716, 'exactly the': 305, 'the best': 1057, 'best place': 146, 'place in': 839, 'in which': 488, 'which human': 1312, 'relations find': 886, 'find sincere': 340, 'sincere fulfillment': 959, 'fulfillment as': 370, 'as one': 102, 'one discerns': 769, 'discerns is': 265, 'the case': 1058, 'case with': 190, 'with most': 1332, 'most of': 676, 'the people': 1096, 'people we': 828, 'the acting': 1050, 'acting is': 15, 'is good': 509, 'good under': 391, 'under mr': 1227, 'mattei direction': 637, 'direction steve': 261, 'steve buscemi': 993, 'buscemi rosario': 171, 'rosario dawson': 903, 'dawson carol': 241, 'carol kane': 189, 'kane michael': 570, 'michael imperioli': 659, 'imperioli adrian': 473, 'adrian grenier': 20, 'grenier and': 400, 'the rest': 1107, 'rest of': 892, 'the talented': 1115, 'talented cast': 1019, 'cast make': 192, 'make these': 625, 'these characters': 1142, 'characters come': 198, 'come alive': 212, 'alive br': 29, 'br we': 163, 'we wish': 1289, 'wish mr': 1324, 'mattei good': 638, 'good luck': 390, 'luck and': 617, 'and await': 44, 'await anxiously': 111, 'anxiously for': 78, 'for his': 352, 'his next': 450, 'next work': 707, 'probably my': 863, 'my all': 691, 'all time': 33, 'time favorite': 1174, 'favorite movie': 324, 'movie story': 682, 'story of': 997, 'of selflessness': 743, 'selflessness sacrifice': 936, 'sacrifice and': 905, 'and dedication': 47, 'dedication to': 249, 'to noble': 1192, 'noble cause': 714, 'cause but': 194, 'it not': 542, 'not preachy': 722, 'preachy or': 854, 'or boring': 785, 'boring it': 154, 'it just': 541, 'just never': 567, 'never gets': 703, 'gets old': 378, 'old despite': 758, 'despite my': 252, 'my having': 693, 'having seen': 433, 'seen it': 934, 'it some': 545, 'some 15': 976, '15 or': 2, 'or more': 788, 'more times': 674, 'times in': 1179, 'the last': 1082, 'last 25': 582, '25 years': 4, 'years paul': 1367, 'paul lukas': 824, 'lukas performance': 618, 'performance brings': 829, 'brings tears': 169, 'tears to': 1021, 'to my': 1191, 'my eyes': 692, 'eyes and': 313, 'and bette': 45, 'bette davis': 147, 'davis in': 240, 'of her': 735, 'her very': 441, 'very few': 1243, 'few truly': 326, 'truly sympathetic': 1219, 'sympathetic roles': 1017, 'roles is': 900, 'is delight': 505, 'delight the': 250, 'the kids': 1081, 'kids are': 574, 'are as': 83, 'as grandma': 100, 'grandma says': 395, 'says more': 913, 'more like': 671, 'like dressed': 596, 'dressed up': 281, 'up midgets': 1232, 'midgets than': 662, 'than children': 1028, 'children but': 202, 'but that': 178, 'that only': 1042, 'only makes': 780, 'makes them': 626, 'them more': 1133, 'more fun': 669, 'fun to': 372, 'watch and': 1269, 'the mother': 1088, 'mother slow': 677, 'slow awakening': 963, 'awakening to': 112, 'to what': 1205, 'what happening': 1302, 'happening in': 416, 'world and': 1350, 'and under': 73, 'under her': 1226, 'her own': 439, 'own roof': 808, 'roof is': 902, 'is believable': 501, 'believable and': 141, 'and startling': 67, 'startling if': 991, 'if had': 465, 'had dozen': 411, 'dozen thumbs': 276, 'thumbs they': 1171, 'they all': 1146, 'all be': 30, 'be up': 134, 'up for': 1231, 'for this': 359, 'sure would': 1013, 'would like': 1357, 'like to': 600, 'see resurrection': 929, 'resurrection of': 893, 'of up': 750, 'up dated': 1230, 'dated seahunt': 239, 'seahunt series': 921, 'series with': 940, 'the tech': 1116, 'tech they': 1022, 'they have': 1148, 'have today': 432, 'today it': 1208, 'it would': 551, 'would bring': 1356, 'bring back': 168, 'back the': 117, 'the kid': 1080, 'kid excitement': 573, 'excitement in': 307, 'in me': 479, 'me grew': 649, 'grew up': 401, 'up on': 1233, 'on black': 762, 'black and': 151, 'and white': 76, 'white tv': 1319, 'tv and': 1223, 'and seahunt': 64, 'seahunt with': 922, 'with gunsmoke': 1326, 'gunsmoke were': 407, 'were my': 1300, 'my hero': 694, 'hero every': 443, 'every week': 304, 'week you': 1291, 'you have': 1373, 'have my': 431, 'my vote': 697, 'vote for': 1256, 'for comeback': 351, 'comeback of': 213, 'of new': 739, 'new sea': 704, 'sea hunt': 920, 'hunt we': 463, 'we need': 1288, 'need change': 700, 'change of': 196, 'of pace': 740, 'pace in': 815, 'in tv': 487, 'and this': 72, 'this would': 1166, 'would work': 1359, 'work for': 1349, 'for world': 360, 'world of': 1351, 'of under': 749, 'under water': 1228, 'water adventure': 1279, 'adventure oh': 21, 'oh by': 756, 'the way': 1121, 'way thank': 1283, 'thank you': 1034, 'you for': 1372, 'for an': 350, 'an outlet': 43, 'outlet like': 806, 'like this': 599, 'this to': 1164, 'to view': 1202, 'view many': 1246, 'many viewpoints': 633, 'viewpoints about': 1248, 'about tv': 13, 'the many': 1085, 'many movies': 631, 'movies so': 684, 'so any': 965, 'any ole': 79, 'ole way': 761, 'way believe': 1280, 'believe ve': 144, 've got': 1240, 'got what': 394, 'what wanna': 1305, 'wanna say': 1257, 'say would': 912, 'would be': 1355, 'be nice': 129, 'nice to': 708, 'to read': 1195, 'read some': 875, 'some more': 979, 'more plus': 672, 'plus points': 845, 'points about': 848, 'about sea': 10, 'hunt if': 462, 'if my': 467, 'my rhymes': 696, 'rhymes would': 895, 'be 10': 124, '10 lines': 1, 'lines would': 602, 'would you': 1360, 'you let': 1374, 'let me': 589, 'me submit': 651, 'submit or': 1004, 'or leave': 787, 'leave me': 587, 'me out': 650, 'out to': 805, 'be in': 127, 'in doubt': 477, 'doubt and': 273, 'and have': 53, 'have me': 430, 'me to': 653, 'to quit': 1194, 'quit if': 870, 'if this': 469, 'is so': 522, 'so then': 972, 'then must': 1136, 'must go': 690, 'go so': 386, 'so lets': 968, 'lets do': 590, 'do it': 269, 'show was': 955, 'was an': 1258, 'an amazing': 40, 'amazing fresh': 39, 'fresh innovative': 366, 'innovative idea': 494, 'idea in': 464, 'the 70': 1049, '70 when': 5, 'when it': 1306, 'it first': 537, 'first aired': 341, 'aired the': 28, 'first or': 344, 'or years': 793, 'years were': 1368, 'were brilliant': 1299, 'brilliant but': 167, 'but things': 180, 'things dropped': 1153, 'dropped off': 282, 'off after': 754, 'after that': 22, 'that by': 1036, 'by 1990': 181, '1990 the': 3, 'was not': 1261, 'not really': 723, 'really funny': 882, 'funny anymore': 373, 'anymore and': 80, 'and it': 57, 'it continued': 536, 'continued its': 228, 'its decline': 554, 'decline further': 247, 'further to': 374, 'the complete': 1062, 'complete waste': 222, 'waste of': 1268, 'of time': 748, 'time it': 1175, 'is today': 526, 'today br': 1207, 'it truly': 547, 'truly disgraceful': 1216, 'disgraceful how': 267, 'how far': 458, 'far this': 322, 'show has': 950, 'has fallen': 422, 'fallen the': 317, 'the writing': 1126, 'writing is': 1364, 'is painfully': 518, 'painfully bad': 817, 'bad the': 120, 'the performances': 1097, 'performances are': 831, 'are almost': 82, 'almost as': 36, 'as bad': 97, 'bad if': 118, 'if not': 468, 'not for': 717, 'the mildly': 1086, 'mildly entertaining': 663, 'entertaining respite': 294, 'respite of': 891, 'the guest': 1078, 'guest hosts': 405, 'hosts this': 455, 'show probably': 952, 'probably wouldn': 864, 'wouldn still': 1362, 'still be': 994, 'be on': 130, 'air find': 27, 'find it': 339, 'it so': 544, 'so hard': 967, 'hard to': 418, 'to believe': 1184, 'believe that': 143, 'that the': 1046, 'same creator': 906, 'creator that': 232, 'that hand': 1038, 'hand selected': 414, 'selected the': 935, 'the original': 1093, 'original cast': 795, 'cast also': 191, 'also chose': 38, 'chose the': 203, 'the band': 1056, 'band of': 121, 'of hacks': 734, 'hacks that': 410, 'that followed': 1037, 'followed how': 349, 'how can': 457, 'can one': 186, 'one recognize': 775, 'recognize such': 883, 'such brilliance': 1007, 'brilliance and': 166, 'then see': 1137, 'see fit': 927, 'fit to': 346, 'to replace': 1196, 'replace it': 889, 'it with': 550, 'with such': 1336, 'such mediocrity': 1009, 'mediocrity felt': 655, 'felt must': 325, 'must give': 689, 'give stars': 380, 'stars out': 990, 'of respect': 742, 'respect for': 890, 'cast that': 193, 'that made': 1041, 'made this': 620, 'show such': 954, 'such huge': 1008, 'huge success': 460, 'success as': 1005, 'as it': 101, 'is now': 517, 'now the': 725, 'is just': 514, 'just awful': 564, 'awful can': 115, 'can believe': 184, 'believe it': 142, 'it still': 546, 'still on': 996, 'encouraged by': 292, 'the positive': 1101, 'positive comments': 851, 'comments about': 221, 'about this': 12, 'this film': 1157, 'film on': 333, 'on here': 764, 'here was': 442, 'was looking': 1260, 'looking forward': 612, 'forward to': 364, 'to watching': 1204, 'watching this': 1278, 'film bad': 328, 'bad mistake': 119, 'mistake ve': 664, 've seen': 1242, 'seen 950': 933, '950 films': 6, 'films and': 338, 'is truly': 527, 'truly one': 1217, 'the worst': 1125, 'worst of': 1353, 'of them': 747, 'them it': 1132, 'it awful': 533, 'awful in': 116, 'in almost': 475, 'almost every': 37, 'every way': 303, 'way editing': 1281, 'editing pacing': 287, 'pacing storyline': 816, 'storyline acting': 998, 'acting soundtrack': 16, 'soundtrack the': 984, 'film only': 334, 'only song': 781, 'song lame': 981, 'lame country': 581, 'country tune': 231, 'tune is': 1221, 'is played': 519, 'played no': 841, 'no less': 711, 'less than': 588, 'than four': 1030, 'four times': 365, 'times the': 1180, 'film looks': 332, 'looks cheap': 613, 'cheap and': 201, 'and nasty': 61, 'nasty and': 698, 'and is': 56, 'is boring': 502, 'boring in': 153, 'the extreme': 1069, 'extreme rarely': 311, 'rarely have': 872, 'have been': 426, 'been so': 138, 'so happy': 966, 'happy to': 417, 'the end': 1067, 'end credits': 293, 'credits of': 233, 'of film': 732, 'film br': 329, 'that prevents': 1043, 'prevents me': 857, 'me giving': 648, 'giving this': 383, 'this score': 1161, 'score is': 918, 'is harvey': 511, 'harvey keitel': 420, 'keitel while': 572, 'while this': 1317, 'is far': 508, 'far from': 321, 'from his': 367, 'his best': 446, 'best performance': 145, 'performance he': 830, 'he at': 434, 'at least': 107, 'least seems': 586, 'be making': 128, 'making bit': 627, 'bit of': 149, 'of an': 729, 'an effort': 41, 'effort one': 288, 'one for': 770, 'for keitel': 354, 'keitel obsessives': 571, 'obsessives only': 726, 'you like': 1375, 'like original': 598, 'original gut': 796, 'gut wrenching': 408, 'wrenching laughter': 1363, 'laughter you': 585, 'you will': 1380, 'will like': 1321, 'movie if': 680, 'you are': 1370, 'are young': 92, 'young or': 1381, 'or old': 789, 'old then': 759, 'then you': 1139, 'will love': 1322, 'love this': 616, 'movie hell': 679, 'hell even': 438, 'even my': 299, 'my mom': 695, 'mom liked': 665, 'liked it': 601, 'it br': 534, 'br great': 157, 'great camp': 397}\n",
            "BoW representation for review 0:  [[0 0 0 ... 1 0 0]]\n",
            "BoW representation for review 1:  [[0 0 0 ... 0 0 0]]\n",
            "BoW representation for review 2:  [[0 0 0 ... 0 0 0]]\n",
            "BoW representation for review 3:  [[1 0 0 ... 0 1 1]]\n",
            "BoW representation for review 4:  [[0 0 0 ... 0 0 0]]\n",
            "BoW representation for review 5:  [[0 0 1 ... 0 0 0]]\n",
            "BoW representation for review 6:  [[0 1 0 ... 0 0 0]]\n",
            "BoW representation for review 7:  [[0 0 0 ... 0 0 0]]\n",
            "BoW representation for review 8:  [[0 0 0 ... 0 0 0]]\n",
            "BoW representation for review 9:  [[0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For part 2, the vocabulary has 1386 elements\n",
        "print(len(count_vect.vocabulary_))\n",
        "print(bow_rep.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOQkrrZ49W8Z",
        "outputId": "88e52226-e72d-42ac-c27c-b13688ff8cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1386\n",
            "(10, 1386)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2B\n",
        "# Part 3\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "bow_rep_tfidf = tfidf.fit_transform(imdb_review_list)\n",
        "\n",
        "#TFIDF representation for all documents in our corpus\n",
        "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray())\n",
        "print(\"\\n\")\n",
        "\n",
        "# df_tfidf = pd.DataFrame(bow_rep_tfidf.toarray(),columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kci47XdfN5ZJ",
        "outputId": "945870e8-726b-475c-b4ae-dca4c6a18e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF representation for all documents in our corpus\n",
            " [[0.         0.         0.         ... 0.         0.04922065 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.07220094 0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.07095822 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.16036163 0.         0.        ]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2B\n",
        "# Part 4\n",
        "'''\n",
        "Two reasons that make one-hot encoding rarely use at the moment is because:\n",
        "1. It can lead to high dimensionalty, as the size of the vector depends on the\n",
        "number of categories in the variable. For text data, this can get extrememly large\n",
        "due to there are many possible words in a vocabulary. As a consequence, the model\n",
        "will be slower to train and also increase the chance of overfitting.\n",
        "2. It can lead to sparse data as most of the elements will mainly be 0, with the\n",
        "exception of some value of 1. As a result, when we need to deal with irrelevant\n",
        "information, it can reduce the efficiency of the model.\n",
        "\n",
        "Other vector representations that also share the issues:\n",
        "- Bag-of-words: This one can also result in high dimensionality and sparsity,\n",
        "when the vocabulary is large and the documents it contains are short.\n",
        "- TF-IDF: This one also face high dimensionality and sparsity, as it uses the same\n",
        "size of vector as bag-of-words. On top of that, it also assigns low weights to\n",
        "common words that might not be informative.\n",
        "'''"
      ],
      "metadata": {
        "id": "NiacJXFeOZS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3"
      ],
      "metadata": {
        "id": "5kPcfd1jWu8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3 (20 marks)\n",
        "\n",
        "A) Assume you have to make your own training set to pass as input to Word2Vec instead of\n",
        "just a list of lists of text. Write a windowing function that can take text as input and make 1\n",
        "training set in the CBOW format as seen in slide 5 of the Word2Vec slides. Make the window\n",
        "size a parameter of your function that can be changed. Test your code with file nlptext.txt.\n",
        "Note that you do not have to train a Word2Vec model. (6 marks)\n",
        "\n",
        "B) Suppose you are training a Word2Vec model with the text in part A. (10 marks)\n",
        "1. Given that you set your hyperparameters for Word2Vec as vector_size=50, window=3,\n",
        "and min_count=5, what will the dimensions of the embedding matrix be and why (i.e.\n",
        "what do the dimensions of the embedding matrix represent)? What will the dimensions\n",
        "be for the context matrix? Assume you do not do any preprocessing of the text other\n",
        "than simple word tokenization with NLTK.\n",
        "2. How would the answer to these questions change if vector_size=50, window=5, and\n",
        "min_count=5?\n",
        "3. How would the answer to these questions change if vector_size=100, window=3, and\n",
        "min_count=5?\n",
        "\n",
        "C) What is the purpose of negative sampling when training Word2Vec? (2 marks)\n",
        "\n",
        "D) Why does CBOW train faster than Skip-gram? Hint: Think about how each model is\n",
        "updated. (2 marks)"
      ],
      "metadata": {
        "id": "zcpuTay_OpDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3A\n",
        "with open(\"/content/nlptext.txt\", 'r') as f:\n",
        "  nlptext = f.read().split(\"\\n\\n\")\n",
        "\n",
        "print(len(nlptext))\n",
        "\n",
        "# Add nlptext to 1 sentence\n",
        "nlptext_1_sentence = \"\"\n",
        "for i in range(len(nlptext)):\n",
        "  nlptext_1_sentence += nlptext[i]\n",
        "\n",
        "print(nlptext_1_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rc5fB6TOrQ5",
        "outputId": "1ac181d1-8c33-42e3-dd01-ad029bfe6102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Most natural language processing systems were based on complex sets of hand-written rules. There was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics, whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part of speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors, and produce more reliable results when integrated into a larger system comprising multiple subtasks.Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available, which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to make my own training set\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# training_set = []\n",
        "# tokens = word_tokenize(nlptext_1_sentence)\n",
        "\n",
        "# # The windowing function I follow the approach from Word2Vec_Practice notebook from D2L\n",
        "# def windowing(training_set, tokens, window_size):\n",
        "#   for i, word in enumerate(tokens):\n",
        "#     for j in range(i-window_size, i+window_size+1):\n",
        "#       if i != j and 0 <= j < len(tokens):\n",
        "#         training_set.append((word, tokens[j]))\n",
        "#   return training_set\n",
        "\n",
        "# Function to make my own training set\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "training_set = []\n",
        "tokens = word_tokenize(nlptext_1_sentence)\n",
        "\n",
        "# The windowing function I follow the approach from Word2Vec_Practice notebook from D2L\n",
        "def windowing(training_set, tokens, window_size):\n",
        "  for i, word in enumerate(tokens):\n",
        "    temp_arr = [] # This is for the context\n",
        "    for j in range(i-window_size, i+window_size+1):\n",
        "      if i != j and 0 <= j < len(tokens):\n",
        "        temp_arr.append(tokens[j])\n",
        "    training_set.append((temp_arr, word))\n",
        "  return training_set"
      ],
      "metadata": {
        "id": "9GEu_FLLS_CW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806ec275-1756-4a05-fa9a-f4b371166aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess or not (ask on discussion)\n",
        "training_set = windowing(training_set, tokens, window_size=2) # Set the window size = 2 as an example\n",
        "training_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdx2oL4RJoXK",
        "outputId": "b2176d85-405d-484e-f753-0694f5821980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['natural', 'language'], 'Most'),\n",
              " (['Most', 'language', 'processing'], 'natural'),\n",
              " (['Most', 'natural', 'processing', 'systems'], 'language'),\n",
              " (['natural', 'language', 'systems', 'were'], 'processing'),\n",
              " (['language', 'processing', 'were', 'based'], 'systems'),\n",
              " (['processing', 'systems', 'based', 'on'], 'were'),\n",
              " (['systems', 'were', 'on', 'complex'], 'based'),\n",
              " (['were', 'based', 'complex', 'sets'], 'on'),\n",
              " (['based', 'on', 'sets', 'of'], 'complex'),\n",
              " (['on', 'complex', 'of', 'hand-written'], 'sets'),\n",
              " (['complex', 'sets', 'hand-written', 'rules'], 'of'),\n",
              " (['sets', 'of', 'rules', '.'], 'hand-written'),\n",
              " (['of', 'hand-written', '.', 'There'], 'rules'),\n",
              " (['hand-written', 'rules', 'There', 'was'], '.'),\n",
              " (['rules', '.', 'was', 'a'], 'There'),\n",
              " (['.', 'There', 'a', 'revolution'], 'was'),\n",
              " (['There', 'was', 'revolution', 'in'], 'a'),\n",
              " (['was', 'a', 'in', 'natural'], 'revolution'),\n",
              " (['a', 'revolution', 'natural', 'language'], 'in'),\n",
              " (['revolution', 'in', 'language', 'processing'], 'natural'),\n",
              " (['in', 'natural', 'processing', 'with'], 'language'),\n",
              " (['natural', 'language', 'with', 'the'], 'processing'),\n",
              " (['language', 'processing', 'the', 'introduction'], 'with'),\n",
              " (['processing', 'with', 'introduction', 'of'], 'the'),\n",
              " (['with', 'the', 'of', 'machine'], 'introduction'),\n",
              " (['the', 'introduction', 'machine', 'learning'], 'of'),\n",
              " (['introduction', 'of', 'learning', 'algorithms'], 'machine'),\n",
              " (['of', 'machine', 'algorithms', 'for'], 'learning'),\n",
              " (['machine', 'learning', 'for', 'language'], 'algorithms'),\n",
              " (['learning', 'algorithms', 'language', 'processing'], 'for'),\n",
              " (['algorithms', 'for', 'processing', '.'], 'language'),\n",
              " (['for', 'language', '.', 'This'], 'processing'),\n",
              " (['language', 'processing', 'This', 'was'], '.'),\n",
              " (['processing', '.', 'was', 'due'], 'This'),\n",
              " (['.', 'This', 'due', 'to'], 'was'),\n",
              " (['This', 'was', 'to', 'both'], 'due'),\n",
              " (['was', 'due', 'both', 'the'], 'to'),\n",
              " (['due', 'to', 'the', 'steady'], 'both'),\n",
              " (['to', 'both', 'steady', 'increase'], 'the'),\n",
              " (['both', 'the', 'increase', 'in'], 'steady'),\n",
              " (['the', 'steady', 'in', 'computational'], 'increase'),\n",
              " (['steady', 'increase', 'computational', 'power'], 'in'),\n",
              " (['increase', 'in', 'power', '('], 'computational'),\n",
              " (['in', 'computational', '(', 'see'], 'power'),\n",
              " (['computational', 'power', 'see', 'Moore'], '('),\n",
              " (['power', '(', 'Moore', \"'s\"], 'see'),\n",
              " (['(', 'see', \"'s\", 'law'], 'Moore'),\n",
              " (['see', 'Moore', 'law', ')'], \"'s\"),\n",
              " (['Moore', \"'s\", ')', 'and'], 'law'),\n",
              " ([\"'s\", 'law', 'and', 'the'], ')'),\n",
              " (['law', ')', 'the', 'gradual'], 'and'),\n",
              " ([')', 'and', 'gradual', 'lessening'], 'the'),\n",
              " (['and', 'the', 'lessening', 'of'], 'gradual'),\n",
              " (['the', 'gradual', 'of', 'the'], 'lessening'),\n",
              " (['gradual', 'lessening', 'the', 'dominance'], 'of'),\n",
              " (['lessening', 'of', 'dominance', 'of'], 'the'),\n",
              " (['of', 'the', 'of', 'Chomskyan'], 'dominance'),\n",
              " (['the', 'dominance', 'Chomskyan', 'theories'], 'of'),\n",
              " (['dominance', 'of', 'theories', 'of'], 'Chomskyan'),\n",
              " (['of', 'Chomskyan', 'of', 'linguistics'], 'theories'),\n",
              " (['Chomskyan', 'theories', 'linguistics', ','], 'of'),\n",
              " (['theories', 'of', ',', 'whose'], 'linguistics'),\n",
              " (['of', 'linguistics', 'whose', 'theoretical'], ','),\n",
              " (['linguistics', ',', 'theoretical', 'underpinnings'], 'whose'),\n",
              " ([',', 'whose', 'underpinnings', 'discouraged'], 'theoretical'),\n",
              " (['whose', 'theoretical', 'discouraged', 'the'], 'underpinnings'),\n",
              " (['theoretical', 'underpinnings', 'the', 'sort'], 'discouraged'),\n",
              " (['underpinnings', 'discouraged', 'sort', 'of'], 'the'),\n",
              " (['discouraged', 'the', 'of', 'corpus'], 'sort'),\n",
              " (['the', 'sort', 'corpus', 'linguistics'], 'of'),\n",
              " (['sort', 'of', 'linguistics', 'that'], 'corpus'),\n",
              " (['of', 'corpus', 'that', 'underlies'], 'linguistics'),\n",
              " (['corpus', 'linguistics', 'underlies', 'the'], 'that'),\n",
              " (['linguistics', 'that', 'the', 'machine'], 'underlies'),\n",
              " (['that', 'underlies', 'machine', 'learning'], 'the'),\n",
              " (['underlies', 'the', 'learning', 'approach'], 'machine'),\n",
              " (['the', 'machine', 'approach', 'to'], 'learning'),\n",
              " (['machine', 'learning', 'to', 'language'], 'approach'),\n",
              " (['learning', 'approach', 'language', 'processing'], 'to'),\n",
              " (['approach', 'to', 'processing', '.'], 'language'),\n",
              " (['to', 'language', '.', 'Some'], 'processing'),\n",
              " (['language', 'processing', 'Some', 'of'], '.'),\n",
              " (['processing', '.', 'of', 'the'], 'Some'),\n",
              " (['.', 'Some', 'the', 'earliest-used'], 'of'),\n",
              " (['Some', 'of', 'earliest-used', 'machine'], 'the'),\n",
              " (['of', 'the', 'machine', 'learning'], 'earliest-used'),\n",
              " (['the', 'earliest-used', 'learning', 'algorithms'], 'machine'),\n",
              " (['earliest-used', 'machine', 'algorithms', ','], 'learning'),\n",
              " (['machine', 'learning', ',', 'such'], 'algorithms'),\n",
              " (['learning', 'algorithms', 'such', 'as'], ','),\n",
              " (['algorithms', ',', 'as', 'decision'], 'such'),\n",
              " ([',', 'such', 'decision', 'trees'], 'as'),\n",
              " (['such', 'as', 'trees', ','], 'decision'),\n",
              " (['as', 'decision', ',', 'produced'], 'trees'),\n",
              " (['decision', 'trees', 'produced', 'systems'], ','),\n",
              " (['trees', ',', 'systems', 'of'], 'produced'),\n",
              " ([',', 'produced', 'of', 'hard'], 'systems'),\n",
              " (['produced', 'systems', 'hard', 'if-then'], 'of'),\n",
              " (['systems', 'of', 'if-then', 'rules'], 'hard'),\n",
              " (['of', 'hard', 'rules', 'similar'], 'if-then'),\n",
              " (['hard', 'if-then', 'similar', 'to'], 'rules'),\n",
              " (['if-then', 'rules', 'to', 'existing'], 'similar'),\n",
              " (['rules', 'similar', 'existing', 'hand-written'], 'to'),\n",
              " (['similar', 'to', 'hand-written', 'rules'], 'existing'),\n",
              " (['to', 'existing', 'rules', '.'], 'hand-written'),\n",
              " (['existing', 'hand-written', '.', 'However'], 'rules'),\n",
              " (['hand-written', 'rules', 'However', ','], '.'),\n",
              " (['rules', '.', ',', 'part'], 'However'),\n",
              " (['.', 'However', 'part', 'of'], ','),\n",
              " (['However', ',', 'of', 'speech'], 'part'),\n",
              " ([',', 'part', 'speech', 'tagging'], 'of'),\n",
              " (['part', 'of', 'tagging', 'introduced'], 'speech'),\n",
              " (['of', 'speech', 'introduced', 'the'], 'tagging'),\n",
              " (['speech', 'tagging', 'the', 'use'], 'introduced'),\n",
              " (['tagging', 'introduced', 'use', 'of'], 'the'),\n",
              " (['introduced', 'the', 'of', 'hidden'], 'use'),\n",
              " (['the', 'use', 'hidden', 'Markov'], 'of'),\n",
              " (['use', 'of', 'Markov', 'models'], 'hidden'),\n",
              " (['of', 'hidden', 'models', 'to'], 'Markov'),\n",
              " (['hidden', 'Markov', 'to', 'natural'], 'models'),\n",
              " (['Markov', 'models', 'natural', 'language'], 'to'),\n",
              " (['models', 'to', 'language', 'processing'], 'natural'),\n",
              " (['to', 'natural', 'processing', ','], 'language'),\n",
              " (['natural', 'language', ',', 'and'], 'processing'),\n",
              " (['language', 'processing', 'and', 'increasingly'], ','),\n",
              " (['processing', ',', 'increasingly', ','], 'and'),\n",
              " ([',', 'and', ',', 'research'], 'increasingly'),\n",
              " (['and', 'increasingly', 'research', 'has'], ','),\n",
              " (['increasingly', ',', 'has', 'focused'], 'research'),\n",
              " ([',', 'research', 'focused', 'on'], 'has'),\n",
              " (['research', 'has', 'on', 'statistical'], 'focused'),\n",
              " (['has', 'focused', 'statistical', 'models'], 'on'),\n",
              " (['focused', 'on', 'models', ','], 'statistical'),\n",
              " (['on', 'statistical', ',', 'which'], 'models'),\n",
              " (['statistical', 'models', 'which', 'make'], ','),\n",
              " (['models', ',', 'make', 'soft'], 'which'),\n",
              " ([',', 'which', 'soft', ','], 'make'),\n",
              " (['which', 'make', ',', 'probabilistic'], 'soft'),\n",
              " (['make', 'soft', 'probabilistic', 'decisions'], ','),\n",
              " (['soft', ',', 'decisions', 'based'], 'probabilistic'),\n",
              " ([',', 'probabilistic', 'based', 'on'], 'decisions'),\n",
              " (['probabilistic', 'decisions', 'on', 'attaching'], 'based'),\n",
              " (['decisions', 'based', 'attaching', 'weights'], 'on'),\n",
              " (['based', 'on', 'weights', 'to'], 'attaching'),\n",
              " (['on', 'attaching', 'to', 'the'], 'weights'),\n",
              " (['attaching', 'weights', 'the', 'features'], 'to'),\n",
              " (['weights', 'to', 'features', 'making'], 'the'),\n",
              " (['to', 'the', 'making', 'up'], 'features'),\n",
              " (['the', 'features', 'up', 'the'], 'making'),\n",
              " (['features', 'making', 'the', 'input'], 'up'),\n",
              " (['making', 'up', 'input', 'data'], 'the'),\n",
              " (['up', 'the', 'data', '.'], 'input'),\n",
              " (['the', 'input', '.', 'The'], 'data'),\n",
              " (['input', 'data', 'The', 'cache'], '.'),\n",
              " (['data', '.', 'cache', 'language'], 'The'),\n",
              " (['.', 'The', 'language', 'models'], 'cache'),\n",
              " (['The', 'cache', 'models', 'upon'], 'language'),\n",
              " (['cache', 'language', 'upon', 'which'], 'models'),\n",
              " (['language', 'models', 'which', 'many'], 'upon'),\n",
              " (['models', 'upon', 'many', 'speech'], 'which'),\n",
              " (['upon', 'which', 'speech', 'recognition'], 'many'),\n",
              " (['which', 'many', 'recognition', 'systems'], 'speech'),\n",
              " (['many', 'speech', 'systems', 'now'], 'recognition'),\n",
              " (['speech', 'recognition', 'now', 'rely'], 'systems'),\n",
              " (['recognition', 'systems', 'rely', 'are'], 'now'),\n",
              " (['systems', 'now', 'are', 'examples'], 'rely'),\n",
              " (['now', 'rely', 'examples', 'of'], 'are'),\n",
              " (['rely', 'are', 'of', 'such'], 'examples'),\n",
              " (['are', 'examples', 'such', 'statistical'], 'of'),\n",
              " (['examples', 'of', 'statistical', 'models'], 'such'),\n",
              " (['of', 'such', 'models', '.'], 'statistical'),\n",
              " (['such', 'statistical', '.', 'Such'], 'models'),\n",
              " (['statistical', 'models', 'Such', 'models'], '.'),\n",
              " (['models', '.', 'models', 'are'], 'Such'),\n",
              " (['.', 'Such', 'are', 'generally'], 'models'),\n",
              " (['Such', 'models', 'generally', 'more'], 'are'),\n",
              " (['models', 'are', 'more', 'robust'], 'generally'),\n",
              " (['are', 'generally', 'robust', 'when'], 'more'),\n",
              " (['generally', 'more', 'when', 'given'], 'robust'),\n",
              " (['more', 'robust', 'given', 'unfamiliar'], 'when'),\n",
              " (['robust', 'when', 'unfamiliar', 'input'], 'given'),\n",
              " (['when', 'given', 'input', ','], 'unfamiliar'),\n",
              " (['given', 'unfamiliar', ',', 'especially'], 'input'),\n",
              " (['unfamiliar', 'input', 'especially', 'input'], ','),\n",
              " (['input', ',', 'input', 'that'], 'especially'),\n",
              " ([',', 'especially', 'that', 'contains'], 'input'),\n",
              " (['especially', 'input', 'contains', 'errors'], 'that'),\n",
              " (['input', 'that', 'errors', ','], 'contains'),\n",
              " (['that', 'contains', ',', 'and'], 'errors'),\n",
              " (['contains', 'errors', 'and', 'produce'], ','),\n",
              " (['errors', ',', 'produce', 'more'], 'and'),\n",
              " ([',', 'and', 'more', 'reliable'], 'produce'),\n",
              " (['and', 'produce', 'reliable', 'results'], 'more'),\n",
              " (['produce', 'more', 'results', 'when'], 'reliable'),\n",
              " (['more', 'reliable', 'when', 'integrated'], 'results'),\n",
              " (['reliable', 'results', 'integrated', 'into'], 'when'),\n",
              " (['results', 'when', 'into', 'a'], 'integrated'),\n",
              " (['when', 'integrated', 'a', 'larger'], 'into'),\n",
              " (['integrated', 'into', 'larger', 'system'], 'a'),\n",
              " (['into', 'a', 'system', 'comprising'], 'larger'),\n",
              " (['a', 'larger', 'comprising', 'multiple'], 'system'),\n",
              " (['larger', 'system', 'multiple', 'subtasks.Many'], 'comprising'),\n",
              " (['system', 'comprising', 'subtasks.Many', 'of'], 'multiple'),\n",
              " (['comprising', 'multiple', 'of', 'the'], 'subtasks.Many'),\n",
              " (['multiple', 'subtasks.Many', 'the', 'notable'], 'of'),\n",
              " (['subtasks.Many', 'of', 'notable', 'early'], 'the'),\n",
              " (['of', 'the', 'early', 'successes'], 'notable'),\n",
              " (['the', 'notable', 'successes', 'occurred'], 'early'),\n",
              " (['notable', 'early', 'occurred', 'in'], 'successes'),\n",
              " (['early', 'successes', 'in', 'the'], 'occurred'),\n",
              " (['successes', 'occurred', 'the', 'field'], 'in'),\n",
              " (['occurred', 'in', 'field', 'of'], 'the'),\n",
              " (['in', 'the', 'of', 'machine'], 'field'),\n",
              " (['the', 'field', 'machine', 'translation'], 'of'),\n",
              " (['field', 'of', 'translation', ','], 'machine'),\n",
              " (['of', 'machine', ',', 'due'], 'translation'),\n",
              " (['machine', 'translation', 'due', 'especially'], ','),\n",
              " (['translation', ',', 'especially', 'to'], 'due'),\n",
              " ([',', 'due', 'to', 'work'], 'especially'),\n",
              " (['due', 'especially', 'work', 'at'], 'to'),\n",
              " (['especially', 'to', 'at', 'IBM'], 'work'),\n",
              " (['to', 'work', 'IBM', 'Research'], 'at'),\n",
              " (['work', 'at', 'Research', ','], 'IBM'),\n",
              " (['at', 'IBM', ',', 'where'], 'Research'),\n",
              " (['IBM', 'Research', 'where', 'successively'], ','),\n",
              " (['Research', ',', 'successively', 'more'], 'where'),\n",
              " ([',', 'where', 'more', 'complicated'], 'successively'),\n",
              " (['where', 'successively', 'complicated', 'statistical'], 'more'),\n",
              " (['successively', 'more', 'statistical', 'models'], 'complicated'),\n",
              " (['more', 'complicated', 'models', 'were'], 'statistical'),\n",
              " (['complicated', 'statistical', 'were', 'developed'], 'models'),\n",
              " (['statistical', 'models', 'developed', '.'], 'were'),\n",
              " (['models', 'were', '.', 'These'], 'developed'),\n",
              " (['were', 'developed', 'These', 'systems'], '.'),\n",
              " (['developed', '.', 'systems', 'were'], 'These'),\n",
              " (['.', 'These', 'were', 'able'], 'systems'),\n",
              " (['These', 'systems', 'able', 'to'], 'were'),\n",
              " (['systems', 'were', 'to', 'take'], 'able'),\n",
              " (['were', 'able', 'take', 'advantage'], 'to'),\n",
              " (['able', 'to', 'advantage', 'of'], 'take'),\n",
              " (['to', 'take', 'of', 'existing'], 'advantage'),\n",
              " (['take', 'advantage', 'existing', 'multilingual'], 'of'),\n",
              " (['advantage', 'of', 'multilingual', 'textual'], 'existing'),\n",
              " (['of', 'existing', 'textual', 'corpora'], 'multilingual'),\n",
              " (['existing', 'multilingual', 'corpora', 'that'], 'textual'),\n",
              " (['multilingual', 'textual', 'that', 'had'], 'corpora'),\n",
              " (['textual', 'corpora', 'had', 'been'], 'that'),\n",
              " (['corpora', 'that', 'been', 'produced'], 'had'),\n",
              " (['that', 'had', 'produced', 'by'], 'been'),\n",
              " (['had', 'been', 'by', 'the'], 'produced'),\n",
              " (['been', 'produced', 'the', 'Parliament'], 'by'),\n",
              " (['produced', 'by', 'Parliament', 'of'], 'the'),\n",
              " (['by', 'the', 'of', 'Canada'], 'Parliament'),\n",
              " (['the', 'Parliament', 'Canada', 'and'], 'of'),\n",
              " (['Parliament', 'of', 'and', 'the'], 'Canada'),\n",
              " (['of', 'Canada', 'the', 'European'], 'and'),\n",
              " (['Canada', 'and', 'European', 'Union'], 'the'),\n",
              " (['and', 'the', 'Union', 'as'], 'European'),\n",
              " (['the', 'European', 'as', 'a'], 'Union'),\n",
              " (['European', 'Union', 'a', 'result'], 'as'),\n",
              " (['Union', 'as', 'result', 'of'], 'a'),\n",
              " (['as', 'a', 'of', 'laws'], 'result'),\n",
              " (['a', 'result', 'laws', 'calling'], 'of'),\n",
              " (['result', 'of', 'calling', 'for'], 'laws'),\n",
              " (['of', 'laws', 'for', 'the'], 'calling'),\n",
              " (['laws', 'calling', 'the', 'translation'], 'for'),\n",
              " (['calling', 'for', 'translation', 'of'], 'the'),\n",
              " (['for', 'the', 'of', 'all'], 'translation'),\n",
              " (['the', 'translation', 'all', 'governmental'], 'of'),\n",
              " (['translation', 'of', 'governmental', 'proceedings'], 'all'),\n",
              " (['of', 'all', 'proceedings', 'into'], 'governmental'),\n",
              " (['all', 'governmental', 'into', 'all'], 'proceedings'),\n",
              " (['governmental', 'proceedings', 'all', 'official'], 'into'),\n",
              " (['proceedings', 'into', 'official', 'languages'], 'all'),\n",
              " (['into', 'all', 'languages', 'of'], 'official'),\n",
              " (['all', 'official', 'of', 'the'], 'languages'),\n",
              " (['official', 'languages', 'the', 'corresponding'], 'of'),\n",
              " (['languages', 'of', 'corresponding', 'systems'], 'the'),\n",
              " (['of', 'the', 'systems', 'of'], 'corresponding'),\n",
              " (['the', 'corresponding', 'of', 'government'], 'systems'),\n",
              " (['corresponding', 'systems', 'government', '.'], 'of'),\n",
              " (['systems', 'of', '.', 'However'], 'government'),\n",
              " (['of', 'government', 'However', ','], '.'),\n",
              " (['government', '.', ',', 'most'], 'However'),\n",
              " (['.', 'However', 'most', 'other'], ','),\n",
              " (['However', ',', 'other', 'systems'], 'most'),\n",
              " ([',', 'most', 'systems', 'depended'], 'other'),\n",
              " (['most', 'other', 'depended', 'on'], 'systems'),\n",
              " (['other', 'systems', 'on', 'corpora'], 'depended'),\n",
              " (['systems', 'depended', 'corpora', 'specifically'], 'on'),\n",
              " (['depended', 'on', 'specifically', 'developed'], 'corpora'),\n",
              " (['on', 'corpora', 'developed', 'for'], 'specifically'),\n",
              " (['corpora', 'specifically', 'for', 'the'], 'developed'),\n",
              " (['specifically', 'developed', 'the', 'tasks'], 'for'),\n",
              " (['developed', 'for', 'tasks', 'implemented'], 'the'),\n",
              " (['for', 'the', 'implemented', 'by'], 'tasks'),\n",
              " (['the', 'tasks', 'by', 'these'], 'implemented'),\n",
              " (['tasks', 'implemented', 'these', 'systems'], 'by'),\n",
              " (['implemented', 'by', 'systems', ','], 'these'),\n",
              " (['by', 'these', ',', 'which'], 'systems'),\n",
              " (['these', 'systems', 'which', 'was'], ','),\n",
              " (['systems', ',', 'was', '('], 'which'),\n",
              " ([',', 'which', '(', 'and'], 'was'),\n",
              " (['which', 'was', 'and', 'often'], '('),\n",
              " (['was', '(', 'often', 'continues'], 'and'),\n",
              " (['(', 'and', 'continues', 'to'], 'often'),\n",
              " (['and', 'often', 'to', 'be'], 'continues'),\n",
              " (['often', 'continues', 'be', ')'], 'to'),\n",
              " (['continues', 'to', ')', 'a'], 'be'),\n",
              " (['to', 'be', 'a', 'major'], ')'),\n",
              " (['be', ')', 'major', 'limitation'], 'a'),\n",
              " ([')', 'a', 'limitation', 'in'], 'major'),\n",
              " (['a', 'major', 'in', 'the'], 'limitation'),\n",
              " (['major', 'limitation', 'the', 'success'], 'in'),\n",
              " (['limitation', 'in', 'success', 'of'], 'the'),\n",
              " (['in', 'the', 'of', 'these'], 'success'),\n",
              " (['the', 'success', 'these', 'systems'], 'of'),\n",
              " (['success', 'of', 'systems', '.'], 'these'),\n",
              " (['of', 'these', '.', 'As'], 'systems'),\n",
              " (['these', 'systems', 'As', 'a'], '.'),\n",
              " (['systems', '.', 'a', 'result'], 'As'),\n",
              " (['.', 'As', 'result', ','], 'a'),\n",
              " (['As', 'a', ',', 'a'], 'result'),\n",
              " (['a', 'result', 'a', 'great'], ','),\n",
              " (['result', ',', 'great', 'deal'], 'a'),\n",
              " ([',', 'a', 'deal', 'of'], 'great'),\n",
              " (['a', 'great', 'of', 'research'], 'deal'),\n",
              " (['great', 'deal', 'research', 'has'], 'of'),\n",
              " (['deal', 'of', 'has', 'gone'], 'research'),\n",
              " (['of', 'research', 'gone', 'into'], 'has'),\n",
              " (['research', 'has', 'into', 'methods'], 'gone'),\n",
              " (['has', 'gone', 'methods', 'of'], 'into'),\n",
              " (['gone', 'into', 'of', 'more'], 'methods'),\n",
              " (['into', 'methods', 'more', 'effectively'], 'of'),\n",
              " (['methods', 'of', 'effectively', 'learning'], 'more'),\n",
              " (['of', 'more', 'learning', 'from'], 'effectively'),\n",
              " (['more', 'effectively', 'from', 'limited'], 'learning'),\n",
              " (['effectively', 'learning', 'limited', 'amounts'], 'from'),\n",
              " (['learning', 'from', 'amounts', 'of'], 'limited'),\n",
              " (['from', 'limited', 'of', 'data.Recent'], 'amounts'),\n",
              " (['limited', 'amounts', 'data.Recent', 'research'], 'of'),\n",
              " (['amounts', 'of', 'research', 'has'], 'data.Recent'),\n",
              " (['of', 'data.Recent', 'has', 'increasingly'], 'research'),\n",
              " (['data.Recent', 'research', 'increasingly', 'focused'], 'has'),\n",
              " (['research', 'has', 'focused', 'on'], 'increasingly'),\n",
              " (['has', 'increasingly', 'on', 'unsupervised'], 'focused'),\n",
              " (['increasingly', 'focused', 'unsupervised', 'and'], 'on'),\n",
              " (['focused', 'on', 'and', 'semi-supervised'], 'unsupervised'),\n",
              " (['on', 'unsupervised', 'semi-supervised', 'learning'], 'and'),\n",
              " (['unsupervised', 'and', 'learning', 'algorithms'], 'semi-supervised'),\n",
              " (['and', 'semi-supervised', 'algorithms', '.'], 'learning'),\n",
              " (['semi-supervised', 'learning', '.', 'Such'], 'algorithms'),\n",
              " (['learning', 'algorithms', 'Such', 'algorithms'], '.'),\n",
              " (['algorithms', '.', 'algorithms', 'are'], 'Such'),\n",
              " (['.', 'Such', 'are', 'able'], 'algorithms'),\n",
              " (['Such', 'algorithms', 'able', 'to'], 'are'),\n",
              " (['algorithms', 'are', 'to', 'learn'], 'able'),\n",
              " (['are', 'able', 'learn', 'from'], 'to'),\n",
              " (['able', 'to', 'from', 'data'], 'learn'),\n",
              " (['to', 'learn', 'data', 'that'], 'from'),\n",
              " (['learn', 'from', 'that', 'has'], 'data'),\n",
              " (['from', 'data', 'has', 'not'], 'that'),\n",
              " (['data', 'that', 'not', 'been'], 'has'),\n",
              " (['that', 'has', 'been', 'hand-annotated'], 'not'),\n",
              " (['has', 'not', 'hand-annotated', 'with'], 'been'),\n",
              " (['not', 'been', 'with', 'the'], 'hand-annotated'),\n",
              " (['been', 'hand-annotated', 'the', 'desired'], 'with'),\n",
              " (['hand-annotated', 'with', 'desired', 'answers'], 'the'),\n",
              " (['with', 'the', 'answers', ','], 'desired'),\n",
              " (['the', 'desired', ',', 'or'], 'answers'),\n",
              " (['desired', 'answers', 'or', 'using'], ','),\n",
              " (['answers', ',', 'using', 'a'], 'or'),\n",
              " ([',', 'or', 'a', 'combination'], 'using'),\n",
              " (['or', 'using', 'combination', 'of'], 'a'),\n",
              " (['using', 'a', 'of', 'annotated'], 'combination'),\n",
              " (['a', 'combination', 'annotated', 'and'], 'of'),\n",
              " (['combination', 'of', 'and', 'non-annotated'], 'annotated'),\n",
              " (['of', 'annotated', 'non-annotated', 'data'], 'and'),\n",
              " (['annotated', 'and', 'data', '.'], 'non-annotated'),\n",
              " (['and', 'non-annotated', '.', 'Generally'], 'data'),\n",
              " (['non-annotated', 'data', 'Generally', ','], '.'),\n",
              " (['data', '.', ',', 'this'], 'Generally'),\n",
              " (['.', 'Generally', 'this', 'task'], ','),\n",
              " (['Generally', ',', 'task', 'is'], 'this'),\n",
              " ([',', 'this', 'is', 'much'], 'task'),\n",
              " (['this', 'task', 'much', 'more'], 'is'),\n",
              " (['task', 'is', 'more', 'difficult'], 'much'),\n",
              " (['is', 'much', 'difficult', 'than'], 'more'),\n",
              " (['much', 'more', 'than', 'supervised'], 'difficult'),\n",
              " (['more', 'difficult', 'supervised', 'learning'], 'than'),\n",
              " (['difficult', 'than', 'learning', ','], 'supervised'),\n",
              " (['than', 'supervised', ',', 'and'], 'learning'),\n",
              " (['supervised', 'learning', 'and', 'typically'], ','),\n",
              " (['learning', ',', 'typically', 'produces'], 'and'),\n",
              " ([',', 'and', 'produces', 'less'], 'typically'),\n",
              " (['and', 'typically', 'less', 'accurate'], 'produces'),\n",
              " (['typically', 'produces', 'accurate', 'results'], 'less'),\n",
              " (['produces', 'less', 'results', 'for'], 'accurate'),\n",
              " (['less', 'accurate', 'for', 'a'], 'results'),\n",
              " (['accurate', 'results', 'a', 'given'], 'for'),\n",
              " (['results', 'for', 'given', 'amount'], 'a'),\n",
              " (['for', 'a', 'amount', 'of'], 'given'),\n",
              " (['a', 'given', 'of', 'input'], 'amount'),\n",
              " (['given', 'amount', 'input', 'data'], 'of'),\n",
              " (['amount', 'of', 'data', '.'], 'input'),\n",
              " (['of', 'input', '.', 'However'], 'data'),\n",
              " (['input', 'data', 'However', ','], '.'),\n",
              " (['data', '.', ',', 'there'], 'However'),\n",
              " (['.', 'However', 'there', 'is'], ','),\n",
              " (['However', ',', 'is', 'an'], 'there'),\n",
              " ([',', 'there', 'an', 'enormous'], 'is'),\n",
              " (['there', 'is', 'enormous', 'amount'], 'an'),\n",
              " (['is', 'an', 'amount', 'of'], 'enormous'),\n",
              " (['an', 'enormous', 'of', 'non-annotated'], 'amount'),\n",
              " (['enormous', 'amount', 'non-annotated', 'data'], 'of'),\n",
              " (['amount', 'of', 'data', 'available'], 'non-annotated'),\n",
              " (['of', 'non-annotated', 'available', ','], 'data'),\n",
              " (['non-annotated', 'data', ',', 'which'], 'available'),\n",
              " (['data', 'available', 'which', 'can'], ','),\n",
              " (['available', ',', 'can', 'often'], 'which'),\n",
              " ([',', 'which', 'often', 'make'], 'can'),\n",
              " (['which', 'can', 'make', 'up'], 'often'),\n",
              " (['can', 'often', 'up', 'for'], 'make'),\n",
              " (['often', 'make', 'for', 'the'], 'up'),\n",
              " (['make', 'up', 'the', 'inferior'], 'for'),\n",
              " (['up', 'for', 'inferior', 'results'], 'the'),\n",
              " (['for', 'the', 'results', 'if'], 'inferior'),\n",
              " (['the', 'inferior', 'if', 'the'], 'results'),\n",
              " (['inferior', 'results', 'the', 'algorithm'], 'if'),\n",
              " (['results', 'if', 'algorithm', 'used'], 'the'),\n",
              " (['if', 'the', 'used', 'has'], 'algorithm'),\n",
              " (['the', 'algorithm', 'has', 'a'], 'used'),\n",
              " (['algorithm', 'used', 'a', 'low'], 'has'),\n",
              " (['used', 'has', 'low', 'enough'], 'a'),\n",
              " (['has', 'a', 'enough', 'time'], 'low'),\n",
              " (['a', 'low', 'time', 'complexity'], 'enough'),\n",
              " (['low', 'enough', 'complexity', 'to'], 'time'),\n",
              " (['enough', 'time', 'to', 'be'], 'complexity'),\n",
              " (['time', 'complexity', 'be', 'practical'], 'to'),\n",
              " (['complexity', 'to', 'practical', '.'], 'be'),\n",
              " (['to', 'be', '.'], 'practical'),\n",
              " (['be', 'practical'], '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3B\n",
        "from collections import Counter\n",
        "\n",
        "# Count the number of occurance for each word\n",
        "word_count = Counter(tokens)\n",
        "min_count = 5\n",
        "\n",
        "# Word that occur more than min_count times\n",
        "\n",
        "statisfied_word = {}\n",
        "for word, count in word_count.items():\n",
        "  if count >= min_count:\n",
        "    statisfied_word[word] = count\n",
        "\n",
        "print(f\"The number of words that appear more than {min_count} times is {len(statisfied_word)}\")\n",
        "'''\n",
        "Answer for Exercise 3 Part B1\n",
        "For Exercise 3, part B1, Given that the hyperparameters for Word2Vec are vector_size=50, window=3,\n",
        "and min_count=5. Then the dimension of our embedding matrix will be 17 x 50 in which\n",
        "17 is the number of words appear more than 5 times and 50 is the vector_size\n",
        "\n",
        "Answer for Exercise 3 Part B2\n",
        "For Exercise 3, part B2, Given that the hyperparameters for Word2Vec are vector_size=50, window=5,\n",
        "and min_count=5. Then the dimension of our embedding matrix will be 17 x 50 in which\n",
        "17 is the number of words appear more than 5 times and 50 is the vector_size. Here the change\n",
        "in the size of window doesn't affect the output as the number of rows will depends on the min_count\n",
        "and the number of columns will depends on the vector_size. The window parameter in word2vec determines\n",
        "how many words before and after should be included as the context words for the given words but it\n",
        "doesn't affect the size of the word vectors or the number of unique words in the vocabulary.\n",
        "\n",
        "Answer for Exercise 3 Part B3\n",
        "For Exercise 3, part B3, Given that the hyperparameters for Word2Vec are vector_size=100, window=3,\n",
        "and min_count=5. Then the dimension of our embedding matrix will be 17 x 100 in which\n",
        "17 is the number of words appear more than 5 times and 100 is the vector_size. Here the change\n",
        "in the vector_size does affect the output as the number of columns will depends on the vector_size.\n",
        "The reason is that the number of columns of the embeeding matrix will depends on the vector_size.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "h-y_bf77M5Pp",
        "outputId": "1d9a4ad1-2d0a-44e2-ba52-996418eea14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words that appear more than 5 times is 17\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer for Exercise 3 Part B1\\nFor Exercise 3, part B1, Given that the hyperparameters for Word2Vec are vector_size=50, window=3,\\nand min_count=5. Then the dimension of our embedding matrix will be 17 x 50 in which\\n17 is the number of words appear more than 5 times and 50 is the vector_size\\n\\n\\nFor Exercise 3, part B2, Given that the hyperparameters for Word2Vec are vector_size=50, window=5,\\nand min_count=5. Then the dimension of our embedding matrix will be 17 x 50 in which\\n17 is the number of words appear more than 5 times and 50 is the vector_size\\n\\nFor Exercise 3, part B3, Given that the hyperparameters for Word2Vec are vector_size=100, window=3,\\nand min_count=5. Then the dimension of our embedding matrix will be 17 x 100 in which\\n17 is the number of words appear more than 5 times and 100 is the vector_size\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3C (What is the purpose of negative sampling when training Word2Vec?)\n",
        "'''\n",
        "The purpose of negative sampling when training Word2Vec is to only modify a small\n",
        "percentage of the weights, rather than all of them in each training round of the\n",
        "training process.\n",
        "The reason is that performing the weight and bias updates is computationally\n",
        "expensive if we update everything in every round of the training process.\n",
        "'''"
      ],
      "metadata": {
        "id": "hGHut8aqV9Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3D (Why does CBOW train faster than Skip-gram? Hint: Think about how each model is updated)\n",
        "'''\n",
        "The reason that CBOW train faster than Skip-gram is because the Skip-gram model\n",
        "need to update its weight more times than CBOW for the same amount of training data.\n",
        "As for each training example, CBOW only need to make 1 prediction while Skip-gram\n",
        "need to predict between window_size (for the first and last word) and 2 * window_size.\n",
        "Because of that while CBOW need to only update the weight and bias for 1 times per\n",
        "example, Skip-gram needs to do so between window_size to 2 * window_size times.\n",
        "'''"
      ],
      "metadata": {
        "id": "r69BlgqdXR-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4"
      ],
      "metadata": {
        "id": "3pXKeUnlWw09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4 (20 marks)\n",
        "SpaCy uses a modified version of Word2Vec to get token representations called FastText.\n",
        "FastText is essentially the same as Word2Vec, except that instead of operating on tokens of\n",
        "entire words, it operates on sets of characters.\n",
        "\n",
        "A) Import fasttext from Gensim and get the representation of at least two out of vocab words\n",
        "when you train a model with corpus = [[\"horse\", \"pulled\", \"cart\"], [\"dog\", \"say\", \"woof\"]] and\n",
        "the most similar words to it. Find a word that is not part of this corpus vocabulary that will\n",
        "work with this model, ie. will be represented. Will any word work with this fasttext model?\n",
        "Why or why not? (8 marks)\n",
        "\n",
        "B) Use pretrained Word2Vec and FastText models word2vec-google-news-300 and fasttext-\n",
        "wiki-news-subwords-300, respectively (you may need to load them individually and remove\n",
        "after the analysis is done as they are quite large). Load these models and come up with at least\n",
        "four examples to compare syntactic (2 examples) and semantic (2 examples, hint: how can\n",
        "you make analogies with Gensim?) representations between the two models. Compare and\n",
        "contrast the results. (8 marks)\n",
        "\n",
        "C) What do you think the potential benefits are of using FastText over Word2Vec and vice\n",
        "versa? (4 marks)"
      ],
      "metadata": {
        "id": "dHokoKkdbe8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4A\n",
        "from gensim.models import FastText\n",
        "corpus = [[\"horse\", \"pulled\", \"cart\"], [\"dog\", \"say\", \"woof\"]]\n",
        "\n",
        "out_of_vocab_words = [\"bear\", \"giraffe\"]\n",
        "\n",
        "model = FastText(corpus, min_count=1, max_n=3)\n",
        "model.train(corpus, total_examples = len(corpus), epochs=10)\n",
        "# print(f\"The representation for {corpus[0][0]} is {model.wv[corpus[0][0]]}\\n\")\n",
        "# print(f\"The representation for {corpus[0][1]} is {model.wv[corpus[0][1]]}\")\n",
        "\n",
        "# Print the representation for out of vocab words\n",
        "for word in out_of_vocab_words:\n",
        "  print(f\"Representation for word '{word}' is {model.wv[word]}\")\n",
        "\n",
        "# Print most similar word to out of vocab words\n",
        "for word in out_of_vocab_words:\n",
        "  print(f\"Most similar words to '{word}' is:{model.wv.most_similar(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmgFaCE3bh_5",
        "outputId": "e1fdb44a-2983-412a-f251-1ccdd839021b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representation for word 'bear' is [-1.5895491e-03 -9.2735444e-04  3.4300482e-03  2.0477588e-03\n",
            "  1.6747741e-05  4.6929298e-03  1.7358833e-03  1.3843234e-04\n",
            "  1.7013401e-04 -2.1758371e-03  2.6722024e-03  1.3869964e-03\n",
            "  7.5517129e-04  2.8515097e-03 -3.2175560e-03 -4.0820120e-03\n",
            "  5.8232388e-03 -7.0819757e-03 -2.8517151e-03 -2.6078641e-03\n",
            " -2.0849674e-03 -2.0851435e-04  7.2138757e-04 -4.1426350e-03\n",
            "  2.6957626e-03  6.6661509e-04 -3.2623431e-03  1.5650976e-03\n",
            "  1.5343765e-03  5.4984535e-03  3.4324105e-03  2.9666126e-03\n",
            "  3.6406270e-03  3.5848245e-03  4.3843829e-04  1.2216703e-03\n",
            " -1.5861897e-03  4.6360185e-03 -4.5364485e-03 -4.2342516e-03\n",
            "  1.2428227e-03 -7.6116074e-04  2.9585212e-03 -3.1120786e-03\n",
            " -2.9495242e-04 -7.9474598e-04 -4.9405759e-03 -2.7792375e-03\n",
            "  3.4472817e-03  1.0441137e-03 -4.6287198e-03  2.1738601e-03\n",
            "  2.7638446e-03  1.8589658e-03  3.7521385e-03 -2.3429864e-04\n",
            "  2.2773063e-03 -1.8434511e-03  7.4835867e-04  1.2110728e-03\n",
            "  7.1111005e-03  1.9610277e-03 -2.8161812e-03  4.2774617e-03\n",
            " -6.9973595e-04 -1.4656251e-03  3.9322762e-04 -2.9971327e-03\n",
            " -6.1566196e-04  1.4838065e-03 -1.0511242e-03  7.7945122e-04\n",
            " -2.7129061e-03 -1.0783597e-03  1.6586139e-03  1.8331239e-03\n",
            "  1.3995246e-03 -1.6795711e-03 -1.0009018e-03  3.5462442e-03\n",
            " -5.2213981e-03  1.2882468e-03 -7.4831233e-04 -2.6183175e-03\n",
            " -3.7866433e-03 -2.2313958e-03 -1.3051194e-03 -5.3194375e-03\n",
            " -8.0546294e-04  6.8177329e-03  1.7568201e-03 -4.1248165e-03\n",
            "  6.6062203e-04 -2.2786248e-03 -5.2595139e-04 -3.1599570e-03\n",
            "  1.6883374e-03  1.1096655e-03  3.4064252e-04  8.6125312e-04]\n",
            "Representation for word 'giraffe' is [-1.9459821e-03  3.4215692e-03  6.2369718e-03  2.4224825e-03\n",
            " -2.7880445e-03  2.4519733e-03  5.5827654e-04 -4.6939217e-04\n",
            "  1.6628944e-04 -3.8801718e-03 -8.3666103e-04 -1.7935012e-05\n",
            "  1.6756664e-03  7.4076664e-04  3.1679079e-03  7.0021610e-04\n",
            " -4.9391477e-03  1.0954619e-03  1.8499119e-03 -2.4073587e-03\n",
            " -1.8465529e-03  1.5016800e-03  1.8314162e-03  3.0142411e-03\n",
            "  3.9271768e-03  2.9219734e-04  5.5601872e-03  6.4255443e-04\n",
            "  3.8587596e-04  8.3562353e-04 -1.8327763e-06 -4.5805192e-04\n",
            " -4.7506663e-04  1.5504459e-03 -1.2658673e-03 -9.1703318e-04\n",
            "  2.5263894e-03 -1.8838652e-03  2.4205877e-03 -1.8353480e-03\n",
            "  3.6675935e-03  5.3599902e-04 -5.0766990e-03 -2.4168326e-03\n",
            "  1.4478521e-03  1.2840192e-03 -7.7624514e-04 -1.0758126e-03\n",
            " -2.2639984e-03 -1.0934958e-03  3.6736242e-03 -1.1988860e-03\n",
            "  2.4919093e-04  3.2855053e-03 -1.0850204e-03 -2.7143976e-03\n",
            "  1.2074885e-03  1.3502586e-03  9.0884551e-04 -1.5492480e-03\n",
            "  1.4879940e-03  2.6596929e-03  2.5028477e-03  4.9967458e-04\n",
            "  8.2240591e-04  2.4473344e-04 -4.1275341e-03 -1.3207908e-03\n",
            "  2.4701818e-03 -2.4662085e-03 -1.2413187e-04  4.1146497e-03\n",
            "  2.6728690e-03  3.0002471e-03  7.7085235e-05 -2.7578589e-04\n",
            "  2.3199283e-03 -4.1755275e-03  3.2508219e-04  2.9158352e-03\n",
            "  9.2148420e-04  2.2983742e-03 -3.5263260e-03  2.5206245e-03\n",
            " -3.9195633e-03 -8.0148276e-04  3.1975750e-03  2.0215551e-03\n",
            " -1.1348072e-03  5.2226236e-04  8.8911370e-04  4.0226453e-03\n",
            " -4.3895962e-03 -7.5870421e-04 -1.6712307e-03 -2.6772346e-03\n",
            " -1.9335242e-03 -3.4769662e-03 -1.0908722e-03  1.0649750e-03]\n",
            "Most similar words to 'bear' is:[('pulled', 0.08084706962108612), ('say', 0.07694915682077408), ('horse', 0.0640266016125679), ('woof', 0.03185803070664406), ('dog', 0.015501079149544239), ('cart', -0.024118945002555847)]\n",
            "Most similar words to 'giraffe' is:[('horse', 0.10705604404211044), ('pulled', 0.03680020198225975), ('say', -0.016058431938290596), ('woof', -0.05081108585000038), ('cart', -0.056312285363674164), ('dog', -0.1977260410785675)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4A (cont)\n",
        "\n",
        "A word which is not in the corpus vocabulary which will work with this model is \"cat\".\n",
        "\n",
        "In my opinion, any word will work with this fasttext model. The reason is that fasttext can generate embeedings for any word based on its subword information, even if the word is out of vocabulary.In addition, fasttext uses charater n-grams to represent words and combines them with the hashing function to reduce the dimensionality of the embeedings. By using this way, fasttext can capture the morphological and semantic similarities between words as well as handle rare or misspelled words."
      ],
      "metadata": {
        "id": "7ZkCb42LdoyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4B\n",
        "import gensim.downloader\n",
        "\n",
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "print(\"\\n\")\n",
        "gensim.downloader.info('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnZN0hDsSXcX",
        "outputId": "62fbabc9-2a51-4666-9d90-0abb9940285a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_records': 3000000,\n",
              " 'file_size': 1743563840,\n",
              " 'base_dataset': 'Google News (about 100 billion words)',\n",
              " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
              " 'license': 'not found',\n",
              " 'parameters': {'dimension': 300},\n",
              " 'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              " 'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "  'https://arxiv.org/abs/1301.3781',\n",
              "  'https://arxiv.org/abs/1310.4546',\n",
              "  'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              " 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              " 'file_name': 'word2vec-google-news-300.gz',\n",
              " 'parts': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to download the \"word2vec-google-news-300\" embeddings\n",
        "w2v_model = gensim.downloader.load('word2vec-google-news-300')\n",
        "fasttext_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl9L1En5iyXR",
        "outputId": "82620134-df57-4eaa-916b-0565d18c98a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic example 1: Find most similar word to jump + ran - run in both model\n",
        "dog_syntactic_w2v = w2v_model.most_similar(positive=['jump', 'ran'], negative=['run'])\n",
        "dog_syntactic_fasttext = fasttext_model.most_similar(positive=['jump', 'ran'], negative=['run'])\n",
        "print(f\"Most similar word to 'dog' according to Word2vec: {dog_syntactic_w2v}\")\n",
        "print(f\"Most similar word to 'dog' according to FastText: {dog_syntactic_fasttext}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg3UH-NzjKFl",
        "outputId": "f640362e-a135-47b8-a57e-b75e292f247c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to 'dog' according to Word2vec: [('jumped', 0.6767446994781494), ('leaped', 0.6681877970695496), ('leapt', 0.6557392477989197), ('jumping', 0.5900293588638306), ('jumps', 0.5599877238273621), ('lept', 0.5453763008117676), ('climbed', 0.5316794514656067), ('zoomed', 0.5155339241027832), ('fell', 0.48489847779273987), ('slipped', 0.4796901047229767)]\n",
            "Most similar word to 'dog' according to FastText: [('jumped', 0.7573277950286865), ('jumps', 0.7270587682723999), ('jumping', 0.7110602855682373), ('leapt', 0.6688531041145325), ('leaped', 0.6642698645591736), ('jump.', 0.6389334797859192), ('lept', 0.6102530360221863), ('jumpin', 0.6081960201263428), ('mid-jump', 0.6052793264389038), ('leaping', 0.5966655611991882)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic example 2: Find most similar word to cat + dogs - dog in both model\n",
        "wealthy_syntactic_w2v = w2v_model.most_similar(positive=['cat', 'dogs'], negative=['dog'])\n",
        "wealthy_syntactic_fasttext = fasttext_model.most_similar(positive=['cat', 'dogs'], negative=['dog'])\n",
        "print(f\"Most similar word to 'wealthy' according to Word2vec: {wealthy_syntactic_w2v}\")\n",
        "print(f\"Most similar word to 'wealthy' according to FastText: {wealthy_syntactic_fasttext}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FJfGeBhjs1D",
        "outputId": "53ff25ed-7392-4902-af6c-1e7b75c97b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to 'wealthy' according to Word2vec: [('cats', 0.898319661617279), ('felines', 0.7551507949829102), ('kittens', 0.7065576910972595), ('puppies', 0.7045103311538696), ('pets', 0.7012346982955933), ('kitties', 0.6971524953842163), ('animals', 0.6731154918670654), ('feline', 0.6566011905670166), ('chihuahuas', 0.6526333689689636), ('Chihuahuas', 0.6479750275611877)]\n",
            "Most similar word to 'wealthy' according to FastText: [('cats', 0.9430418014526367), ('felines', 0.7601239085197449), ('housecats', 0.7563892006874084), ('kittens', 0.7071666121482849), ('tomcats', 0.7030109167098999), ('moggies', 0.6991898417472839), ('kitties', 0.6986546516418457), ('sub-cats', 0.6973353624343872), ('supercats', 0.6972814798355103), ('cats.', 0.6946355700492859)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic example 1: Find most similar word to ''king - men + women' in both model (meaning)\n",
        "semantic1_w2v = w2v_model.most_similar(positive=['king', 'women'], negative=['men'])\n",
        "semantic1_fasttext = fasttext_model.most_similar(positive=['king', 'women'], negative=['men'])\n",
        "print(f\"Most similar word to 'king - men + women' according to Word2vec:\")\n",
        "print(semantic1_w2v)\n",
        "print(\"\\n\")\n",
        "print(f\"Most similar word to 'king - men + women' according to FastText:\")\n",
        "print(semantic1_fasttext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gdMpwYMj0v1",
        "outputId": "324c856f-a05f-413c-b52d-549c9bd2a5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to 'king - men + women' according to Word2vec:\n",
            "[('queen', 0.6525818109512329), ('monarch', 0.5959680676460266), ('crown_prince', 0.5324338674545288), ('kings', 0.5282072424888611), ('princess', 0.5240627527236938), ('sultan', 0.4983791410923004), ('monarchy', 0.49648746848106384), ('ruler', 0.49376848340034485), ('prince', 0.48910900950431824), ('Prince_Paras', 0.4794894754886627)]\n",
            "\n",
            "\n",
            "Most similar word to 'king - men + women' according to FastText:\n",
            "[('queen', 0.7628974914550781), ('child-king', 0.7179961800575256), ('king-', 0.7167667746543884), ('boy-king', 0.7065533399581909), ('monarch', 0.7059975266456604), ('king-making', 0.7034288048744202), ('kings', 0.6744516491889954), ('princess', 0.670761227607727), ('prince', 0.6683158278465271), ('queen-mother', 0.6604015231132507)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic example 2: Find most similar word to 'MacOS + Microsoft - Apple' in both model\n",
        "semantic2_w2v = w2v_model.most_similar(positive=['MacOS', 'Microsoft'], negative=['Apple'])\n",
        "semantic2_fasttext = fasttext_model.most_similar(positive=['MacOS', 'Microsoft'], negative=['Apple'])\n",
        "print(f\"Most similar word to 'MacOS + Microsoft - Apple' according to Word2vec:\")\n",
        "print(semantic2_w2v)\n",
        "print(\"\\n\")\n",
        "print(f\"Most similar word to 'MacOS + Microsoft - Apple' according to FastText:\")\n",
        "print(semantic2_fasttext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMZa1xu5kZKO",
        "outputId": "8ebd9882-09d1-435e-b1de-60d285a22699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to 'MacOS + Microsoft - Apple' according to Word2vec:\n",
            "[('Windows_Server', 0.6042938828468323), ('Windows', 0.6012716293334961), ('Win9x', 0.5783877372741699), ('Windows_XP', 0.5762310028076172), ('Windows_Vista', 0.5607187747955322), ('Windows_NT', 0.5532339811325073), ('Win##', 0.5519891977310181), ('Visual_Studio', 0.5497651100158691), ('SQL_Server', 0.5480638742446899), ('Visual_Studio_Tools', 0.5476875901222229)]\n",
            "\n",
            "\n",
            "Most similar word to 'MacOS + Microsoft - Apple' according to FastText:\n",
            "[('MS-Windows', 0.6968671083450317), ('Win95', 0.66008460521698), ('WindowsNT', 0.6585357189178467), ('Win2K', 0.6583325862884521), ('WinNT', 0.6549997329711914), ('MacOSX', 0.6537865400314331), ('Win32s', 0.6474349498748779), ('Windows-', 0.6447452902793884), ('Windows98', 0.644517719745636), ('Win9x', 0.6442602276802063)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic example 3: Find most similar word to 'Berlin - Germany + England' in both model (meaning)\n",
        "semantic3_w2v = w2v_model.most_similar(positive=['Berlin', 'England'], negative=['Germany'])\n",
        "semantic3_fasttext = fasttext_model.most_similar(positive=['Berlin', 'England'], negative=['Germany'])\n",
        "print(f\"Most similar word to 'Berlin - Germany + England' according to Word2vec:\")\n",
        "print(semantic3_w2v)\n",
        "print(\"\\n\")\n",
        "print(f\"Most similar word to 'Berlin - Germany + England' according to FastText:\")\n",
        "print(semantic3_fasttext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1d6R1qrOKMa",
        "outputId": "6707a229-572c-47fd-db0d-106d8dffa5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to 'Berlin - Germany + England' according to Word2vec:\n",
            "[('London', 0.5510433316230774), ('stock_symbol_BNK', 0.5436815619468689), ('ticker_symbol_BNK', 0.5417938232421875), ('Englands', 0.5250540971755981), ('Bristol', 0.49866652488708496), ('Hamp_shire', 0.49121686816215515), ('Manchester', 0.4882766008377075), ('Kensington', 0.48695042729377747), ('Haven', 0.48326244950294495), ('Culinary_Institute_NECI', 0.4787623882293701)]\n",
            "\n",
            "\n",
            "Most similar word to 'Berlin - Germany + England' according to FastText:\n",
            "[('London', 0.7693520784378052), ('Manchester', 0.6989110708236694), ('Edinborough', 0.6697214245796204), ('Englan', 0.667100727558136), ('Bristol', 0.6668068170547485), ('Boston', 0.6604733467102051), ('Attingham', 0.6588860750198364), ('Nottingham', 0.6578013300895691), ('York', 0.6574037075042725), ('Barset', 0.6542746424674988)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4C\n",
        "\n",
        "The potential benefit of FastText over Word2vec:\n",
        "\n",
        "- FastText can provide the embeddings for out of vocabulary words by combining their character as it use n-gram embeedings which Word2Vec can't provide the embeddings for out of vocabulary words as it works on the word level.\n",
        "- FastText can provide better embeddings for language with lots of special symbols (morphologically rich) like Arabic compared to Word2Vec. The reason is that FastText can capture similarities between words that share common prefixes and suffixes as it operates at character level unlike word2vec which operates at word level.\n",
        "- FastText can do better than Word2vec in finding words with similar grammar structure or misspelling as FastText can learn the representations of word in a more flexible way.\n",
        "\n",
        "The potential benefit of Word2vec over FastText:\n",
        "- Word2vec is more efficient compare to FastText. The reason is that FastText need to compute the vector embedding for all the n-grams of all words when Word2vec don't need to do so.\n",
        "- As Word2vec is more efficient, Word2vec will require less memory (ROM, RAM and/or GPU memory) to train compared to FastText."
      ],
      "metadata": {
        "id": "JPSwEqaAlBXE"
      }
    }
  ]
}