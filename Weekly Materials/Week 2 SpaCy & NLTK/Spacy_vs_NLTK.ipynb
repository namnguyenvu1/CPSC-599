{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM9ilaKUn5gD"
      },
      "source": [
        "# **NLTK vs SpaCy**\n",
        "\n",
        "In general, however, NLTK and SpaCy are designed to be efficient and performant libraries for natural language processing, and they both offer a range of optimization techniques and algorithms to help speed up processing.\n",
        "\n",
        "However, there are some differences between tasks and operations that both these libraries do in terms of speed and performance.\n",
        "\n",
        "At times, one may be faster than the other. For example, SpaCy is designed to be particularly fast at tasks like tokenization and part-of-speech tagging, and it uses optimized Cython-based implementations of many of its core algorithms to achieve this performance.\n",
        "\n",
        "On the other hand, NLTK may be faster at certain tasks that require more comprehensive or flexible processing, such as language modeling or machine learning.\n",
        "\n",
        "It is also important to note that there are various other libraries you can use to perform these common pre-processing steps we have covered in the last few classes. For example, Gensim, CoreNLP, and Scikit-learn also have some preprocessing support as well so there are many options to choose from when preprocessing and it might come down to what else you want to do downstream of preprocessing and whether the library used has support for that task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUEsBWAQQk_f"
      },
      "source": [
        "Let's cover some examples where you might see evidence that SpaCy and NLTK are different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKWDaPIYI_O6"
      },
      "source": [
        "### **SpaCy and NLTK Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsP_sV_8nzf_",
        "outputId": "3b1a3ace-1fa6-4715-ad15-557c2da2b371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Stopwords:  179\n",
            "Corpus words:  414574\n",
            "Corpus words (Stopwords removed):  258454\n"
          ]
        }
      ],
      "source": [
        "##NLTK\n",
        "import nltk\n",
        "#if you need to, download the following if missing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "\n",
        "import string\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#accessing the stop words\n",
        "stopwords_nltk = stopwords.words('english')\n",
        "\n",
        "corpus = brown.words(categories=['hobbies'])\n",
        "corpus = [''.join(c for c in word if c.isalpha()) for word in corpus]\n",
        "corpus = [word for word in corpus if word !='']\n",
        "corpus = \" \".join(corpus)\n",
        "\n",
        "#tokenized text\n",
        "tokenized_corpus = word_tokenize(corpus)\n",
        "\n",
        "# How many stop words in NLTK?\n",
        "print(\"Number of Stopwords: \", len(stopwords_nltk))\n",
        "\n",
        "print(\"Corpus words: \", len(corpus))\n",
        "#removal of stopwords\n",
        "nltk_without_stopwords = [word for word in corpus if not word in stopwords_nltk]\n",
        "print(\"Corpus words (Stopwords removed): \", len(nltk_without_stopwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tTuDHSwAjBN7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f30967e-c7f3-498c-d62d-9f3df4810fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Stopwords:  326\n",
            "Corpus words (Stopwords removed):  363164\n"
          ]
        }
      ],
      "source": [
        "##SPACY\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = English()\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "tokens_spacy = tokenizer(corpus)\n",
        "\n",
        "stopwords_spacy = spacy.lang.en.STOP_WORDS\n",
        "# How many stopwords in SpaCy?\n",
        "print(\"Number of Stopwords: \", len(stopwords_spacy))\n",
        "\n",
        "#removal of stopwords\n",
        "spacy_without_stopwords= [word for word in corpus if not word in stopwords_spacy]\n",
        "\n",
        "\n",
        "print(\"Corpus words (Stopwords removed): \", len(spacy_without_stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75-4V1CWj520"
      },
      "source": [
        "## **Lemmatization and POS tagging between NLTK and SpaCy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YveE81QeS_U-",
        "outputId": "2dbc949c-5eb2-4976-a02f-6791580cf0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running-->run\n",
            "is-->be\n",
            "one-->one\n",
            "of-->of\n",
            "the-->the\n",
            "gaits-->gait\n",
            "of-->of\n",
            "terrestrial-->terrestrial\n",
            "locomotion-->locomotion\n",
            "among-->among\n",
            "legged-->legged\n",
            "animals-->animal\n",
            "Running-->VERB\n",
            "is-->AUX\n",
            "one-->NUM\n",
            "of-->ADP\n",
            "the-->DET\n",
            "gaits-->NOUN\n",
            "of-->ADP\n",
            "terrestrial-->ADJ\n",
            "locomotion-->NOUN\n",
            "among-->ADP\n",
            "legged-->ADJ\n",
            "animals-->NOUN\n"
          ]
        }
      ],
      "source": [
        "text = 'Running is one of the gaits of terrestrial locomotion among legged animals'\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text + \"-->\" + token.lemma_)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text + \"-->\" + token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLessE4cS_U-",
        "outputId": "9075a648-e761-4deb-950e-bb1fa8ac8db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running --> Running\n",
            "is --> be\n",
            "one --> one\n",
            "of --> of\n",
            "the --> the\n",
            "gaits --> gait\n",
            "of --> of\n",
            "terrestrial --> terrestrial\n",
            "locomotion --> locomotion\n",
            "among --> among\n",
            "legged --> legged\n",
            "animals --> animal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Show the difference between lemmatization and stemming in NLTK\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "nltk_lemmas = []\n",
        "\n",
        "#for word in tokens:\n",
        "#    nltk_lemmas.append(lemmatizer.lemmatize(word))\n",
        "nltk_lemmas = [lemmatizer.lemmatize(word, pos='v') if word == 'is' else lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "for token, lemma in zip(tokens, nltk_lemmas):\n",
        "    print(f\"{token} --> {lemma}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOTxMgPZS_U-"
      },
      "outputs": [],
      "source": [
        "# Is there a difference between lemmatization in SpaCy and NLTK?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i43tnrgNe5TS"
      },
      "source": [
        "## **Execution Time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G562v8lToUTO",
        "outputId": "a79f0486-0f98-4c9b-ea9f-b1fd67a7ed04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK: 0.1357 seconds\n",
            "SpaCy: 0.0832 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Load the text data\n",
        "with open('plots.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# Tokenize the text using NLTK\n",
        "st = time.time()\n",
        "tokens_nltk = nltk.word_tokenize(text)\n",
        "et = time.time()\n",
        "time_nltk = et - st\n",
        "\n",
        "# Tokenize the text using SpaCy\n",
        "spacy_model = English()\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "st = time.time()\n",
        "tokens = tokenizer(corpus)\n",
        "tokens_spacy = [token.text for token in tokens]\n",
        "et = time.time()\n",
        "time_spacy = et - st\n",
        "\n",
        "# Print the results\n",
        "print(f'NLTK: {time_nltk:.4f} seconds')\n",
        "print(f'SpaCy: {time_spacy:.4f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "1BB2cAkMoMzC",
        "outputId": "a4a76336-9e8e-439d-a26f-a875622698e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK: 0.8632 seconds\n",
            "SpaCy: 1.7484 seconds\n"
          ]
        }
      ],
      "source": [
        "#nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Tag the text using NLTK\n",
        "st = time.time()\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tags_nltk = nltk.pos_tag(tokens)\n",
        "et = time.time()\n",
        "time_nltk = et - st\n",
        "\n",
        "# Tag the text using SpaCy\n",
        "nlp = spacy.load('en_core_web_sm', disable=[\"ner\", \"parser\", \"textcat\", \"entity_linker\", \"sentencizer\"])\n",
        "st = time.time()\n",
        "doc = nlp(text)\n",
        "#tags = [(token.text, token.pos_) for token in doc]\n",
        "et = time.time()\n",
        "time_spacy = et - st\n",
        "\n",
        "# Print the results\n",
        "print(f'NLTK: {time_nltk:.4f} seconds')\n",
        "print(f'SpaCy: {time_spacy:.4f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q42xtSmwo5FL"
      },
      "source": [
        "**Exercise:** Try changing this code to time sentence tokenization."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIHEgW6p1HZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8FwQQOhopdt"
      },
      "source": [
        "Keep in mind that the performance these functions may vary depending on the specific hardware and software environment meaning the results of the benchmark may not be directly comparable between different systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5cIIBudQys1"
      },
      "source": [
        "## **For your interest: Creating your own exceptions in SpaCy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPvBvDgYeTCl"
      },
      "source": [
        "Sometimes, different aspects of these libraries do not work quite the way you would like to. Both NLTK and SpaCy offer options to add your own rules and exceptions to handle the text in a customized way. Some examples are shown below (note that there are far more customization options than I show here). SpaCy has some good documentation on different possibilities for customization. Here is an example: https://spacy.io/usage/rule-based-matching. Or see https://spacy.io/api/tokenizer for more information on how to use the special case attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "Y5Wtbf1STpWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHOlN234ezMd",
        "outputId": "03567183-e389-4829-d464-ef7d13c5e573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We', 'are', 'going', 'to', 'the', 'U.S.']\n"
          ]
        }
      ],
      "source": [
        "from spacy.attrs import ORTH, NORM\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer.add_special_case(\"U.S.\", [{ORTH: \"U.S.\", NORM: \"U.S.\"}])\n",
        "\n",
        "test_text = \"We are going to the U.S.\"\n",
        "\n",
        "doc = nlp(test_text)\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2JeEpxTS_VB"
      },
      "source": [
        "ORTH is the exact verbatim text of a token. NORM is the normalized form of the text.\n",
        "\n",
        "An example that is already built into the standard SpaCy lemmatizer would be how to handle \"don't\", which would look like {ORTH: \"do\"} and {ORTH: \"n't\", NORM: \"not\"}.\n",
        "\n",
        "As another example, let's say we have the name/title of someone that we want to be considered a single entity. For example,\n",
        "This could be useful for organizations as well since these are not always properly recognized."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parts of Speech Tagging**"
      ],
      "metadata": {
        "id": "ertTkX2aTfET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkeA0dODJt07"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a pattern for the phrase\n",
        "pattern = [{'LOWER': 'alexander'}, {'LOWER': 'the'}, {'LOWER': 'great'}]\n",
        "\n",
        "# Add the phrase and pattern to the entity recognizer\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PERSON\", [pattern])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqIkAI7hKAwj",
        "outputId": "c64ae0e5-5dc9-45e7-bcb2-4c806a7d1f82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alexander the Great PERSON\n",
            "Greek NORP\n",
            "Macedon GPE\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Alexander the Great was a king of the ancient Greek kingdom of Macedon.\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional stop words**"
      ],
      "metadata": {
        "id": "p_BSVVbLTbZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en import STOP_WORDS\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# add new stop words\n",
        "custom_stop_words = {\"custom\", \"stop\", \"words\"}\n",
        "\n",
        "# add them to the list of stop words in this spaCy model\n",
        "for word in custom_stop_words:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    lexeme.is_stop = True\n",
        "\n",
        "sentences = [\n",
        "    \"This is a custom sentence.\",\n",
        "    \"The stop sign is red.\",\n",
        "    \"These words are important.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "    print(f\"Original sentence: {sentence}\")\n",
        "    print(\"Stop words in the sentence:\")\n",
        "    for token in doc:\n",
        "        if token.is_stop:\n",
        "            print(token.text)\n",
        "    print(\"----\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtbhv4jeSdVf",
        "outputId": "2f35a46a-29e8-45dc-a52f-1e8b34ca63b9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: This is a custom sentence.\n",
            "Stop words in the sentence:\n",
            "This\n",
            "is\n",
            "a\n",
            "custom\n",
            "----\n",
            "Original sentence: The stop sign is red.\n",
            "Stop words in the sentence:\n",
            "The\n",
            "stop\n",
            "is\n",
            "----\n",
            "Original sentence: These words are important.\n",
            "Stop words in the sentence:\n",
            "These\n",
            "words\n",
            "are\n",
            "----\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}