{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCV4ryRPzpIL"
      },
      "source": [
        "# **Working with SpaCy**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gPYGk6ohz9nX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f46042d-fe9d-473a-de9f-350e8f34459d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-15 20:04:28.109095: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 20:04:28.109215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 20:04:28.113443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 20:04:28.136915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-15 20:04:29.594855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2024-01-15 20:04:47.297054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 20:04:47.297125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 20:04:47.298734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 20:04:47.307401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-15 20:04:48.563709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting xx-ent-wiki-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.6.0/xx_ent_wiki_sm-3.6.0-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from xx-ent-wiki-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n"
          ]
        }
      ],
      "source": [
        "# installing SpaCy and it's language models\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download xx_ent_wiki_sm\n",
        "#!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JPnzq4GXzlJG"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew_dmbIC0Djq"
      },
      "source": [
        "## **SpaCy Pipelines**\n",
        "\n",
        "SpaCy has a variety of already established pipelines\n",
        "It is common convension to assign this pipeline object to the variable \"nlp\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QwS_Nvy4Iwb",
        "outputId": "802c1200-f2bf-4470-95b6-5350ccf143bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x789283501f30>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#call the variable to examine what this object looks like\n",
        "nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSmGRrxz4Sht"
      },
      "source": [
        "Here, we have loaded the small (12 MB) language model for English. So the nlp object basically contains a language model responsible for the tasks that it was trained to perform. You can take a look at the [SpaCy documentation](https://spacy.io/usage/spacy-101#pipelines) for a visualization of what this standard pipeline looks like. The tasks that can be performed with the small language model for English will depend on the components that have been put in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUqOIOzo8vUq",
        "outputId": "04e88f2e-bec3-4d4d-e5f6-61f8eb17168f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7892831a4e20>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7892831a4d60>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7892834e37d0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x789283433640>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x78928345a700>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7892834e33e0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#check pipeline components\n",
        "nlp.pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXTP6F608_lf"
      },
      "source": [
        "This should show you tuples of two different items. The first item of the tuple gives provides the name of a component that is part of the pipeline. Examples of components would be tagger, parser, lemmatizer, and ner. The second item is the actual component in SpaCy that is used to perform the task, whatever it may be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z3IA17R1Hcm"
      },
      "source": [
        "We can also design our own pipelines in SpaCy. Suppose we don't need to parse the text or we do not want to do named entity recognition. We can disable these features in our pipeline, which can be helpful in speeding up the pipeling when we are feeding in a lot of text to the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd9OQM-PkXQX",
        "outputId": "9e0e6280-0781-4fc4-cca7-1936b6fcd921"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7892830e2ce0>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7892830e2ec0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x789282311480>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7892820da340>)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
        "\n",
        "# What does this object look like?\n",
        "nlp.pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5E6u9XWvu_D"
      },
      "source": [
        "Let's say we want to design a pipeline from scratch. You can get started with a \"blank\" pipeline as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE7gUz_68WtB",
        "outputId": "a7613c89-525f-4883-8f3d-f4525dfc95ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#what does this object look like?\n",
        "nlp.pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNQCxju25rT6"
      },
      "source": [
        "As we can see, the blank pipeline has no components. However, it should be noted it will have a tokenizer by default as it is necessary to process any text passed to the nlp object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dZnIl9_49P8",
        "outputId": "47cd8d0e-ccaa-4876-a95d-40120b346e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The blank pipeline has a tokenizer.\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "if nlp.tokenizer is None:\n",
        "    print(\"The blank pipeline does not have a tokenizer.\")\n",
        "else:\n",
        "    print(\"The blank pipeline has a tokenizer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc33f4vnw4-N"
      },
      "source": [
        "We can pass text to a spaCy NLP object by using the nlp method and passing in the text as a string as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zJOIdizpxu52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b848fb-1707-4787-eec0-7c4c1876bc47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This is some text for preprocessing."
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "text = \"This is some text for preprocessing.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# you could also use the from_string method as well to do the same thing\n",
        "# doc = nlp.from_string(text)\n",
        "\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StVvVKTs2sqp"
      },
      "source": [
        "However, at this point, our pipeline is empty so we cannot perform any preprocessing steps besides tokenization. Try adding a couple of components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r_UPMLCd7Nmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d728c4b-f71b-47fd-ec6f-721dc39228cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.tagger.Tagger at 0x78926daa4160>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"tagger\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"William Shakespeare was born in Stratford-upon-Avon, England, in April 1564. He is widely regarded as one of the greatest playwrights and poets in the English language. Shakespeare's works, including Romeo and Juliet, Hamlet, and Macbeth, are celebrated for their profound insight into the human condition.\"\n",
        "nlp_en = spacy.blank(\"en\")\n",
        "nlp_xx = spacy.load(\"xx_ent_wiki_sm\")\n",
        "nlp_en.add_pipe(\"ner\", name=\"ner_xx\", source=nlp_xx)\n",
        "doc = nlp_en(text)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCiwNgEP-9N1",
        "outputId": "90f15e4e-7eac-418d-d376-6c3e62051bcf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(William Shakespeare, Stratford-upon-Avon, England, English language, Shakespeare, Romeo and Juliet, Hamlet, Macbeth)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVjA31dGCYiu"
      },
      "source": [
        "\n",
        "A list of texts can be fed to a pipeline using the `pipe()` method, which can also be useful when processing a large number of documents. There are options to handle the documents in batches as well as updating the number of threads used to process the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJe1L3_njKsP",
        "outputId": "29586c84-ef36-46d7-c25f-ac4cbcfc6069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'some', 'text', '.']\n",
            "['Another', 'text', 'here', '.']\n",
            "['And', 'one', 'more', 'text', '.']\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "texts = [\"This is some text.\", \"Another text here.\", \"And one more text.\"]\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIhtpIS03AgW"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "As you may see from the cell above, when you use the `nlp()` method on a text, the tokenizer component is automatically applied as the first step in the pipeline (even if using a blank pipeline), and the resulting Doc object contains a sequence of token objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ_HYL_n6duz",
        "outputId": "ce59cdec-a9ce-4717-a811-3040ad920d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This\n",
            "is\n",
            "some\n",
            "example\n",
            "text\n",
            "to\n",
            "tokenize\n",
            ".\n",
            "It\n",
            "contains\n",
            "two\n",
            "sentences\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"This is some example text to tokenize. It contains two sentences.\"\n",
        "\n",
        "#pass the text to the nlp object to perform tokenization\n",
        "doc = nlp(text)\n",
        "\n",
        "#iterate over the tokens in the doc object\n",
        "for token in doc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eWqbFP38j7B",
        "outputId": "755bb461-2829-4dd8-813e-76ee63969221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is some example text to tokenize.\n",
            "It contains two sentences.\n"
          ]
        }
      ],
      "source": [
        "for sent in doc.sents:\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy has what is referred to as \"non-destructive tokenization\", meaning you can always have access to the original text (it won't carve up the text stream into little pieces)."
      ],
      "metadata": {
        "id": "3WD8ITu8qfyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc.sents:\n",
        "  print(\">\", sent.start, sent.end)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BerdnWtmq6d6",
        "outputId": "1740f5e5-4a4d-4a35-aab1-2b878d3b8f16"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> 0 8\n",
            "> 8 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPUxpR7BmvW",
        "outputId": "afa10e7f-fdaf-4416-cc3d-f13345d6db43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokenization: ['After', 'today', ',', 'I', \"'ll\", 'never', 'call', 'you', 'a', \"ne'er-do-well\", 'again', '.']\n",
            "spaCy Tokenization: ['After', 'today', ',', 'I', \"'ll\", 'never', 'call', 'you', 'a', \"ne'er\", '-', 'do', '-', 'well', 'again', '.']\n"
          ]
        }
      ],
      "source": [
        "text = [\n",
        "    '\"Can you see the snow-capped mountains?\" asked Martha. \"I can\\'t,\" replied Xavier.',\n",
        "    \"Get me those T.P.S. reports A.S.A.P., Mr. O'Donohue!\",\n",
        "    \"After today, I'll never call you a ne'er-do-well again.\",\n",
        "    \"What's the frequency, Kenneth?\"\n",
        "]\n",
        "\n",
        "#tokenization using NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokens_nltk = nltk.word_tokenize(text[2])\n",
        "print(\"NLTK Tokenization:\", tokens_nltk)\n",
        "\n",
        "#tokenization using spaCy\n",
        "doc = nlp(text[2])\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "print(\"spaCy Tokenization:\", tokens_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE4hhwMmrgrl"
      },
      "source": [
        "What is the difference in the number of stop words offered by NLTK and SpaCy? What are the words that are different between these two lists of words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ao7z_9SHrvdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2c433f-de5c-4105-a470-73be1bd04b35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " 'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anyhow',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'are',\n",
              " 'around',\n",
              " 'as',\n",
              " 'at',\n",
              " 'back',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'but',\n",
              " 'by',\n",
              " 'ca',\n",
              " 'call',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'could',\n",
              " 'did',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'done',\n",
              " 'down',\n",
              " 'due',\n",
              " 'during',\n",
              " 'each',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'enough',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'except',\n",
              " 'few',\n",
              " 'fifteen',\n",
              " 'fifty',\n",
              " 'first',\n",
              " 'five',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forty',\n",
              " 'four',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'further',\n",
              " 'get',\n",
              " 'give',\n",
              " 'go',\n",
              " 'had',\n",
              " 'has',\n",
              " 'have',\n",
              " 'he',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'however',\n",
              " 'hundred',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'indeed',\n",
              " 'into',\n",
              " 'is',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'keep',\n",
              " 'last',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'least',\n",
              " 'less',\n",
              " 'made',\n",
              " 'make',\n",
              " 'many',\n",
              " 'may',\n",
              " 'me',\n",
              " 'meanwhile',\n",
              " 'might',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'much',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " \"n't\",\n",
              " 'name',\n",
              " 'namely',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'next',\n",
              " 'nine',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'none',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'n‘t',\n",
              " 'n’t',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'or',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 'part',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'please',\n",
              " 'put',\n",
              " 'quite',\n",
              " 'rather',\n",
              " 're',\n",
              " 'really',\n",
              " 'regarding',\n",
              " 'same',\n",
              " 'say',\n",
              " 'see',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'serious',\n",
              " 'several',\n",
              " 'she',\n",
              " 'should',\n",
              " 'show',\n",
              " 'side',\n",
              " 'since',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhere',\n",
              " 'still',\n",
              " 'such',\n",
              " 'take',\n",
              " 'ten',\n",
              " 'than',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'thereupon',\n",
              " 'these',\n",
              " 'they',\n",
              " 'third',\n",
              " 'this',\n",
              " 'those',\n",
              " 'though',\n",
              " 'three',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'to',\n",
              " 'together',\n",
              " 'too',\n",
              " 'top',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'twelve',\n",
              " 'twenty',\n",
              " 'two',\n",
              " 'under',\n",
              " 'unless',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'us',\n",
              " 'used',\n",
              " 'using',\n",
              " 'various',\n",
              " 'very',\n",
              " 'via',\n",
              " 'was',\n",
              " 'we',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'whatever',\n",
              " 'when',\n",
              " 'whence',\n",
              " 'whenever',\n",
              " 'where',\n",
              " 'whereafter',\n",
              " 'whereas',\n",
              " 'whereby',\n",
              " 'wherein',\n",
              " 'whereupon',\n",
              " 'wherever',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'whither',\n",
              " 'who',\n",
              " 'whoever',\n",
              " 'whole',\n",
              " 'whom',\n",
              " 'whose',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'would',\n",
              " 'yet',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " '‘d',\n",
              " '‘ll',\n",
              " '‘m',\n",
              " '‘re',\n",
              " '‘s',\n",
              " '‘ve',\n",
              " '’d',\n",
              " '’ll',\n",
              " '’m',\n",
              " '’re',\n",
              " '’s',\n",
              " '’ve'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "stop_words = nlp.Defaults.stop_words\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "spacy_stopwords = set(nlp.Defaults.stop_words)\n",
        "\n",
        "common_stopwords = nltk_stopwords.intersection(spacy_stopwords)\n",
        "\n",
        "nltk_specific_stopwords = nltk_stopwords.difference(spacy_stopwords)\n",
        "spacy_specific_stopwords = spacy_stopwords.difference(nltk_stopwords)\n",
        "\n",
        "#print(len(nltk_stopwords))\n",
        "#print(len(spacy_stopwords))\n",
        "#print(\"Common stopwords:\", common_stopwords)\n",
        "print(\"NLTK specific stopwords:\", nltk_specific_stopwords)\n",
        "print(\"SpaCy specific stopwords:\", spacy_specific_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQg_3j_kRKde",
        "outputId": "86bc5554-c0b4-4565-9f3b-5f74cfb7d3f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK specific stopwords: {'won', \"weren't\", \"doesn't\", 't', 'theirs', 'o', \"aren't\", \"couldn't\", \"don't\", 'needn', 'wouldn', 'ain', 'mustn', \"it's\", 'having', 'hadn', \"should've\", 'ma', \"she's\", 'isn', 'shouldn', 've', 'y', 'hasn', \"isn't\", 'weren', 'aren', 'd', \"mightn't\", 'll', \"won't\", \"you're\", \"you'd\", \"you'll\", 'mightn', 'couldn', 'doesn', 'm', \"hadn't\", 'shan', \"haven't\", \"mustn't\", \"that'll\", \"wasn't\", \"needn't\", \"hasn't\", 'wasn', \"didn't\", 'don', \"you've\", 'didn', \"wouldn't\", \"shan't\", 'haven', \"shouldn't\", 's'}\n",
            "SpaCy specific stopwords: {'show', 'throughout', '’ll', \"'ve\", 'whenever', 'ten', 'beforehand', 'much', 'name', 'third', 'however', '‘ve', 'otherwise', 'get', 'thus', 'call', 'along', 'side', 'make', 'among', '‘re', 'cannot', 'anyway', 'herein', 'becomes', 'every', 'eleven', 'really', 'amongst', 'might', 'within', 'whatever', 'besides', '‘d', 'another', 'becoming', 'many', 'therein', 'whence', 'whereupon', '’ve', 'already', 'fifty', 'made', 'else', 'top', 'almost', 'seem', 'enough', 'yet', 'twelve', 'something', 'nothing', 'must', 'whose', 'due', '‘ll', 'together', 'someone', 'put', '’re', 'afterwards', 'mine', 'whether', 'bottom', 'thence', 'next', 'except', 'since', 'beside', 'therefore', 'namely', 'meanwhile', 'move', 'nobody', 'hence', 'hereby', 'always', 'may', \"'re\", 'sometime', 'see', 'sixty', 'though', 'wherein', '’d', 'neither', 'around', 'well', 'empty', 'behind', 'formerly', 'whole', 'mostly', 'thru', 'give', \"'d\", 'six', 'toward', 'rather', 'full', \"'m\", 'regarding', \"'s\", 'would', 'twenty', 'anyone', 'two', 'whereby', 'four', 'elsewhere', 'towards', 'everywhere', 'even', \"'ll\", 'could', 'via', 'anywhere', 'per', 'fifteen', 'ca', 'please', 'either', '’m', 'somewhere', '‘m', 'five', 'became', 'first', '’s', 'others', 'hundred', 'nevertheless', 'amount', 'latter', '‘s', 'onto', 'forty', 'nowhere', 'seeming', 'anything', 'seemed', 'alone', 'serious', 'never', 'us', 'less', 'latterly', 'although', 'quite', 'using', 'say', 'take', 'various', 'noone', 'thereupon', 'thereafter', 'n‘t', 'last', 'seems', 'without', 'hereafter', 'whereas', 'go', 'ever', 'beyond', \"n't\", 'part', 'least', 'done', 'become', 'across', 'perhaps', 'none', 'often', 'moreover', 'three', 'former', 'eight', 'also', 'sometimes', 'keep', 'anyhow', 'unless', 'wherever', 'whither', 'front', 'one', 'thereby', 'somehow', 'upon', 'everyone', 'hereupon', 'everything', 'still', 'indeed', 'back', 'whereafter', 'several', 'nine', 'whoever', 'n’t', 'used'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVevm_Es3VUZ"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "The default small language model does lemmatization by default so it is easy to extract lemmas (root or reduced form of a word) after creating the doc object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj5GFrnEkK9h",
        "outputId": "dd3f9dbd-a3ad-48a0-868c-4d67cb8ef6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "playing play\n",
            "played play\n",
            "play play\n",
            "player player\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"playing played play player\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XApPdTE3DbY"
      },
      "source": [
        "## POS Tagging and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nn4Rmyd5JpI",
        "outputId": "8fcf6fcd-e209-4a41-9e01-a4151d5fb856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET\n",
            "quick ADJ\n",
            "brown ADJ\n",
            "fox NOUN\n",
            "jumped VERB\n",
            "over ADP\n",
            "the DET\n",
            "lazy ADJ\n",
            "dog NOUN\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"The quick brown fox jumped over the lazy dog\"\"\"\n",
        "text = nlp(text)\n",
        "for w in text:\n",
        "    print (w, w.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAxqT6m_9-6A",
        "outputId": "7ee6f4af-47a6-4a26-f0ff-977d6376f50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox\n",
            "the lazy dog\n"
          ]
        }
      ],
      "source": [
        "for noun in text.noun_chunks:\n",
        "    print(noun.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EQYFWh6jods",
        "outputId": "51cfd070-6e46-4ac6-aadb-ca78ce8adbd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox | fox | nsubj | jumped\n",
            "the lazy dog | dog | pobj | over\n"
          ]
        }
      ],
      "source": [
        "for chunk in text.noun_chunks:\n",
        "    print(f\"{chunk.text} | {chunk.root.text} | {chunk.root.dep_} | {chunk.root.head.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6c-fSI84wyx"
      },
      "source": [
        "## Name Entity Recognition\n",
        "\n",
        "Named Entity Recognition (NER) aims to identify and classify named entities in text into predefined categories such as person names, organizations, locations, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Iqhimd5trR",
        "outputId": "6cf1dcee-104c-481c-88a2-e6a511103832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple - ORG - Companies, agencies, institutions, etc.\n",
            "today - DATE - Absolute or relative dates or periods\n",
            "$1 billion - MONEY - Monetary values, including unit\n",
            "Cupertino - GPE - Countries, cities, states\n",
            "California - GPE - Countries, cities, states\n",
            "Apple - ORG - Companies, agencies, institutions, etc.\n"
          ]
        }
      ],
      "source": [
        "# Define the text to be processed\n",
        "text = \"Apple CEO announced today that the company will be investing $1 billion in a new research and development facility in Cupertino, California, the home of Apple's headquarters.\"\n",
        "doc = nlp(text)\n",
        "for entity in doc.ents:\n",
        "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dejK-1mA7Ji"
      },
      "source": [
        "SpaCy also supports some visualization techniques that can be useful for looking through the output of ner, dependency parsing etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8wxn8zpG-4ko",
        "outputId": "3d1c721a-7542-4252-ea08-3d52abec370c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " CEO announced \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    today\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " that the company will be investing \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " in a new research and development facility in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    California\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", the home of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s headquarters.</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Pass the text to the nlp object to process it\n",
        "doc = nlp(text)\n",
        "\n",
        "# Use the displacy.render method to visualize the parse tree\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb8gIRGQBlnX"
      },
      "source": [
        "**Custom Components**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "def get_upper(doc):\n",
        "    #convert all text in the document to uppercase\n",
        "    return [token.text.upper() for token in doc]\n",
        "\n",
        "#add the custom property to the Doc class\n",
        "Doc.set_extension(\"upper\", getter=get_upper, force=True)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "doc = nlp(\"This is a custom SpaCy extension to make every token uppercase.\")\n",
        "uppercase_text = doc._.upper #Doc._.xxxxx\n",
        "print(uppercase_text)\n",
        "print(doc.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLK-wn9snv3F",
        "outputId": "e0d25872-5572-4993-d208-28e2785673f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['THIS', 'IS', 'A', 'CUSTOM', 'SPACY', 'EXTENSION', 'TO', 'MAKE', 'EVERY', 'TOKEN', 'UPPERCASE', '.']\n",
            "This is a custom SpaCy extension to make every token uppercase.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}