{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall sklearn_crfsuite\n",
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evfyV98Lxk2S",
        "outputId": "f2fd9365-a7e4-4796-b214-4dda42097052"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping sklearn_crfsuite as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting sklearn_crfsuite\n",
            "  Cloning https://github.com/MeMartijn/updated-sklearn-crfsuite.git to /tmp/pip-install-bffy6r7o/sklearn-crfsuite_09acaf6b3da24cd09b9243dd4f3fc670\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MeMartijn/updated-sklearn-crfsuite.git /tmp/pip-install-bffy6r7o/sklearn-crfsuite_09acaf6b3da24cd09b9243dd4f3fc670\n",
            "  Resolved https://github.com/MeMartijn/updated-sklearn-crfsuite.git to commit 675038761b4405f04691a83339d04903790e2b95\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.66.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn_crfsuite\n",
            "  Building wheel for sklearn_crfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn_crfsuite: filename=sklearn_crfsuite-0.3.6-py2.py3-none-any.whl size=10866 sha256=40ac38ff7bc54a523b3908c0f574c2e8e73be532907050c59160da4a00d43343\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4s_9qpez/wheels/0b/bc/07/bd75a6f5fa2bf2ea05a5aad8d9ac66d2b5aab93dfd4e1a89de\n",
            "Successfully built sklearn_crfsuite\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.10 sklearn_crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpJ56B0vLCTi",
        "outputId": "7c880b37-ea95-44a7-aa9b-ec737fd1a14a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Extraction"
      ],
      "metadata": {
        "id": "4yuBKXK23UYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now we have mostly focused on classification problems using BOW or TF-IDF representations of text. However, for some tasks this is not enough. Sometimes it is beneficial to know information about words that come before or after other words of a sentence. Named Entity Extraction is one of those tasks. We have looked at NER during our preprocessing lessons but now we will look into another option of training an NER model ourselves."
      ],
      "metadata": {
        "id": "GQ7DzhRE3axk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition"
      ],
      "metadata": {
        "id": "rQiugeB7VlKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model that can classify words as specific entities we will use Conditional Random Fields (CRF). With the CRF, we use a labeled dataset of input sequences and corresponding label sequences, and apply maximum likelihood estimation to learn the weights of the features that maximize the likelihood of the observed label sequences.\n",
        "\n",
        "The following code comes from the textbook: https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch5/02_NERTraining.ipynb"
      ],
      "metadata": {
        "id": "nRrenF-KTH9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from sklearn_crfsuite import CRF, metrics\n",
        "#from sklearn.metrics import make_scorer,confusion_matrix\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "import warnings\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol9M67VFTYRI",
        "outputId": "fd756c92-a317-4628-9b8f-1b041e2f0fd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the training/testing data.\n",
        "input: conll format data, but with only 2 tab separated colums - words and NEtags.\n",
        "output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n",
        "\"\"\"\n",
        "def load__data_conll(file_path):\n",
        "    myoutput,words,tags = [],[],[]\n",
        "    fh = open(file_path)\n",
        "    for line in fh:\n",
        "        line = line.strip()\n",
        "        if \"\\t\" not in line:\n",
        "            #Sentence ended.\n",
        "            myoutput.append([words,tags])\n",
        "            words,tags = [],[]\n",
        "        else:\n",
        "            word, tag = line.split(\"\\t\")\n",
        "            words.append(word)\n",
        "            tags.append(tag)\n",
        "    fh.close()\n",
        "    return myoutput"
      ],
      "metadata": {
        "id": "Whd6Y1Q23ZW2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore what the following function does to a sentence. Try it with some sentences of your own.**"
      ],
      "metadata": {
        "id": "4c2Y8z0lT0db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2feats(sentence):\n",
        "    feats = []\n",
        "    sen_tags = pos_tag(sentence)\n",
        "    for i in range(0,len(sentence)):\n",
        "        word = sentence[i]\n",
        "        wordfeats = {}\n",
        "        wordfeats['word'] = word\n",
        "        if i == 0:\n",
        "            wordfeats[\"prevWord\"] = wordfeats[\"prevSecondWord\"] = \"<S>\"\n",
        "        elif i==1:\n",
        "            wordfeats[\"prevWord\"] = sentence[0]\n",
        "            wordfeats[\"prevSecondWord\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"prevWord\"] = sentence[i-1]\n",
        "            wordfeats[\"prevSecondWord\"] = sentence[i-2]\n",
        "        if i == len(sentence)-2:\n",
        "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
        "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
        "        elif i==len(sentence)-1:\n",
        "            wordfeats[\"nextWord\"] = \"</S>\"\n",
        "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
        "            wordfeats[\"nextNextWord\"] = sentence[i+2]\n",
        "\n",
        "        wordfeats['tag'] = sen_tags[i][1]\n",
        "        if i == 0:\n",
        "            wordfeats[\"prevTag\"] = wordfeats[\"prevSecondTag\"] = \"<S>\"\n",
        "        elif i == 1:\n",
        "            wordfeats[\"prevTag\"] = sen_tags[0][1]\n",
        "            wordfeats[\"prevSecondTag\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n",
        "\n",
        "            wordfeats[\"prevSecondTag\"] = sen_tags[i - 2][1]\n",
        "\n",
        "        if i == len(sentence) - 2:\n",
        "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
        "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
        "        elif i == len(sentence) - 1:\n",
        "            wordfeats[\"nextTag\"] = \"</S>\"\n",
        "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
        "            wordfeats[\"nextNextTag\"] = sen_tags[i + 2][1]\n",
        "        #That is it! You can add whatever you want!\n",
        "        feats.append(wordfeats)\n",
        "    return feats"
      ],
      "metadata": {
        "id": "mn1lpn90TuYD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aagh7RtOeysd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract features from the conll data, after loading it.\n",
        "def get_feats_conll(conll_data):\n",
        "    feats = []\n",
        "    labels = []\n",
        "    for sentence in conll_data:\n",
        "        feats.append(sent2feats(sentence[0]))\n",
        "        labels.append(sentence[1])\n",
        "    return feats, labels"
      ],
      "metadata": {
        "id": "1QfsLTvjYkqq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train the CRF model."
      ],
      "metadata": {
        "id": "yxpkajU1Uq7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a sequence model\n",
        "def train_seq(X_train,Y_train,X_dev,Y_dev):\n",
        "    crf = CRF(algorithm='lbfgs', c1=0.1, c2=10, max_iterations=50)#, all_possible_states=True)\n",
        "    #Just to fit on training data\n",
        "    crf.fit(X_train, Y_train)\n",
        "    labels = list(crf.classes_)\n",
        "    #testing:\n",
        "    y_pred = crf.predict(X_dev)\n",
        "    sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
        "    print(\"Overall F1 score: \", metrics.flat_f1_score(Y_dev, y_pred,average='weighted', labels=labels))\n",
        "    print(metrics.flat_classification_report(Y_dev, y_pred, labels=sorted_labels, digits=3))\n",
        "    #get_confusion_matrix(Y_dev, y_pred,labels=sorted_labels)"
      ],
      "metadata": {
        "id": "d2Sjzr9mUuqD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'train.txt'\n",
        "test_path = 'test.txt'\n",
        "\n",
        "conll_train = load__data_conll(train_path)\n",
        "conll_dev = load__data_conll(test_path)\n",
        "\n",
        "print(\"Training a sequence classification model with CRF\")\n",
        "feats, labels = get_feats_conll(conll_train)\n",
        "devfeats, devlabels = get_feats_conll(conll_dev)\n",
        "train_seq(feats, labels, devfeats, devlabels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQFlOCdvVGY3",
        "outputId": "9314265a-0740-402f-bcd4-c1337c3696e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training a sequence classification model with CRF\n",
            "Overall F1 score:  0.9255163144785534\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.973     0.981     0.977     38289\n",
            "       B-LOC      0.694     0.765     0.728      1667\n",
            "       I-LOC      0.738     0.482     0.584       257\n",
            "      B-MISC      0.650     0.310     0.419       701\n",
            "      I-MISC      0.624     0.505     0.558       214\n",
            "       B-ORG      0.670     0.561     0.610      1660\n",
            "       I-ORG      0.551     0.704     0.618       834\n",
            "       B-PER      0.773     0.766     0.769      1616\n",
            "       I-PER      0.819     0.886     0.851      1156\n",
            "\n",
            "    accuracy                          0.928     46394\n",
            "   macro avg      0.721     0.662     0.679     46394\n",
            "weighted avg      0.926     0.928     0.926     46394\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity Linking"
      ],
      "metadata": {
        "id": "sTogt9jtVcwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entity Linking is the challenge of resolving ambiguous textual mentions to unique concepts in a knowledge base. The full tutorial also has a video on SpaCy:\n",
        "\n",
        "Video: https://spacy.io/universe/project/video-entity-linking\n",
        "\n",
        "Notebook: https://github.com/explosion/projects/blob/v3/tutorials/nel_emerson/notebooks/notebook_video.ipynb"
      ],
      "metadata": {
        "id": "k6ofqE_aojO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy==3.0.6\n",
        "!pip install spacy-lookups-data\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ8dNdBbqczy",
        "outputId": "8ca3bb6e-ff49-4eb5-ac8c-587ae4a1b2b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-lookups-data\n",
            "  Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy-lookups-data) (67.7.2)\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.5\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(f\"Named Entity '{ent.text}' with label '{ent.label_}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuKi41T81Ame",
        "outputId": "5bbba75e-6a00-492a-d41f-eb67bf6585af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity 'Emerson' with label 'PERSON'\n",
            "Named Entity 'Wimbledon' with label 'EVENT'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def load_entities():\n",
        "    entities_loc = \"entities.csv\"\n",
        "\n",
        "    names = dict()\n",
        "    descriptions = dict()\n",
        "    with open(entities_loc, \"r\", encoding=\"utf8\") as csvfile:\n",
        "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
        "        for row in csvreader:\n",
        "            qid = row[0]\n",
        "            name = row[1]\n",
        "            desc = row[2]\n",
        "            names[qid] = name\n",
        "            descriptions[qid] = desc\n",
        "    return names, descriptions"
      ],
      "metadata": {
        "id": "LeOuGoeH74tI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_dict, desc_dict = load_entities()\n",
        "for QID in name_dict.keys():\n",
        "    print(f\"{QID}, name={name_dict[QID]}, desc={desc_dict[QID]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYAhUJsI8EaC",
        "outputId": "d02c1c27-0af8-4948-a6c6-97b7c3e47ffc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q312545, name=Roy Stanley Emerson, desc=Australian tennis player\n",
            "Q48226, name=Ralph Waldo Emerson, desc=American philosopher, essayist, and poet\n",
            "Q215952, name=Emerson Ferreira da Rosa, desc=Brazilian footballer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 3 entries here, of 3 different people called Emerson. The first step to perform Entity Linking, is to set up a knowledge base that contains the unique identifiers of the entities we are interested in."
      ],
      "metadata": {
        "id": "4KT4hRm98gbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from spacy.kb import KnowledgeBase\n",
        "#kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)\n",
        "\n",
        "from spacy.kb import InMemoryLookupKB\n",
        "kb = InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=300)\n",
        "\n"
      ],
      "metadata": {
        "id": "Rab4lS4r8b_P"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will add our entities to our knowledgebase. We provide the qid, the entity vector, and freq (Estimate of the frequency of the entity in a typical corpus)."
      ],
      "metadata": {
        "id": "P9GqOrfZnRL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for qid, desc in desc_dict.items():\n",
        "    desc_doc = nlp(desc)\n",
        "    desc_enc = desc_doc.vector\n",
        "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)   # 342 is an arbitrary value here"
      ],
      "metadata": {
        "id": "Yi0PoYZJ8lth"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first add the full names. Here, we are 100% certain that they resolve to their corresponding QID, as there is no ambiguity."
      ],
      "metadata": {
        "id": "BnIqohm9n3EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for qid, name in name_dict.items():\n",
        "    kb.add_alias(alias=name, entities=[qid], probabilities=[1])   # 100% prior probability P(entity|alias)"
      ],
      "metadata": {
        "id": "8_P9m6fx8qOc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to add the alias \"Emerson\". We'll assume that each of our 3 Emersons is equally famous and thus we set their probabilities to be equal for each entity."
      ],
      "metadata": {
        "id": "c_WImyF5n4Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qids = name_dict.keys()\n",
        "probs = [0.3 for qid in qids]\n",
        "kb.add_alias(alias=\"Emerson\", entities=qids, probabilities=probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BltGVel-8x4l",
        "outputId": "33b11e1c-ed20-4959-f25c-ab8c89000e41"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4831166512461469197"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this will be our Knowledge base. We can check the entities and aliases that are contained in it:"
      ],
      "metadata": {
        "id": "uz-fkOuz82-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Entities in the KB: {kb.get_entity_strings()}\")\n",
        "print(f\"Aliases in the KB: {kb.get_alias_strings()}\")\n",
        "print(f\"Candidates for 'Roy Stanley Emerson': {[c.entity_ for c in kb.get_alias_candidates('Roy Stanley Emerson')]}\")\n",
        "print(f\"Candidates for 'Emerson': {[c.entity_ for c in kb.get_alias_candidates('Emerson')]}\")\n",
        "print(f\"Candidates for 'Bob': {[c.entity_ for c in kb.get_alias_candidates('Sofie')]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovtlAX7u83sN",
        "outputId": "07ea088e-01c7-401e-e192-039ecb858c31"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities in the KB: ['Q215952', 'Q312545', 'Q48226']\n",
            "Aliases in the KB: ['Roy Stanley Emerson', 'Emerson Ferreira da Rosa', 'Ralph Waldo Emerson', 'Emerson']\n",
            "Candidates for 'Roy Stanley Emerson': ['Q312545']\n",
            "Candidates for 'Emerson': ['Q312545', 'Q48226', 'Q215952']\n",
            "Candidates for 'Bob': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQiNeRD80i3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HxgHniJAjmfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the directory and file names to whatever you like\n",
        "import os\n",
        "output_dir = Path.cwd().parent / \"my_output\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "kb.to_disk(output_dir / \"my_kb\")"
      ],
      "metadata": {
        "id": "0ZE56FktjnD-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk(output_dir / \"my_nlp\")"
      ],
      "metadata": {
        "id": "JotYxUM9jskq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_loc = \"emerson_annotated_text.jsonl\"\n",
        "with open(json_loc, \"r\", encoding=\"utf8\") as jsonfile:\n",
        "    line = jsonfile.readline()\n",
        "    print(line)   # print just the first line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k53HBoVh8__S",
        "outputId": "4e7d5a87-0029-4449-f7e5-333318336815"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\"Interestingly, Emerson is one of only five tennis players all-time to win multiple slam sets in two disciplines, only matched by Frank Sedgman, Margaret Court, Martina Navratilova and Serena Williams.\",\"_input_hash\":2024197919,\"_task_hash\":-1926469210,\"spans\":[{\"start\":15,\"end\":22,\"text\":\"Emerson\",\"rank\":0,\"label\":\"ORG\",\"score\":1,\"source\":\"en_core_web_lg\",\"input_hash\":2024197919}],\"meta\":{\"score\":1},\"options\":[{\"id\":\"Q48226\",\"html\":\"<a href='https://www.wikidata.org/wiki/Q48226'>Q48226: American philosopher, essayist, and poet</a>\"},{\"id\":\"Q215952\",\"html\":\"<a href='https://www.wikidata.org/wiki/Q215952'>Q215952: Brazilian footballer</a>\"},{\"id\":\"Q312545\",\"html\":\"<a href='https://www.wikidata.org/wiki/Q312545'>Q312545: Australian tennis player</a>\"},{\"id\":\"NIL_otherLink\",\"text\":\"Link not in options\"},{\"id\":\"NIL_ambiguous\",\"text\":\"Need more context\"}],\"_session_id\":null,\"_view_id\":\"choice\",\"accept\":[\"Q312545\"],\"answer\":\"accept\"}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "with open(json_loc, \"r\", encoding=\"utf8\") as jsonfile:\n",
        "    for line in jsonfile:\n",
        "        example = json.loads(line)\n",
        "        text = example[\"text\"]\n",
        "        if example[\"answer\"] == \"accept\":\n",
        "            QID = example[\"accept\"][0]\n",
        "            offset = (example[\"spans\"][0][\"start\"], example[\"spans\"][0][\"end\"])\n",
        "            entity_label = example[\"spans\"][0][\"label\"]\n",
        "            entities = [(offset[0], offset[1], entity_label)]\n",
        "            links_dict = {QID: 1.0}\n",
        "        dataset.append((text, {\"links\": {offset: links_dict}, \"entities\": entities}))"
      ],
      "metadata": {
        "id": "IyzbXXJw9eDK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check what this looks like."
      ],
      "metadata": {
        "id": "ARtaHSHC9qwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9JNccBr9miv",
        "outputId": "e50d5153-a726-431e-9068-cf1a4d8e325e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Interestingly, Emerson is one of only five tennis players all-time to win multiple slam sets in two disciplines, only matched by Frank Sedgman, Margaret Court, Martina Navratilova and Serena Williams.',\n",
              " {'links': {(15, 22): {'Q312545': 1.0}}, 'entities': [(15, 22, 'ORG')]})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How many cases do we have annotated?**"
      ],
      "metadata": {
        "id": "EgRcHjuM93mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_ids = []\n",
        "for text, annot in dataset:\n",
        "    for span, links_dict in annot[\"links\"].items():\n",
        "        for link, value in links_dict.items():\n",
        "            if value:\n",
        "                gold_ids.append(link)\n",
        "\n",
        "from collections import Counter\n",
        "print(Counter(gold_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDUvb1a7-TPg",
        "outputId": "b293f937-167f-4132-91b9-55619cf8c2b4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'Q312545': 10, 'Q48226': 10, 'Q215952': 10})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the training and test dataset."
      ],
      "metadata": {
        "id": "69fJBnmuhMXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "train_dataset = []\n",
        "test_dataset = []\n",
        "for QID in qids:\n",
        "    indices = [i for i, j in enumerate(gold_ids) if j == QID]\n",
        "    train_dataset.extend(dataset[index] for index in indices[0:8])  # first 8 in training\n",
        "    test_dataset.extend(dataset[index] for index in indices[8:10])  # last 2 in test\n",
        "\n",
        "random.shuffle(train_dataset)\n",
        "random.shuffle(test_dataset)"
      ],
      "metadata": {
        "id": "qKj2DNDa-EPq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJssC-w_RCNI",
        "outputId": "f3cd1b2b-9ba5-40b1-e3bf-6a16f86f4083"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Carlyle in particular was a strong influence on him; Emerson would later serve as an unofficial literary agent in the United States for Carlyle, and in March 1835, he tried to persuade Carlyle to come to America to lecture.',\n",
              "  {'links': {(53, 60): {'Q48226': 1.0}}, 'entities': [(53, 60, 'ORG')]}),\n",
              " ('In 1841 Emerson published Essays, his second book, which included the famous essay \"Self-Reliance\".',\n",
              "  {'links': {(8, 15): {'Q48226': 1.0}}, 'entities': [(8, 15, 'PERSON')]}),\n",
              " (\"Emerson's first Wimbledon singles title came in 1964, with a final victory over Fred Stolle.\",\n",
              "  {'links': {(0, 7): {'Q312545': 1.0}}, 'entities': [(0, 7, 'ORG')]}),\n",
              " ('Emerson was inducted into the International Tennis Hall of Fame in 1982 and the Sport Australia Hall of Fame in 1986.',\n",
              "  {'links': {(0, 7): {'Q312545': 1.0}}, 'entities': [(0, 7, 'ORG')]}),\n",
              " ('Emerson made his Brazil debut on 10 September 1997, in a home friendly match against Ecuador, in Salvador, Bahia, also scoring a goal in the match, as Brazil went on to win 4-2.',\n",
              "  {'links': {(0, 7): {'Q215952': 1.0}}, 'entities': [(0, 7, 'ORG')]}),\n",
              " ('Emerson scored his second international goal on 31 March 1999, in a friendly match against Japan in Tokyo, which Brazil won 2-0.',\n",
              "  {'links': {(0, 7): {'Q215952': 1.0}}, 'entities': [(0, 7, 'ORG')]})]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our datasets now properly set up, we'll now create Example objects to feed into the training process. This object is new in spaCy v3. Essentially, it contains a document with predictions (predicted) and one with gold-standard annotations (reference). During training, the pipeline will compare its predictions to the gold-standard, and update the weights of the neural network accordingly.\n",
        "\n",
        "For entity linking, the algorithm needs access to gold-standard sentences, because the algorithms use the context from the sentence to perform the disambiguation. You can either provide gold-standard sent_starts annotations, or run a component such as the parser or sentencizer on your reference documents:"
      ],
      "metadata": {
        "id": "d4rvjqrshcd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.training import Example\n",
        "\n",
        "TRAIN_EXAMPLES = []\n",
        "if \"sentencizer\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "sentencizer = nlp.get_pipe(\"sentencizer\")\n",
        "for text, annotation in train_dataset:\n",
        "    example = Example.from_dict(nlp.make_doc(text), annotation)\n",
        "    example.reference = sentencizer(example.reference)\n",
        "    TRAIN_EXAMPLES.append(example)"
      ],
      "metadata": {
        "id": "KRpN7gS8hdcx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll create a new Entity Linking component and add it to the pipeline.\n",
        "\n",
        "We also need to make sure the entity_linker component is properly initialized. To do this, we need a get_examples function that returns some example training data, as well as a kb_loader argument."
      ],
      "metadata": {
        "id": "87a7_TethrTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.ml.models import load_kb\n",
        "\n",
        "entity_linker = nlp.add_pipe(\"entity_linker\", config={\"incl_prior\": False}, last=True)\n",
        "entity_linker.initialize(get_examples=lambda: TRAIN_EXAMPLES, kb_loader=load_kb(output_dir / \"my_kb\"))"
      ],
      "metadata": {
        "id": "Ceg7Vzg7hqsN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will run the actual training loop for the new component, taking care to only train the entity linker and not the other components."
      ],
      "metadata": {
        "id": "71HEZRQMiYs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import minibatch\n",
        "# batch size\n",
        "batch_size = 32\n",
        "\n",
        "with nlp.select_pipes(enable=[\"entity_linker\"]):\n",
        "    optimizer = nlp.resume_training()\n",
        "\n",
        "    prev_training_loss = float('inf')\n",
        "    patience = 5  # consecutive iterations allowed with no improvement\n",
        "    count_no_improvement = 0\n",
        "\n",
        "    for itn in range(100):\n",
        "        random.shuffle(TRAIN_EXAMPLES)\n",
        "\n",
        "        # training\n",
        "        batches = minibatch(TRAIN_EXAMPLES, size=batch_size)\n",
        "        losses = {}\n",
        "        for batch in batches:\n",
        "            nlp.update(\n",
        "                batch,\n",
        "                drop=0.2,       # prevent overfitting\n",
        "                losses=losses,\n",
        "                sgd=optimizer,\n",
        "            )\n",
        "\n",
        "        #calculate average training loss\n",
        "        avg_training_loss = losses.get(\"entity_linker\", float('inf'))\n",
        "\n",
        "        print(f\"Iteration {itn}, Training Loss: {avg_training_loss}\")\n",
        "\n",
        "        #check for early stopping based on training loss (you could also add a validation set)\n",
        "        if avg_training_loss >= prev_training_loss:\n",
        "            count_no_improvement += 1\n",
        "        else:\n",
        "            count_no_improvement = 0\n",
        "\n",
        "        if count_no_improvement >= patience:\n",
        "            print(f\"Stopping early at iteration {itn} as training loss did not improve.\")\n",
        "            break\n",
        "\n",
        "        prev_training_loss = avg_training_loss\n",
        "\n",
        "print(f\"Final Iteration {itn}, Training Loss: {avg_training_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD8lgdD0TiIK",
        "outputId": "028910cc-c13c-4bdf-edfe-f75dc197f81e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Training Loss: 0.2325533390045166\n",
            "Stopping early at iteration 41 as training loss did not improve.\n",
            "Final Iteration 41, Training Loss: 0.24009428024291993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text, true_annot in test_dataset:\n",
        "    print(text)\n",
        "    print(f\"Gold annotation: {true_annot}\")\n",
        "    doc = nlp(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
        "    for ent in doc.ents:\n",
        "        if ent.text == \"Emerson\":\n",
        "            print(f\"Prediction: {ent.text}, {ent.label_}, {ent.kb_id_}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHd3pi0tiuZZ",
        "outputId": "12cc38fd-dd89-402c-ab28-7da6de0adb72"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carlyle in particular was a strong influence on him; Emerson would later serve as an unofficial literary agent in the United States for Carlyle, and in March 1835, he tried to persuade Carlyle to come to America to lecture.\n",
            "Gold annotation: {'links': {(53, 60): {'Q48226': 1.0}}, 'entities': [(53, 60, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "In 1841 Emerson published Essays, his second book, which included the famous essay \"Self-Reliance\".\n",
            "Gold annotation: {'links': {(8, 15): {'Q48226': 1.0}}, 'entities': [(8, 15, 'PERSON')]}\n",
            "Prediction: Emerson, ORG, Q48226\n",
            "\n",
            "Emerson's first Wimbledon singles title came in 1964, with a final victory over Fred Stolle.\n",
            "Gold annotation: {'links': {(0, 7): {'Q312545': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q48226\n",
            "\n",
            "Emerson was inducted into the International Tennis Hall of Fame in 1982 and the Sport Australia Hall of Fame in 1986.\n",
            "Gold annotation: {'links': {(0, 7): {'Q312545': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "Emerson made his Brazil debut on 10 September 1997, in a home friendly match against Ecuador, in Salvador, Bahia, also scoring a goal in the match, as Brazil went on to win 4-2.\n",
            "Gold annotation: {'links': {(0, 7): {'Q215952': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "Emerson scored his second international goal on 31 March 1999, in a friendly match against Japan in Tokyo, which Brazil won 2-0.\n",
            "Gold annotation: {'links': {(0, 7): {'Q215952': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_, ent.kb_id_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9nZzI2mo_Pa",
        "outputId": "acd02f60-0c5c-4fe4-cfe8-1dcc9d68fe56"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emerson PERSON Q215952\n",
            "Wimbledon EVENT NIL\n"
          ]
        }
      ]
    }
  ]
}