{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Word2Vec in Gensim**"
      ],
      "metadata": {
        "id": "6hUB2m63WnBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec is a technique that is able to learn distributed vector representations of words using a large corpus of text. It was developed by researchers at Google and has been widely used for natural language processing tasks."
      ],
      "metadata": {
        "id": "ADR7pRIBWs8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install packages if required\n",
        "#!pip install scikit-learn\n",
        "#!pip install gensim\n",
        "#!pip install spacy"
      ],
      "metadata": {
        "id": "Jr5qlqzzbBuT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some different models built with different texts available through Gensim."
      ],
      "metadata": {
        "id": "fyR8K5eQjsnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tip:** You may want to check the size of these pretrained models before you decide if you want to load them into memory or not. You can do this without loading as shown below."
      ],
      "metadata": {
        "id": "eOjK1CN4L9Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "print(\"\\n\")\n",
        "gensim.downloader.info('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoH7_R9MMIow",
        "outputId": "20d109fd-38fc-4d19-a826-811453d6699c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_records': 3000000,\n",
              " 'file_size': 1743563840,\n",
              " 'base_dataset': 'Google News (about 100 billion words)',\n",
              " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
              " 'license': 'not found',\n",
              " 'parameters': {'dimension': 300},\n",
              " 'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              " 'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "  'https://arxiv.org/abs/1301.3781',\n",
              "  'https://arxiv.org/abs/1310.4546',\n",
              "  'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              " 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              " 'file_name': 'word2vec-google-news-300.gz',\n",
              " 'parts': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can load a pretrained Word2Vec model with the following:"
      ],
      "metadata": {
        "id": "6UBQkH2hMwZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to download the \"word2vec-google-news-300\" embeddings\n",
        "w2v_model = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "gln2ZIg5js3h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since these models can be quite big, you may want to remove them from working memory after you are done with them\n",
        "# del w2v_model"
      ],
      "metadata": {
        "id": "rFuqsJPL-Rcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFDPxWqwWaxu",
        "outputId": "1cba6170-275f-4daa-d9b3-5df91a0afe90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in corpus:  3000000\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of words in corpus: \",len(w2v_model.index_to_key)) # Number of words in the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the vector representation for a word?\n",
        "print(w2v_model['computer'])\n",
        "\n",
        "print(\"Length of word vector: \", len(w2v_model['computer']))"
      ],
      "metadata": {
        "id": "KnYKvkZZeoH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305c75ac-4d25-42a1-f6fd-6895cc2d2866"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
            " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
            "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
            "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
            " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
            " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
            "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
            "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
            " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
            " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
            " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
            "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
            " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
            "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
            "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
            "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
            " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
            " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
            "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
            "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
            "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
            "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
            " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
            " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
            "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
            "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
            " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
            "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
            " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
            " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
            " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
            " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
            "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
            " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
            "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
            "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
            " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
            " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
            " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
            " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
            " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
            " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
            " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
            " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
            "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
            " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
            "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
            "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
            " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
            "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
            " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
            "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
            " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
            " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
            " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
            " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
            " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
            " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
            " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
            "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
            "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
            " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
            "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
            " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
            "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
            " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
            "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
            " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
            " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
            " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
            "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
            "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
            "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
            "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
            "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n",
            "Length of word vector:  300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the similarity between 'cat' and 'dog'\n",
        "print(\"Similarity between cat and dog: \", w2v_model.similarity('cat', 'dog'))\n",
        "\n",
        "print(\"Similarity between cat and computer: \", w2v_model.similarity('cat', 'computer'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhJfCniBpCo8",
        "outputId": "36868c20-c3b6-4ead-b79f-ac6c3b707cde"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between cat and dog:  0.76094574\n",
            "Similarity between cat and computer:  0.17324439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = w2v_model.most_similar(negative=[\"woman\"], positive=[\"king\", \"queen\"])\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMlkjlRm7f9K",
        "outputId": "7a61c047-8004-483f-9d5d-28cfc2b51277"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('kings', 0.6367637515068054),\n",
              " ('monarch', 0.5600019693374634),\n",
              " ('queens', 0.5444643497467041),\n",
              " ('princes', 0.5285636782646179),\n",
              " ('royal', 0.510769784450531),\n",
              " ('prince', 0.4869095981121063),\n",
              " ('NYC_anglophiles_aflutter', 0.4691288471221924),\n",
              " ('crown_prince', 0.46789005398750305),\n",
              " ('Savory_aromas_wafted', 0.4651806354522705),\n",
              " ('royals', 0.46382129192352295)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# phrase in this word2vec model\n",
        "print(w2v_model['proctocolitis'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "6MluqoNPLABr",
        "outputId": "3242475f-0cec-4631-fa34-d71f36ff10eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'proctocolitis' not present\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2a1b4668a67a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# phrase in this word2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proctocolitis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute will be removed in 4.0.0, use self.vectors instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msyn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mInput\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0muse_norm\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mresulting\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnormalized\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m \u001b[0meuclidean\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \"\"\"Save KeyedVectors.\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'proctocolitis' not present\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us examine the model by knowing what the most similar words are, for a given word\n",
        "# by default gives top 10\n",
        "w2v_model.most_similar('horse', topn=5)"
      ],
      "metadata": {
        "id": "-fM6LxG3ekMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f84ea14-e6cf-41d2-917b-0f7d60b3e3b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('horses', 0.8654032945632935),\n",
              " ('racehorse', 0.752392590045929),\n",
              " ('stallion', 0.7200170159339905),\n",
              " ('thoroughbred', 0.7158915400505066),\n",
              " ('horseman', 0.6845748424530029)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have also seen that SpaCy can give us a word or sentence representation."
      ],
      "metadata": {
        "id": "gXwR5OFfrQbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# process a sentence using the model\n",
        "mydoc = nlp(\"Canada is a large country\")\n",
        "#Get a vector for individual words\n",
        "#print(doc[0].vector) #vector for 'Canada', the first word in the text\n",
        "print(mydoc.vector) #Averaged vector for the entire sentence"
      ],
      "metadata": {
        "id": "1uwtUHgTe6hD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e5004a-eb19-4242-b4ec-04be40c2dd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.44864374e-01 -1.56845257e-01 -5.19747622e-02  5.86494267e-01\n",
            "  8.10811967e-02 -1.65754989e-01  7.57052720e-01  2.63185889e-01\n",
            "  1.40734492e-02  2.51211464e-01  2.43307427e-01 -2.79111534e-01\n",
            " -3.70179832e-01  5.22314429e-01 -5.23915410e-01  4.84695425e-03\n",
            "  4.30857569e-01 -2.19760254e-01 -3.72532457e-01  1.71566337e-01\n",
            " -2.67529279e-01  2.24802848e-02 -3.03287357e-01 -1.04288436e-01\n",
            "  1.51315406e-01 -5.31261384e-01  4.36048269e-01  2.97305524e-01\n",
            "  4.72418487e-01  3.90211403e-01  2.69951403e-01  2.36672014e-01\n",
            "  4.59462464e-01 -4.97865111e-01 -1.82451054e-01 -1.67997599e-01\n",
            "  1.93978697e-01  5.16766071e-01 -2.88335413e-01  3.74710053e-01\n",
            " -1.11499667e-01  3.33659947e-01  5.49611822e-02  2.53970414e-01\n",
            " -5.02043903e-01  3.85194987e-01 -1.86397389e-01  8.60191345e-01\n",
            "  2.11835742e-01 -1.24764726e-01 -7.09948778e-01  7.70933092e-01\n",
            " -1.79754198e-01 -6.63751960e-01 -4.01271343e-01  1.83464423e-01\n",
            " -2.96254933e-01  7.63848484e-01 -3.35624158e-01 -1.81755573e-01\n",
            "  5.62856086e-02 -5.20981967e-01  2.32470542e-01  7.93596357e-02\n",
            "  1.17028512e-01 -1.91331297e-01  2.83491552e-01  3.67665291e-03\n",
            "  3.98660034e-01 -5.55250108e-01  2.48089619e-02  2.39709094e-01\n",
            "  3.95606980e-02 -3.22076678e-01 -6.04971290e-01 -7.20350385e-01\n",
            " -9.31409597e-02 -8.10149968e-01 -5.13003230e-01 -3.73546213e-01\n",
            " -2.60708272e-01  1.43927217e-01 -2.70993322e-01 -6.63320005e-01\n",
            "  6.42718398e-04  1.37684375e-01  2.20302776e-01  4.54716794e-02\n",
            " -2.09064916e-01  3.36762726e-01 -1.24832727e-01 -4.11891073e-01\n",
            "  1.14090490e+00 -2.19411142e-02 -4.46765989e-01  4.73975614e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training from Scratch Using Gensim**"
      ],
      "metadata": {
        "id": "4kPCVzX9axGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, Word2Vec as a pretrained model is not enough.\n",
        "\n",
        "**Word2Vec cannot handle OOV words.** Perhaps the original version was not trained on a large portion of the vocabulary you are interested in (for example, you are working with text that has a lot of scientific or medical vocab). Perhaps your vocabulary is nothing like the text used for training Word2Vec making it completely useless e.g., graph embedding where you would like a representation of the nodes of a graph.\n",
        "\n",
        "It is quite simple to train Word2Vec from scratch using Gensim on your own vocabulary.\n",
        "\n",
        "Below is an example of training a Word2Vec model with Gensim."
      ],
      "metadata": {
        "id": "UMIao3hGcicb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Format: Gensim's Word2Vec requires a list of lists type of input. Every document is a list of tokens for that document, and these documents are then also stored in a list.**"
      ],
      "metadata": {
        "id": "dzrKxQUFiIXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# define training data\n",
        "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
        "\n",
        "#Training the model\n",
        "model_cbow = Word2Vec(corpus, vector_size=100, window=3, min_count=1, sg=0)\n",
        "\n",
        "# Save the model (if you want)\n",
        "# model_cbow.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "9zyUd-lhchq-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the Word2Vec definition [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec). What do the following parameters mean?\n",
        "\n",
        "\n",
        "*   vector_size\n",
        "*   window\n",
        "*   min_count\n",
        "*   sg"
      ],
      "metadata": {
        "id": "i54_XTYxfHNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it is possible to train Word2Vec without having any knowledge of the underlying architecture. However, it is useful to have some idea of what each hyperparameter means and what these paparmeters reference to in the training procedure as well as how the training data should be prepared. Having this knowledge is also beneficial if something goes wrong or behaviour isn't as expected!"
      ],
      "metadata": {
        "id": "6qryYQ79cvfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "model = FastText(corpus, min_count=1, max_n=3)\n",
        "vector = model.wv['dog']  # get vector for word\n",
        "# vector\n",
        "# model.wv.most_similar('and')\n"
      ],
      "metadata": {
        "id": "YdTDivajjfee"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Can we update a model we already have?**\n",
        "Yes!"
      ],
      "metadata": {
        "id": "7r6B0PXph1dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "old_sentences = [[\"bad\",\"robots\"],[\"good\",\"human\"]]\n",
        "\n",
        "old_model = Word2Vec(old_sentences, vector_size = 4, window=5, min_count = 1)\n",
        "print(old_model.wv.key_to_index)\n",
        "\n",
        "old_model.save(\"old_model\")\n",
        "new_model = Word2Vec.load(\"old_model\")\n",
        "\n",
        "new_sentences = [['yes', 'this', 'is', 'the', 'word2vec', 'model'],[ 'if',\"you\",\"have\",\"think\",\"about\",\"it\"]]\n",
        "new_model.build_vocab(new_sentences, update = True)\n",
        "new_model.train(new_sentences, total_examples=2, epochs=old_model.epochs)\n",
        "print(new_model.wv.key_to_index)"
      ],
      "metadata": {
        "id": "iz4gYg-Ph0Dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c8610b-cc3e-4ac6-8154-13661a3ccfe8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'human': 0, 'good': 1, 'robots': 2, 'bad': 3}\n",
            "{'human': 0, 'good': 1, 'robots': 2, 'bad': 3, 'yes': 4, 'this': 5, 'is': 6, 'the': 7, 'word2vec': 8, 'model': 9, 'if': 10, 'you': 11, 'have': 12, 'think': 13, 'about': 14, 'it': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training from Scratch Using Pytorch (More Advanced)**"
      ],
      "metadata": {
        "id": "oBkEN5Eke9l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just a simple example of how you could implement a word2vec model from scratch using PyTorch. There are many variations of the word2vec model that you could explore, such as using subword embeddings or incorporating additional context information into the model."
      ],
      "metadata": {
        "id": "8vHiz1b6fCdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture for Word2Vec is reletively simple (~4 lines of important statements to set up how it works) and below we set up a blueprint that we can use to initialize the network"
      ],
      "metadata": {
        "id": "9sAMTPuRxGrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "JqByygwdz3Z0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "      # first, we are defining the word2vec class as a child class of Module in pytorch so we can inherit its methods\n",
        "        super().__init__()\n",
        "        # this is our embedding layer for the words we input to convert to a one-hot-encoding input\n",
        "        # and project the weights from the hidden layer\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "        # this is our activation function (we discussed it being linear in class)\n",
        "        # we also remove the bias/intercept with bias=False since we apply\n",
        "        # softmax for rescaling anyway\n",
        "        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Encode input to lower-dimensional representation\n",
        "        hidden = self.embed(input)\n",
        "        # Expand hidden layer to predictions\n",
        "        logits = self.expand(hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "t3SKolNCxFRT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is it!\n",
        "\n",
        "Now, let's test this out with some data. First, we will prepare the data and make them into integers instead of strings. Gensim does this part for us automatically but with Pytorch we will have to do this step ourselves."
      ],
      "metadata": {
        "id": "bsXG_VFV0fqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the text data\n",
        "text = \"The farm was home to a variety of animals, each with their own distinct personalities and characteristics. The cows were docile and hardworking, providing the farm with milk and cream. The pigs were intelligent and ambitious, often vying for more power and control. The chickens clucked and pecked around the barnyard, laying eggs for the farm's breakfast. The horses were strong and proud, plowing the fields and carrying heavy loads. The sheep were gentle and timid, content to graze in the meadow. But the true leader of the farm was a pig named Napoleon, who through manipulation and deceit, rose to power and convinced the other animals to overthrow their human owner and run the farm themselves, with the pigs as the ruling class in George Orwell's Animal Farm.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Create a vocabulary of unique words\n",
        "vocab = set(tokens)\n",
        "\n",
        "# Create training data\n",
        "data = []\n",
        "window_size = 2\n",
        "for i, word in enumerate(tokens):\n",
        "    for j in range(i-window_size, i+window_size+1):\n",
        "        if i != j and 0 <= j < len(tokens):\n",
        "            data.append((word, tokens[j]))\n"
      ],
      "metadata": {
        "id": "fGo18B3G08pV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0547ca-744e-4dd3-cf60-82bab3a841f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make use of DataLoader for preparing our training dataset so we are going to make each of our words an integer."
      ],
      "metadata": {
        "id": "pv6uudKziSzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a mapping from integers to words\n",
        "id2tok = dict(enumerate(vocab))\n",
        "\n",
        "# Create a mapping from words to integers\n",
        "word2int = {word: i for i, word in id2tok.items()}\n",
        "\n",
        "# Convert words to integers\n",
        "data = [(word2int[word[0]], word2int[word[1]]) for word in data]\n",
        "\n",
        "# Create a Pytorch dataloader\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "s4YpiftTiOco"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_size = 100\n",
        "model = Word2Vec(vocab_size=len(vocab), embedding_size=feature_size)\n",
        "\n",
        "# Relevant if you have a GPU you want to use, we will ignore this step\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "#model.to(device)\n",
        "\n",
        "# Training parameters\n",
        "learning = 3e-4\n",
        "epochs = 200\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning)"
      ],
      "metadata": {
        "id": "x3eTitfSymPV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the actual training!"
      ],
      "metadata": {
        "id": "hcmRUSaGzYZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "running_loss = []\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for i, (center, context) in enumerate(dataloader):\n",
        "      # again, the commented to(device) code is only if you want to make use of GPU\n",
        "        #center, context = center.to(device), context.to(device)\n",
        "        #print(center, context)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input=center)\n",
        "        loss = loss_fn(logits, context)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward() # This is where we backpropogate and update the weights of the network\n",
        "        optimizer.step()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    running_loss.append(epoch_loss)\n",
        "\n",
        "print(running_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aerDelwDzbxa",
        "outputId": "9e54c073-93e9-4986-c795-c4575cff9031"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.602848906266062, 4.526897079066226, 4.47815317856638, 4.46295135899594, 4.367240039925826, 4.352342555397435, 4.28582333263598, 4.266253345891049, 4.214065300790887, 4.174345317639802, 4.134563596625077, 4.083735842453806, 4.070022319492541, 4.0348262159447925, 3.9781655135907625, 3.956468193154586, 3.928064835698981, 3.8824787516342965, 3.855940241562693, 3.8301482953523336, 3.8001373316112317, 3.7539776877353064, 3.7328179259049263, 3.7080609672947933, 3.667464996639051, 3.649512253309551, 3.605811508078324, 3.589195728302002, 3.574406536001908, 3.5508708577407035, 3.5093746687236584, 3.4813657183396187, 3.467848627190841, 3.4519452044838355, 3.38978153780887, 3.392011203263935, 3.37001895904541, 3.344103223399112, 3.3390895190991854, 3.3091216463791695, 3.2933091991826107, 3.272037204943205, 3.2467315071507503, 3.233396517603021, 3.2131396594800448, 3.188509050168489, 3.171923926002101, 3.1782631497634086, 3.152783055054514, 3.118756369540566, 3.088004375758924, 3.0869644190135754, 3.07946420970716, 3.0710159853885046, 3.022783731159411, 3.0318809308503805, 3.0354267045071253, 3.01285635797601, 2.996948580992849, 2.994137249494854, 2.9593324159321033, 2.959264642313907, 2.9441820194846704, 2.922451985509772, 2.8983961030056604, 2.8913127748589766, 2.909376533407914, 2.8680404738376013, 2.8521989019293534, 2.850215610704924, 2.852765020571257, 2.814208482441149, 2.795697099284122, 2.799861581701981, 2.805378085688541, 2.775048168081986, 2.7789988894211617, 2.7485058307647705, 2.75114033096715, 2.7695786576522026, 2.7146692777934827, 2.7562205665989925, 2.736641005465859, 2.7193899405630013, 2.6805475636532434, 2.6720817716498124, 2.6671115724663985, 2.6814973605306527, 2.64607394369025, 2.6556096955349573, 2.637820369318912, 2.6224419317747416, 2.640612890845851, 2.6271237825092517, 2.6542437202052067, 2.6274118172494987, 2.6098335291209973, 2.594013716045179, 2.6027189555921053, 2.5888189265602515, 2.5838154993559184, 2.576120740488956, 2.596184956400018, 2.57031912552683, 2.551319486216495, 2.5724904537200928, 2.538091910512824, 2.548520703064768, 2.536032375536467, 2.5434318969124243, 2.5017078801205286, 2.5186074281993664, 2.512770878641229, 2.504299414785285, 2.534683014217176, 2.4806498728300395, 2.487016113180863, 2.478717979631926, 2.4657567174811112, 2.488292041577791, 2.4659319425884045, 2.4849440800516227, 2.4769500933195414, 2.476032119048269, 2.4547840419568514, 2.456005673659475, 2.4302519873568884, 2.4308919153715434, 2.461566874855443, 2.4796119489167867, 2.424267292022705, 2.4282134081188, 2.4278600717845715, 2.4309766418055485, 2.4232687824650814, 2.4066898571817497, 2.4031625044973275, 2.415430683838694, 2.3992077049456144, 2.3795211189671566, 2.387581988384849, 2.4120151369195235, 2.389918415169967, 2.4018182378066215, 2.3743874775735954, 2.3757869444395365, 2.36299467086792, 2.3580032147859273, 2.3730839302665307, 2.3781933031584086, 2.373546976792185, 2.3621015548706055, 2.3377553475530526, 2.381412305329975, 2.3796239401164807, 2.350236553894846, 2.3377661516791894, 2.333529974284925, 2.32669052324797, 2.3335834427883753, 2.318084578765066, 2.3260412843603837, 2.3239544316342005, 2.355989845175492, 2.317502542545921, 2.3147745508896675, 2.294834042850294, 2.3265809761850456, 2.3048083280262195, 2.292043792574029, 2.301479753695036, 2.319232225418091, 2.288863125600313, 2.2774084179024947, 2.3317950273814954, 2.302353959334524, 2.284820820155897, 2.2857649326324463, 2.2839523993040385, 2.2577173145193803, 2.304526216105411, 2.289355001951519, 2.2901708766033777, 2.2901392359482613, 2.2775822564175257, 2.301903818783007, 2.2806539221813806, 2.257331183082179, 2.287446323194002, 2.2491355946189477, 2.2769333563352885, 2.252643083271227, 2.2658421428580033, 2.250576188689784, 2.272389606425637, 2.2624399285567436, 2.250500327662418, 2.26052282985888, 2.2684667863343893, 2.284075379371643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the running_loss values, we are slowly yet surely decreasing our error. However, do our representations make sense yet? We can check on our representations below for each word in our vocabulary."
      ],
      "metadata": {
        "id": "XtOt8F0-0jiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordvecs = model.expand.weight.cpu().detach().numpy() #just want the vectors now so we detach from tensor object\n",
        "print(wordvecs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS-562yY9Kda",
        "outputId": "6f4fdaed-460c-4ed9-efa2-450d053976c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.09599644e-01 -2.69629732e-02 -2.12840706e-01 -8.20920914e-02\n",
            " -5.04826829e-02 -3.78768966e-02  1.14996642e-01  1.83683857e-02\n",
            " -9.31274071e-02  1.45465389e-01  7.30959848e-02  2.29303213e-03\n",
            " -4.28109281e-02  1.80684477e-01 -8.86503831e-02  1.25743851e-01\n",
            "  1.21448645e-02  1.26253385e-02 -1.44656032e-01  9.45089385e-02\n",
            "  1.58332035e-01 -2.33851315e-04 -2.56919172e-02  2.18188763e-02\n",
            "  1.17239535e-01  3.13984901e-02  6.82145134e-02  6.63065091e-02\n",
            " -8.44094157e-02  5.30883633e-02 -8.95121619e-02  3.00137289e-02\n",
            " -3.41570377e-02  6.10541329e-02  7.55806714e-02 -1.33047402e-01\n",
            " -1.12024911e-01  1.59632504e-01 -5.45366332e-02 -4.07549702e-02\n",
            " -1.56891435e-01 -8.51264149e-02  5.40869348e-02 -1.22832097e-02\n",
            "  7.58123621e-02  2.73967050e-02  1.27216890e-01  7.79426470e-02\n",
            "  1.95566490e-01  1.71155930e-01  1.32297695e-01  2.25129843e-01\n",
            " -3.56455147e-02 -4.44173217e-02 -7.88564757e-02 -3.94883566e-02\n",
            "  1.27443865e-01 -1.21899888e-01 -1.04736909e-01  1.27263665e-01\n",
            " -5.06845117e-02 -1.94851950e-01 -1.33503094e-01 -5.67758083e-03\n",
            " -7.51572698e-02  5.07518649e-02  1.69834673e-01 -2.16145511e-03\n",
            " -6.28081709e-03 -2.92552084e-01  5.15171960e-02  1.19559728e-01\n",
            " -2.01355934e-01 -1.47973478e-01  2.95431539e-02 -1.01521827e-01\n",
            " -5.39135896e-02 -3.37147713e-02 -1.34210989e-01 -9.26180780e-02\n",
            "  1.12849660e-02  9.67098325e-02 -1.34685829e-01 -4.66570891e-02\n",
            "  3.26140188e-02  1.23606242e-01  9.47199613e-02  1.37423590e-01\n",
            " -1.75489366e-01 -9.45805907e-02  2.69727092e-02  1.13108963e-01\n",
            "  1.51437223e-01 -1.10394597e-01  4.57983874e-02  1.11574151e-01\n",
            "  1.11102648e-01  1.07662687e-02 -5.00907637e-02  1.16186850e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have some code to calculate the cosine distance between vectors, which we will cover more when we cover visualization methods."
      ],
      "metadata": {
        "id": "lt4QlrWiigj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "\n",
        "words_of_interest = ['Napoleon', 'horses']\n",
        "\n",
        "def get_distance_matrix(wordvecs, metric):\n",
        "    dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n",
        "    return dist_matrix\n",
        "\n",
        "def get_k_similar_words(word, dist_matrix, k=5):\n",
        "    idx = word2int[word]\n",
        "    dists = dist_matrix[idx]\n",
        "    ind = np.argpartition(dists, k)[:k+1]\n",
        "    ind = ind[np.argsort(dists[ind])][1:]\n",
        "    out = [(i, id2tok[i], dists[i]) for i in ind]\n",
        "    return out\n",
        "\n",
        "dmat = get_distance_matrix(wordvecs, 'cosine')\n",
        "\n",
        "for word in words_of_interest:\n",
        "    print(word, [t[1] for t in get_k_similar_words(word, dmat)], \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvRf8EQMA8o6",
        "outputId": "af668f0a-226d-4362-f9a1-082e21d80b6f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Napoleon ['named', 'through', 'pig', 'manipulation', 'a'] \n",
            "\n",
            "horses ['sheep', 'cows', 'control', 'characteristics', 'chickens'] \n",
            "\n"
          ]
        }
      ]
    }
  ]
}