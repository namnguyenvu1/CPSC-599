{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Intent Detection Using LSTM**"
      ],
      "metadata": {
        "id": "g50uyYU9CsNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example of intent detection is available in the Textbook: https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch6/01_CNN_RNN_ATIS_intents.ipynb\n",
        "\n",
        "The code has been updated due to keras restructuring and an equivalent in Pytorch has been added."
      ],
      "metadata": {
        "id": "E22fkvtzk0Tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ssnBnOnclGQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_hepYs4vChU8"
      },
      "outputs": [],
      "source": [
        "#general imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "random.seed(0) #for reproducability of results\n",
        "\n",
        "#basic imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#NN imports\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "\n",
        "#encoder\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "J1uypwGslIxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_data(filename):\n",
        "    df = pd.read_csv(filename,delim_whitespace=True,names=['word','label'])\n",
        "    beg_indices = list(df[df['word'] == 'BOS'].index)+[df.shape[0]]\n",
        "    sents,labels,intents = [],[],[]\n",
        "    for i in range(len(beg_indices[:-1])):\n",
        "        sents.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['word'].values)\n",
        "        labels.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['label'].values)\n",
        "        intents.append(df.loc[beg_indices[i+1]-1]['label'])    \n",
        "    return np.array(sents, dtype=object), np.array(labels, dtype=object), np.array(intents, dtype=object)\n",
        "\n",
        "def get_data2(filename):\n",
        "    with open(filename) as f:\n",
        "        contents = f.read()\n",
        "    sents,labels,intents = [],[],[]\n",
        "    for line in contents.strip().split('\\n'):\n",
        "        words,labs = [i.split(' ') for i in line.split('\\t')]\n",
        "        sents.append(words[1:-1])\n",
        "        labels.append(labs[1:-1])\n",
        "        intents.append(labs[-1])\n",
        "    return np.array(sents, dtype=object), np.array(labels, dtype=object), np.array(intents, dtype=object)"
      ],
      "metadata": {
        "id": "5ymMFwsSGOhn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents,labels,intents = get_data2('atis.train.w-intent.iob')\n",
        "\n",
        "train_sentences = [\" \".join(i) for i in sents]\n",
        "\n",
        "train_texts = train_sentences\n",
        "train_labels= intents.tolist()\n",
        "\n",
        "vals = []\n",
        "\n",
        "for i in range(len(train_labels)):\n",
        "    if \"#\" in train_labels[i]:\n",
        "        vals.append(i)\n",
        "        \n",
        "for i in vals[::-1]:\n",
        "    train_labels.pop(i)\n",
        "    train_texts.pop(i)\n",
        "\n",
        "print (\"Number of training sentences :\",len(train_texts))\n",
        "print (\"Number of unique intents :\",len(set(train_labels)))\n",
        "\n",
        "for i in zip(train_texts[:5], train_labels[:5]):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfDbTZoBDuVF",
        "outputId": "4f4f43a9-c7eb-4af9-dc4d-2d374d4e419e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences : 4952\n",
            "Number of unique intents : 17\n",
            "('i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', 'atis_flight')\n",
            "('what flights are available from pittsburgh to baltimore on thursday morning', 'atis_flight')\n",
            "('what is the arrival time in san francisco for the 755 am flight leaving washington', 'atis_flight_time')\n",
            "('cheapest airfare from tacoma to orlando', 'atis_airfare')\n",
            "('round trip fares from pittsburgh to philadelphia under 1000 dollars', 'atis_airfare')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents,labels,intents = get_data('atis.test.w-intent.iob')\n",
        "\n",
        "test_sentences = [\" \".join(i) for i in sents]\n",
        "\n",
        "test_texts = test_sentences\n",
        "test_labels = intents.tolist()\n",
        "\n",
        "new_labels = set(test_labels) - set(train_labels)\n",
        "\n",
        "vals = []\n",
        "\n",
        "for i in range(len(test_labels)):\n",
        "    if \"#\" in test_labels[i]:\n",
        "        vals.append(i)\n",
        "    elif test_labels[i] in new_labels:\n",
        "        print(test_labels[i])\n",
        "        vals.append(i)\n",
        "        \n",
        "for i in vals[::-1]:\n",
        "    test_labels.pop(i)\n",
        "    test_texts.pop(i)\n",
        "\n",
        "print (\"Number of testing sentences :\",len(test_texts))\n",
        "print (\"Number of unique intents :\",len(set(test_labels)))\n",
        "\n",
        "for i in zip(test_texts[:5], test_labels[:5]):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bykLVSr6D0KL",
        "outputId": "9155d05b-be80-4853-ed89-4376897b385e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "atis_day_name\n",
            "atis_day_name\n",
            "Number of testing sentences : 876\n",
            "Number of unique intents : 15\n",
            "('i would like to find a flight from charlotte to las vegas that makes a stop in st. louis', 'atis_flight')\n",
            "('on april first i need a ticket from tacoma to san jose departing before 7 am', 'atis_airfare')\n",
            "('on april first i need a flight going from phoenix to san diego', 'atis_flight')\n",
            "('i would like a flight traveling one way from phoenix to san diego on april first', 'atis_flight')\n",
            "('i would like a flight from orlando to salt lake city for april first on delta airlines', 'atis_flight')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 300\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.3"
      ],
      "metadata": {
        "id": "5HVIM1HAD8Ut"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djn-ApegD_u_",
        "outputId": "147fa93b-a50a-488f-9024-7f1a006b0a0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 897 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(train_labels)\n",
        "train_labels = le.transform(train_labels)\n",
        "test_labels = le.transform(test_labels)"
      ],
      "metadata": {
        "id": "0tBQmR75EE8q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding sequences"
      ],
      "metadata": {
        "id": "9ea7Ym45lQ9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding sequences involves adding zeros (or any other designated padding value) to the beginning or end of a sequence so that all sequences have the same length. This is necessary for many machine learning models, such as neural networks, which require inputs to have a fixed shape.\n",
        "\n",
        "In the code snippet you provided, the pad_sequences function is used to pad the training and test sequences to a maximum sequence length of MAX_SEQUENCE_LENGTH. This is done using zeros as the padding value. The resulting padded sequences are then split into training and validation sets using train_test_split.\n",
        "\n",
        "After padding, the data is converted into a one-hot encoded format using the to_categorical function. The resulting arrays are then shuffled and split into training and validation sets using train_test_split based on a validation split percentage specified by VALIDATION_SPLIT."
      ],
      "metadata": {
        "id": "30cPCFShnesJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        " #initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "trainvalid_labels = to_categorical(train_labels)\n",
        "\n",
        "test_labels = to_categorical(np.asarray(test_labels), num_classes= trainvalid_labels.shape[1])\n",
        "\n",
        "# split the training data into a training set and a validation set\n",
        "indices = np.arange(trainvalid_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "trainvalid_data = trainvalid_data[indices]\n",
        "trainvalid_labels = trainvalid_labels[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
        "x_train = trainvalid_data[:-num_validation_samples]\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\n",
        "x_val = trainvalid_data[-num_validation_samples:]\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\n",
        "#This is the data we will use for CNN and RNN training\n",
        "print('Splitting the train data into train and valid is done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WzKl7ppEM0I",
        "outputId": "a6db1da0-a951-4568-f824-d6674172d8af"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting the train data into train and valid is done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training an LSTM using Keras"
      ],
      "metadata": {
        "id": "-v_YLmpDlUDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "rnnmodel.summary()\n",
        "\n",
        "print('Training the RNN')\n",
        "rnnmodel.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRu34Ez9E2K5",
        "outputId": "321c06fb-45c3-45d2-ca7a-9657e673b9b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 17)                2193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,693,777\n",
            "Trainable params: 2,693,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training the RNN\n",
            "109/109 [==============================] - 130s 1s/step - loss: 0.1603 - accuracy: 0.7263 - val_loss: 0.1032 - val_accuracy: 0.7333\n",
            "28/28 [==============================] - 4s 156ms/step - loss: 0.1152 - accuracy: 0.7215\n",
            "Test accuracy with RNN: 0.7214611768722534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similar Pytorch Equivalent"
      ],
      "metadata": {
        "id": "oUFkue9glaGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or if you want to do something similar in Pytorch see the code below. First, we need to prepare the data in a way that Pytorch expects. We can still use the padded sequences, but we"
      ],
      "metadata": {
        "id": "ipM804LDLajV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "trainvalid_labels = to_categorical(train_labels)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "trainvalid_data_tensor = torch.tensor(trainvalid_data)\n",
        "trainvalid_labels_tensor = torch.tensor(trainvalid_labels)\n",
        "\n",
        "test_data_tensor = torch.tensor(test_data)\n",
        "test_labels_tensor = torch.tensor(test_labels)\n",
        "\n",
        "# Create a TensorDataset for train data\n",
        "train_dataset = TensorDataset(trainvalid_data_tensor, trainvalid_labels_tensor)\n",
        "\n",
        "# Create a DataLoader for train data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Create a TensorDataset for test data\n",
        "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
        "\n",
        "# Create a DataLoader for test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "CdgUI26qJ1ne"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = self.dropout(hidden[-1])\n",
        "        predictions = self.fc(hidden)\n",
        "        return predictions\n",
        "\n",
        "model = LSTMModel(vocab_size=MAX_NUM_WORDS, embedding_dim=128, hidden_dim=128, output_dim=len(trainvalid_labels[0]), dropout=0.2)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "N_EPOCHS = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print('Test accuracy with PyTorch LSTM:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQGW959xINUC",
        "outputId": "4ee2a2b8-bcbe-41ad-fc08-b45c0cf2a7ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with PyTorch LSTM: 0.728310502283105\n"
          ]
        }
      ]
    }
  ]
}