{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfJ-zud4Y7LJ"
      },
      "source": [
        "# **LSTM Model for Text Generation**\n",
        "\n",
        "This model has been updated from the one from the exercise notebook in class. We now use the entire Alice in Wonderland corpus for training the LSTM model. However, if you get memory errors you may decrease the amount of sentences used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK83PL0HCx36",
        "outputId": "fc64c2c9-de1c-4845-e6dd-4c4373a2573f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "import re\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load text from NLTK Gutenberg corpus\n",
        "text = gutenberg.raw(\"carroll-alice.txt\")\n",
        "text = text.replace('\\t', ' ').replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Define the corpus \n",
        "# YOU CAN TAKE A SMALL PIECE OF THIS LIST IF YOU EXPERIENCE MEMORY ISSUES\n",
        "corpus = sent_tokenize(text)\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenizer = lambda x: x.split()\n",
        "tokenized_corpus = [tokenizer(doc) for doc in corpus]\n",
        "\n",
        "# Create a vocabulary and dictionary of indices\n",
        "vocab = list(set([word for doc in tokenized_corpus for word in doc]))\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Convert the corpus to indices\n",
        "corpus_idx = [[word_to_idx[word] for word in doc] for doc in tokenized_corpus]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilj9mQoPQgdi",
        "outputId": "f6eb3ad3-f541-41e0-cde4-26213da5274b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add brief comments to the LSTM class to explain the purpose of each line of code.**"
      ],
      "metadata": {
        "id": "NydFQbyxRkdP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jxBb5lQRuKp",
        "outputId": "b34eb46d-ed0b-413b-bc20-6ed47f366063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:, Loss: 142.05\n",
            "Epoch 2:, Loss: 135.98\n",
            "Epoch 3:, Loss: 132.06\n",
            "Epoch 4:, Loss: 125.22\n",
            "Epoch 5:, Loss: 121.83\n",
            "Epoch 6:, Loss: 116.80\n",
            "Epoch 7:, Loss: 109.49\n",
            "Epoch 8:, Loss: 106.17\n",
            "Epoch 9:, Loss: 101.25\n",
            "Epoch 10:, Loss: 97.73\n",
            "Epoch 11:, Loss: 95.16\n",
            "Epoch 12:, Loss: 89.99\n",
            "Epoch 13:, Loss: 84.95\n",
            "Epoch 14:, Loss: 81.45\n",
            "Epoch 15:, Loss: 77.81\n",
            "Epoch 16:, Loss: 73.02\n",
            "Epoch 17:, Loss: 70.26\n",
            "Epoch 18:, Loss: 66.03\n",
            "Epoch 19:, Loss: 64.91\n",
            "Epoch 20:, Loss: 62.91\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# DOCUMENT THE CLASS\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        output, (hidden, cell) = self.lstm(input.view(1, 1, -1), (hidden, cell))\n",
        "        output = self.fc(output.view(1, -1))\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size)\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "hidden_size=32\n",
        "num_epoch=20\n",
        "model = LSTM(vocab_size, hidden_size, vocab_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the LSTM model\n",
        "for epoch in range(num_epoch):\n",
        "  for batch_start in range(0, len(corpus_idx), batch_size):\n",
        "      batch = corpus_idx[batch_start:batch_start + batch_size]\n",
        "      hidden, cell = model.initHidden()\n",
        "      loss = 0\n",
        "      for doc in batch:\n",
        "          for i in range(len(doc)-1):\n",
        "              input = torch.zeros(1, vocab_size)\n",
        "              input[0, doc[i]] = 1\n",
        "              target = torch.tensor([doc[i+1]], dtype=torch.long)\n",
        "              output, hidden, cell = model(input, hidden, cell)\n",
        "              loss += nn.functional.nll_loss(output, target)\n",
        "      loss /= batch_size\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "      optimizer.step()\n",
        "  print('Epoch {}:, Loss: {:.2f}'.format(epoch+1, loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text"
      ],
      "metadata": {
        "id": "sx7ifoDRFzJ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RM-wKDNRYT28"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, start_word, length, temperature=1.0):\n",
        "    with torch.no_grad():\n",
        "        # Initialize hidden and cell state\n",
        "        hidden, cell = model.initHidden()\n",
        "\n",
        "        # Convert the start word to a tensor\n",
        "        start_tensor = torch.zeros(1, vocab_size)\n",
        "        start_tensor[0, word_to_idx[start_word]] = 1\n",
        "\n",
        "        # Generate the initial hidden and cell state using the start word\n",
        "        output, hidden, cell = model(start_tensor, hidden, cell)\n",
        "\n",
        "        # Sample the next word based on the output probabilities and temperature\n",
        "        output = output.squeeze().div(temperature).exp().cpu()\n",
        "        word_idx = torch.multinomial(output, 1).item()\n",
        "\n",
        "        # Generate the rest of the text\n",
        "        output_text = [start_word]\n",
        "        for i in range(length - 1):\n",
        "            # Convert the previous predicted word to a tensor\n",
        "            input_tensor = torch.zeros(1, vocab_size)\n",
        "            input_tensor[0, word_idx] = 1\n",
        "\n",
        "            # Generate the next hidden and cell state using the previous predicted word\n",
        "            output, hidden, cell = model(input_tensor, hidden, cell)\n",
        "\n",
        "            # Sample the next word based on the output probabilities and temperature\n",
        "            output = output.squeeze().div(temperature).exp().cpu()\n",
        "            word_idx = torch.multinomial(output, 1).item()\n",
        "\n",
        "            # Convert the predicted word index to a string and add it to the generated text\n",
        "            output_text.append(idx_to_word[word_idx])\n",
        "\n",
        "        # End it with a period\n",
        "        output_text += '.'\n",
        "        return ' '.join(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate text using different values for temperature.**"
      ],
      "metadata": {
        "id": "ddoRL8L5TN_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHn8SfcSYzs0"
      },
      "outputs": [],
      "source": [
        "# Set the seed text and length of the generated text\n",
        "\n",
        "\n",
        "# Generate the text with different temperatures\n",
        "\n",
        "\n",
        "# Print the generated text\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}